<!DOCTYPE html>
<html class="client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-enabled vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<title>AI safety - Wikipedia</title>
<script>(function(){var className="client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-enabled vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-disabled skin-theme-clientpref-day vector-toc-available";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\w+$|[^\w-]+/g,'')+'-clientpref-\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={"wgBreakFrames":false,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":[
"",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"3313dbd3-be12-4120-9b0e-c282167b481c","wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"AI_safety","wgTitle":"AI safety","wgCurRevisionId":1233050013,"wgRevisionId":1233050013,"wgArticleId":72360809,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: missing periodical","CS1 maint: DOI inactive as of January 2024","CS1 maint: location missing publisher","Articles with short description","Short description is different from Wikidata","Articles lacking reliable references from July 2023","All articles lacking reliable references","All articles with unsourced statements","Articles with unsourced statements from March 2024","Articles with excerpts","Artificial intelligence",
"Existential risk from artificial general intelligence","Cybernetics"],"wgPageViewLanguage":"en","wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"AI_safety","wgRelevantArticleId":72360809,"wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgNoticeProject":"wikipedia","wgCiteReferencePreviewsActive":false,"wgFlaggedRevsParams":{"tags":{"status":{"levels":1}}},"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsFlags":6,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":true,"watchlist":true,"tagline":false,"nearby":true},"wgWMESchemaEditAttemptStepOversample":false,"wgWMEPageLength":80000,"wgULSCurrentAutonym":"English","wgCentralAuthMobileDomain":false,"wgEditSubmitButtonLabelPublish":true,"wgULSPosition":"interlanguage","wgULSisCompactLinksEnabled":false,
"wgVector2022LanguageInHeader":true,"wgULSisLanguageSelectorEmpty":false,"wgWikibaseItemId":"Q116291231","wgCheckUserClientHintsHeadersJsApi":["architecture","bitness","brands","fullVersionList","mobile","model","platform","platformVersion"],"GEHomepageSuggestedEditsEnableTopics":true,"wgGETopicsMatchModeEnabled":false,"wgGEStructuredTaskRejectionReasonTextInputEnabled":false,"wgGELevelingUpEnabledForUser":false};RLSTATE={"ext.globalCssJs.user.styles":"ready","site.styles":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","skins.vector.search.codex.styles":"ready","skins.vector.styles":"ready","skins.vector.icons":"ready","jquery.makeCollapsible.styles":"ready","ext.wikimediamessages.styles":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","wikibase.client.init":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements",
"mediawiki.page.media","site","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","skins.vector.js","ext.centralNotice.geoIP","ext.centralNotice.startUp","ext.gadget.ReferenceTooltips","ext.gadget.switcher","ext.urlShortener.toolbar","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.echo.centralauth","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.cx.uls.quick.actions","wikibase.client.vector-2022","ext.checkUser.clientHints","ext.growthExperiments.SuggestedEditSession"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.impl(function(){return["user.options@12s5i",function($,jQuery,require,module){mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
}];});});</script>
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cext.wikimediamessages.styles%7Cjquery.makeCollapsible.styles%7Cskins.vector.icons%2Cstyles%7Cskins.vector.search.codex.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector-2022">
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector-2022"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector-2022">
<meta name="generator" content="MediaWiki 1.43.0-wmf.12">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="max-image-preview:standard">
<meta name="format-detection" content="telephone=no">
<meta name="viewport" content="width=1120">
<meta property="og:title" content="AI safety - Wikipedia">
<meta property="og:type" content="website">
<link rel="preconnect" href="//upload.wikimedia.org">
<link rel="alternate" media="only screen and (max-width: 640px)" href="//en.m.wikipedia.org/wiki/AI_safety">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=AI_safety&amp;action=edit">
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png">
<link rel="icon" href="/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="/w/rest.php/v1/search" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd">
<link rel="canonical" href="https://en.wikipedia.org/wiki/AI_safety">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<link rel="dns-prefetch" href="//login.wikimedia.org">
</head>
<body class="skin--responsive skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-AI_safety rootpage-AI_safety skin-vector-2022 action-view"><a class="mw-jump-link" href="#bodyContent">Jump to content</a>
<div class="vector-header-container">
	<header class="vector-header mw-header">
		<div class="vector-header-start">
			<nav class="vector-main-menu-landmark" aria-label="Site">
				
<div id="vector-main-menu-dropdown" class="vector-dropdown vector-main-menu-dropdown vector-button-flush-left vector-button-flush-right"  >
	<input type="checkbox" id="vector-main-menu-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-main-menu-dropdown" class="vector-dropdown-checkbox "  aria-label="Main menu"  >
	<label id="vector-main-menu-dropdown-label" for="vector-main-menu-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-menu mw-ui-icon-wikimedia-menu"></span>

<span class="vector-dropdown-label-text">Main menu</span>
	</label>
	<div class="vector-dropdown-content">


				<div id="vector-main-menu-unpinned-container" class="vector-unpinned-container">
		
<div id="vector-main-menu" class="vector-main-menu vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-main-menu-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="main-menu-pinned"
	data-pinnable-element-id="vector-main-menu"
	data-pinned-container-id="vector-main-menu-pinned-container"
	data-unpinned-container-id="vector-main-menu-unpinned-container"
>
	<div class="vector-pinnable-header-label">Main menu</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-main-menu.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-main-menu.unpin">hide</button>
</div>

	
<div id="p-navigation" class="vector-menu mw-portlet mw-portlet-navigation"  >
	<div class="vector-menu-heading">
		Navigation
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-mainpage-description" class="mw-list-item"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z"><span>Main page</span></a></li><li id="n-contents" class="mw-list-item"><a href="/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia"><span>Contents</span></a></li><li id="n-currentevents" class="mw-list-item"><a href="/wiki/Portal:Current_events" title="Articles related to current events"><span>Current events</span></a></li><li id="n-randompage" class="mw-list-item"><a href="/wiki/Special:Random" title="Visit a randomly selected article [x]" accesskey="x"><span>Random article</span></a></li><li id="n-aboutsite" class="mw-list-item"><a href="/wiki/Wikipedia:About" title="Learn about Wikipedia and how it works"><span>About Wikipedia</span></a></li><li id="n-contactpage" class="mw-list-item"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia"><span>Contact us</span></a></li><li id="n-sitesupport" class="mw-list-item"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us by donating to the Wikimedia Foundation"><span>Donate</span></a></li>
		</ul>
		
	</div>
</div>

	
	
<div id="p-interaction" class="vector-menu mw-portlet mw-portlet-interaction"  >
	<div class="vector-menu-heading">
		Contribute
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="n-help" class="mw-list-item"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia"><span>Help</span></a></li><li id="n-introduction" class="mw-list-item"><a href="/wiki/Help:Introduction" title="Learn how to edit Wikipedia"><span>Learn to edit</span></a></li><li id="n-portal" class="mw-list-item"><a href="/wiki/Wikipedia:Community_portal" title="The hub for editors"><span>Community portal</span></a></li><li id="n-recentchanges" class="mw-list-item"><a href="/wiki/Special:RecentChanges" title="A list of recent changes to Wikipedia [r]" accesskey="r"><span>Recent changes</span></a></li><li id="n-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_upload_wizard" title="Add images or other media for use on Wikipedia"><span>Upload file</span></a></li>
		</ul>
		
	</div>
</div>

</div>

				</div>

	</div>
</div>

		</nav>
			
<a href="/wiki/Main_Page" class="mw-logo">
	<img class="mw-logo-icon" src="/static/images/icons/wikipedia.png" alt="" aria-hidden="true" height="50" width="50">
	<span class="mw-logo-container skin-invert">
		<img class="mw-logo-wordmark" alt="Wikipedia" src="/static/images/mobile/copyright/wikipedia-wordmark-en.svg" style="width: 7.5em; height: 1.125em;">
		<img class="mw-logo-tagline" alt="The Free Encyclopedia" src="/static/images/mobile/copyright/wikipedia-tagline-en.svg" width="117" height="13" style="width: 7.3125em; height: 0.8125em;">
	</span>
</a>

		</div>
		<div class="vector-header-end">
			
<div id="p-search" role="search" class="vector-search-box-vue  vector-search-box-collapses vector-search-box-show-thumbnail vector-search-box-auto-expand-width vector-search-box">
	<a href="/wiki/Special:Search" class="cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only search-toggle" title="Search Wikipedia [f]" accesskey="f"><span class="vector-icon mw-ui-icon-search mw-ui-icon-wikimedia-search"></span>

<span>Search</span>
	</a>
	<div class="vector-typeahead-search-container">
		<div class="cdx-typeahead-search cdx-typeahead-search--show-thumbnail cdx-typeahead-search--auto-expand-width">
			<form action="/w/index.php" id="searchform" class="cdx-search-input cdx-search-input--has-end-button">
				<div id="simpleSearch" class="cdx-search-input__input-wrapper"  data-search-loc="header-moved">
					<div class="cdx-text-input cdx-text-input--has-start-icon">
						<input
							class="cdx-text-input__input"
							 type="search" name="search" placeholder="Search Wikipedia" aria-label="Search Wikipedia" autocapitalize="sentences" title="Search Wikipedia [f]" accesskey="f" id="searchInput"
							>
						<span class="cdx-text-input__icon cdx-text-input__start-icon"></span>
					</div>
					<input type="hidden" name="title" value="Special:Search">
				</div>
				<button class="cdx-button cdx-search-input__end-button">Search</button>
			</form>
		</div>
	</div>
</div>

			<nav class="vector-user-links vector-user-links-wide" aria-label="Personal tools">
	<div class="vector-user-links-main">
	
<div id="p-vector-user-menu-preferences" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-userpage" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	<nav class="vector-appearance-landmark" aria-label="Appearance">
		
<div id="vector-appearance-dropdown" class="vector-dropdown "  >
	<input type="checkbox" id="vector-appearance-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-appearance-dropdown" class="vector-dropdown-checkbox "  aria-label="Appearance"  >
	<label id="vector-appearance-dropdown-label" for="vector-appearance-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-appearance mw-ui-icon-wikimedia-appearance"></span>

<span class="vector-dropdown-label-text">Appearance</span>
	</label>
	<div class="vector-dropdown-content">


			<div id="vector-appearance-unpinned-container" class="vector-unpinned-container">
				
			</div>
		
	</div>
</div>

	</nav>
	
<div id="p-vector-user-menu-notifications" class="vector-menu mw-portlet emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

	
<div id="p-vector-user-menu-overflow" class="vector-menu mw-portlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			<li id="pt-createaccount-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:CreateAccount&amp;returnto=AI+safety" title="You are encouraged to create an account and log in; however, it is not mandatory" class=""><span>Create account</span></a>
</li>
<li id="pt-login-2" class="user-links-collapsible-item mw-list-item user-links-collapsible-item"><a data-mw="interface" href="/w/index.php?title=Special:UserLogin&amp;returnto=AI+safety" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o" class=""><span>Log in</span></a>
</li>

			
		</ul>
		
	</div>
</div>

	</div>
	
<div id="vector-user-links-dropdown" class="vector-dropdown vector-user-menu vector-button-flush-right vector-user-menu-logged-out"  title="Log in and more options" >
	<input type="checkbox" id="vector-user-links-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-user-links-dropdown" class="vector-dropdown-checkbox "  aria-label="Personal tools"  >
	<label id="vector-user-links-dropdown-label" for="vector-user-links-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-ellipsis mw-ui-icon-wikimedia-ellipsis"></span>

<span class="vector-dropdown-label-text">Personal tools</span>
	</label>
	<div class="vector-dropdown-content">


		
<div id="p-personal" class="vector-menu mw-portlet mw-portlet-personal user-links-collapsible-item"  title="User menu" >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-createaccount" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=AI+safety" title="You are encouraged to create an account and log in; however, it is not mandatory"><span class="vector-icon mw-ui-icon-userAdd mw-ui-icon-wikimedia-userAdd"></span> <span>Create account</span></a></li><li id="pt-login" class="user-links-collapsible-item mw-list-item"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=AI+safety" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o"><span class="vector-icon mw-ui-icon-logIn mw-ui-icon-wikimedia-logIn"></span> <span>Log in</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-user-menu-anon-editor" class="vector-menu mw-portlet mw-portlet-user-menu-anon-editor"  >
	<div class="vector-menu-heading">
		Pages for logged out editors <a href="/wiki/Help:Introduction" aria-label="Learn more about editing"><span>learn more</span></a>
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="pt-anoncontribs" class="mw-list-item"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y"><span>Contributions</span></a></li><li id="pt-anontalk" class="mw-list-item"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

	
	</div>
</div>

</nav>

		</div>
	</header>
</div>
<div class="mw-page-container">
	<div class="mw-page-container-inner">
		<div class="vector-sitenotice-container">
			<div id="siteNotice" class="notheme"><!-- CentralNotice --></div>
		</div>
		<div class="vector-column-start">
			<div class="vector-main-menu-container">
		<div id="mw-navigation">
			<nav id="mw-panel" class="vector-main-menu-landmark" aria-label="Site">
				<div id="vector-main-menu-pinned-container" class="vector-pinned-container">
				
				</div>
		</nav>
		</div>
	</div>
	<div class="vector-sticky-pinned-container">
				<nav id="mw-panel-toc" aria-label="Contents" data-event-name="ui.sidebar-toc" class="mw-table-of-contents-container vector-toc-landmark">
					<div id="vector-toc-pinned-container" class="vector-pinned-container">
					<div id="vector-toc" class="vector-toc vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-toc-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="toc-pinned"
	data-pinnable-element-id="vector-toc"
	
	
>
	<h2 class="vector-pinnable-header-label">Contents</h2>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-toc.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-toc.unpin">hide</button>
</div>


	<ul class="vector-toc-contents" id="mw-panel-toc-list">
		<li id="toc-mw-content-text"
			class="vector-toc-list-item vector-toc-level-1">
			<a href="#" class="vector-toc-link">
				<div class="vector-toc-text">(Top)</div>
			</a>
		</li>
		<li id="toc-Motivations"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Motivations">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">1</span>
				<span>Motivations</span>
			</div>
		</a>
		
			<button aria-controls="toc-Motivations-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Motivations subsection</span>
			</button>
		
		<ul id="toc-Motivations-sublist" class="vector-toc-list">
			<li id="toc-Existential_safety"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Existential_safety">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">1.1</span>
					<span>Existential safety</span>
				</div>
			</a>
			
			<ul id="toc-Existential_safety-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-History"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#History">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">2</span>
				<span>History</span>
			</div>
		</a>
		
		<ul id="toc-History-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-Research_focus"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#Research_focus">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">3</span>
				<span>Research focus</span>
			</div>
		</a>
		
			<button aria-controls="toc-Research_focus-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle Research focus subsection</span>
			</button>
		
		<ul id="toc-Research_focus-sublist" class="vector-toc-list">
			<li id="toc-Robustness"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Robustness">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.1</span>
					<span>Robustness</span>
				</div>
			</a>
			
			<ul id="toc-Robustness-sublist" class="vector-toc-list">
				<li id="toc-Adversarial_robustness"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Adversarial_robustness">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.1.1</span>
					<span>Adversarial robustness</span>
				</div>
			</a>
			
			<ul id="toc-Adversarial_robustness-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Monitoring"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Monitoring">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2</span>
					<span>Monitoring</span>
				</div>
			</a>
			
			<ul id="toc-Monitoring-sublist" class="vector-toc-list">
				<li id="toc-Estimating_uncertainty"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Estimating_uncertainty">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2.1</span>
					<span>Estimating uncertainty</span>
				</div>
			</a>
			
			<ul id="toc-Estimating_uncertainty-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Detecting_malicious_use"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Detecting_malicious_use">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2.2</span>
					<span>Detecting malicious use</span>
				</div>
			</a>
			
			<ul id="toc-Detecting_malicious_use-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Transparency"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Transparency">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2.3</span>
					<span>Transparency</span>
				</div>
			</a>
			
			<ul id="toc-Transparency-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Detecting_trojans"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Detecting_trojans">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.2.4</span>
					<span>Detecting trojans</span>
				</div>
			</a>
			
			<ul id="toc-Detecting_trojans-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
		<li id="toc-Alignment"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Alignment">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.3</span>
					<span>Alignment</span>
				</div>
			</a>
			
			<ul id="toc-Alignment-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Systemic_safety_and_sociotechnical_factors"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Systemic_safety_and_sociotechnical_factors">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.4</span>
					<span>Systemic safety and sociotechnical factors</span>
				</div>
			</a>
			
			<ul id="toc-Systemic_safety_and_sociotechnical_factors-sublist" class="vector-toc-list">
				<li id="toc-Cyber_defense"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Cyber_defense">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.4.1</span>
					<span>Cyber defense</span>
				</div>
			</a>
			
			<ul id="toc-Cyber_defense-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Improving_institutional_decision-making"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Improving_institutional_decision-making">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.4.2</span>
					<span>Improving institutional decision-making</span>
				</div>
			</a>
			
			<ul id="toc-Improving_institutional_decision-making-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Facilitating_cooperation"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Facilitating_cooperation">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.4.3</span>
					<span>Facilitating cooperation</span>
				</div>
			</a>
			
			<ul id="toc-Facilitating_cooperation-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Challenges_of_large_language_models"
			class="vector-toc-list-item vector-toc-level-3">
			<a class="vector-toc-link" href="#Challenges_of_large_language_models">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">3.4.4</span>
					<span>Challenges of large language models</span>
				</div>
			</a>
			
			<ul id="toc-Challenges_of_large_language_models-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
		</li>
	</ul>
	</li>
	<li id="toc-In_governance"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#In_governance">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">4</span>
				<span>In governance</span>
			</div>
		</a>
		
			<button aria-controls="toc-In_governance-sublist" class="cdx-button cdx-button--weight-quiet cdx-button--icon-only vector-toc-toggle">
				<span class="vector-icon mw-ui-icon-wikimedia-expand"></span>
				<span>Toggle In governance subsection</span>
			</button>
		
		<ul id="toc-In_governance-sublist" class="vector-toc-list">
			<li id="toc-Research"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Research">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.1</span>
					<span>Research</span>
				</div>
			</a>
			
			<ul id="toc-Research-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Scaling_Local_AI_Safety_Measures_to_Global_Solutions"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Scaling_Local_AI_Safety_Measures_to_Global_Solutions">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.2</span>
					<span>Scaling Local AI Safety Measures to Global Solutions</span>
				</div>
			</a>
			
			<ul id="toc-Scaling_Local_AI_Safety_Measures_to_Global_Solutions-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Government_action"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Government_action">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.3</span>
					<span>Government action</span>
				</div>
			</a>
			
			<ul id="toc-Government_action-sublist" class="vector-toc-list">
			</ul>
		</li>
		<li id="toc-Corporate_self-regulation"
			class="vector-toc-list-item vector-toc-level-2">
			<a class="vector-toc-link" href="#Corporate_self-regulation">
				<div class="vector-toc-text">
					<span class="vector-toc-numb">4.4</span>
					<span>Corporate self-regulation</span>
				</div>
			</a>
			
			<ul id="toc-Corporate_self-regulation-sublist" class="vector-toc-list">
			</ul>
		</li>
	</ul>
	</li>
	<li id="toc-See_also"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#See_also">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">5</span>
				<span>See also</span>
			</div>
		</a>
		
		<ul id="toc-See_also-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-References"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#References">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">6</span>
				<span>References</span>
			</div>
		</a>
		
		<ul id="toc-References-sublist" class="vector-toc-list">
		</ul>
	</li>
	<li id="toc-External_links"
		class="vector-toc-list-item vector-toc-level-1 vector-toc-list-item-expanded">
		<a class="vector-toc-link" href="#External_links">
			<div class="vector-toc-text">
				<span class="vector-toc-numb">7</span>
				<span>External links</span>
			</div>
		</a>
		
		<ul id="toc-External_links-sublist" class="vector-toc-list">
		</ul>
	</li>
</ul>
</div>

					</div>
		</nav>
			</div>
		</div>
		<div class="mw-content-container">
			<main id="content" class="mw-body">
				<header class="mw-body-header vector-page-titlebar">
					<nav aria-label="Contents" class="vector-toc-landmark">
						
<div id="vector-page-titlebar-toc" class="vector-dropdown vector-page-titlebar-toc vector-button-flush-left"  >
	<input type="checkbox" id="vector-page-titlebar-toc-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-titlebar-toc" class="vector-dropdown-checkbox "  aria-label="Toggle the table of contents"  >
	<label id="vector-page-titlebar-toc-label" for="vector-page-titlebar-toc-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--icon-only " aria-hidden="true"  ><span class="vector-icon mw-ui-icon-listBullet mw-ui-icon-wikimedia-listBullet"></span>

<span class="vector-dropdown-label-text">Toggle the table of contents</span>
	</label>
	<div class="vector-dropdown-content">


							<div id="vector-page-titlebar-toc-unpinned-container" class="vector-unpinned-container">
			</div>
		
	</div>
</div>

					</nav>
					<h1 id="firstHeading" class="firstHeading mw-first-heading"><span class="mw-page-title-main">AI safety</span></h1>
							
<div id="p-lang-btn" class="vector-dropdown mw-portlet mw-portlet-lang"  >
	<input type="checkbox" id="p-lang-btn-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-p-lang-btn" class="vector-dropdown-checkbox mw-interlanguage-selector" aria-label="Go to an article in another language. Available in 13 languages"   >
	<label id="p-lang-btn-label" for="p-lang-btn-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet cdx-button--action-progressive mw-portlet-lang-heading-13" aria-hidden="true"  ><span class="vector-icon mw-ui-icon-language-progressive mw-ui-icon-wikimedia-language-progressive"></span>

<span class="vector-dropdown-label-text">13 languages</span>
	</label>
	<div class="vector-dropdown-content">

		<div class="vector-menu-content">
			
			<ul class="vector-menu-content-list">
				
				<li class="interlanguage-link interwiki-af mw-list-item"><a href="https://af.wikipedia.org/wiki/Kunsmatige-intelligensie-veiligheid" title="Kunsmatige-intelligensie-veiligheid – Afrikaans" lang="af" hreflang="af" class="interlanguage-link-target"><span>Afrikaans</span></a></li><li class="interlanguage-link interwiki-ar mw-list-item"><a href="https://ar.wikipedia.org/wiki/%D8%B3%D9%84%D8%A7%D9%85%D8%A9_%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1_%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A" title="سلامة الذكاء الاصطناعي – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target"><span>العربية</span></a></li><li class="interlanguage-link interwiki-az mw-list-item"><a href="https://az.wikipedia.org/wiki/S%C3%BCni_intellekt_t%C9%99hl%C3%BCk%C9%99sizliyi" title="Süni intellekt təhlükəsizliyi – Azerbaijani" lang="az" hreflang="az" class="interlanguage-link-target"><span>Azərbaycanca</span></a></li><li class="interlanguage-link interwiki-ca mw-list-item"><a href="https://ca.wikipedia.org/wiki/Seguretat_de_la_IA" title="Seguretat de la IA – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target"><span>Català</span></a></li><li class="interlanguage-link interwiki-es mw-list-item"><a href="https://es.wikipedia.org/wiki/Seguridad_de_la_inteligencia_artificial" title="Seguridad de la inteligencia artificial – Spanish" lang="es" hreflang="es" class="interlanguage-link-target"><span>Español</span></a></li><li class="interlanguage-link interwiki-fa mw-list-item"><a href="https://fa.wikipedia.org/wiki/%D8%A7%DB%8C%D9%85%D9%86%DB%8C_%D9%87%D9%88%D8%B4_%D9%85%D8%B5%D9%86%D9%88%D8%B9%DB%8C" title="ایمنی هوش مصنوعی – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target"><span>فارسی</span></a></li><li class="interlanguage-link interwiki-fr mw-list-item"><a href="https://fr.wikipedia.org/wiki/S%C3%BBret%C3%A9_des_intelligences_artificielles" title="Sûreté des intelligences artificielles – French" lang="fr" hreflang="fr" class="interlanguage-link-target"><span>Français</span></a></li><li class="interlanguage-link interwiki-ko mw-list-item"><a href="https://ko.wikipedia.org/wiki/AI_%EC%95%88%EC%A0%84" title="AI 안전 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target"><span>한국어</span></a></li><li class="interlanguage-link interwiki-ps mw-list-item"><a href="https://ps.wikipedia.org/wiki/%D8%AF_%D9%85%D8%B5%D9%86%D9%88%D8%B9%D9%8A_%DA%81%D9%8A%D8%B1%DA%A9%D8%AA%D9%8A%D8%A7_%D8%AE%D9%88%D9%86%D8%AF%D9%8A%D8%AA%D9%88%D8%A8" title="د مصنوعي ځيرکتيا خونديتوب – Pashto" lang="ps" hreflang="ps" class="interlanguage-link-target"><span>پښتو</span></a></li><li class="interlanguage-link interwiki-pt mw-list-item"><a href="https://pt.wikipedia.org/wiki/Seguran%C3%A7a_da_intelig%C3%AAncia_artificial" title="Segurança da inteligência artificial – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target"><span>Português</span></a></li><li class="interlanguage-link interwiki-sr mw-list-item"><a href="https://sr.wikipedia.org/wiki/Bezbednost_ve%C5%A1ta%C4%8Dke_inteligencije" title="Bezbednost veštačke inteligencije – Serbian" lang="sr" hreflang="sr" class="interlanguage-link-target"><span>Српски / srpski</span></a></li><li class="interlanguage-link interwiki-tr mw-list-item"><a href="https://tr.wikipedia.org/wiki/Yapay_zeka_g%C3%BCvenli%C4%9Fi" title="Yapay zeka güvenliği – Turkish" lang="tr" hreflang="tr" class="interlanguage-link-target"><span>Türkçe</span></a></li><li class="interlanguage-link interwiki-uk mw-list-item"><a href="https://uk.wikipedia.org/wiki/%D0%91%D0%B5%D0%B7%D0%BF%D0%B5%D0%BA%D0%B0_%D1%88%D1%82%D1%83%D1%87%D0%BD%D0%BE%D0%B3%D0%BE_%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82%D1%83" title="Безпека штучного інтелекту – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target"><span>Українська</span></a></li>
			</ul>
			<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q116291231#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
		</div>

	</div>
</div>
</header>
				<div class="vector-page-toolbar">
					<div class="vector-page-toolbar-container">
						<div id="left-navigation">
							<nav aria-label="Namespaces">
								
<div id="p-associated-pages" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-associated-pages"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-nstab-main" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/AI_safety" title="View the content page [c]" accesskey="c"><span>Article</span></a></li><li id="ca-talk" class="vector-tab-noicon mw-list-item"><a href="/wiki/Talk:AI_safety" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t"><span>Talk</span></a></li>
		</ul>
		
	</div>
</div>

								
<div id="vector-variants-dropdown" class="vector-dropdown emptyPortlet"  >
	<input type="checkbox" id="vector-variants-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-variants-dropdown" class="vector-dropdown-checkbox " aria-label="Change language variant"   >
	<label id="vector-variants-dropdown-label" for="vector-variants-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">English</span>
	</label>
	<div class="vector-dropdown-content">


					
<div id="p-variants" class="vector-menu mw-portlet mw-portlet-variants emptyPortlet"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			
		</ul>
		
	</div>
</div>

				
	</div>
</div>

							</nav>
						</div>
						<div id="right-navigation" class="vector-collapsible">
							<nav aria-label="Views">
								
<div id="p-views" class="vector-menu vector-menu-tabs mw-portlet mw-portlet-views"  >
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-view" class="selected vector-tab-noicon mw-list-item"><a href="/wiki/AI_safety"><span>Read</span></a></li><li id="ca-edit" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=AI_safety&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-history" class="vector-tab-noicon mw-list-item"><a href="/w/index.php?title=AI_safety&amp;action=history" title="Past revisions of this page [h]" accesskey="h"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

							</nav>
				
							<nav class="vector-page-tools-landmark" aria-label="Page tools">
								
<div id="vector-page-tools-dropdown" class="vector-dropdown vector-page-tools-dropdown"  >
	<input type="checkbox" id="vector-page-tools-dropdown-checkbox" role="button" aria-haspopup="true" data-event-name="ui.dropdown-vector-page-tools-dropdown" class="vector-dropdown-checkbox "  aria-label="Tools"  >
	<label id="vector-page-tools-dropdown-label" for="vector-page-tools-dropdown-checkbox" class="vector-dropdown-label cdx-button cdx-button--fake-button cdx-button--fake-button--enabled cdx-button--weight-quiet" aria-hidden="true"  ><span class="vector-dropdown-label-text">Tools</span>
	</label>
	<div class="vector-dropdown-content">


									<div id="vector-page-tools-unpinned-container" class="vector-unpinned-container">
						
<div id="vector-page-tools" class="vector-page-tools vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-page-tools-pinnable-header vector-pinnable-header-unpinned"
	data-feature-name="page-tools-pinned"
	data-pinnable-element-id="vector-page-tools"
	data-pinned-container-id="vector-page-tools-pinned-container"
	data-unpinned-container-id="vector-page-tools-unpinned-container"
>
	<div class="vector-pinnable-header-label">Tools</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-page-tools.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-page-tools.unpin">hide</button>
</div>

	
<div id="p-cactions" class="vector-menu mw-portlet mw-portlet-cactions emptyPortlet vector-has-collapsible-items"  title="More options" >
	<div class="vector-menu-heading">
		Actions
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="ca-more-view" class="selected vector-more-collapsible-item mw-list-item"><a href="/wiki/AI_safety"><span>Read</span></a></li><li id="ca-more-edit" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=AI_safety&amp;action=edit" title="Edit this page [e]" accesskey="e"><span>Edit</span></a></li><li id="ca-more-history" class="vector-more-collapsible-item mw-list-item"><a href="/w/index.php?title=AI_safety&amp;action=history"><span>View history</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-tb" class="vector-menu mw-portlet mw-portlet-tb"  >
	<div class="vector-menu-heading">
		General
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="t-whatlinkshere" class="mw-list-item"><a href="/wiki/Special:WhatLinksHere/AI_safety" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j"><span>What links here</span></a></li><li id="t-recentchangeslinked" class="mw-list-item"><a href="/wiki/Special:RecentChangesLinked/AI_safety" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k"><span>Related changes</span></a></li><li id="t-upload" class="mw-list-item"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u"><span>Upload file</span></a></li><li id="t-specialpages" class="mw-list-item"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q"><span>Special pages</span></a></li><li id="t-permalink" class="mw-list-item"><a href="/w/index.php?title=AI_safety&amp;oldid=1233050013" title="Permanent link to this revision of this page"><span>Permanent link</span></a></li><li id="t-info" class="mw-list-item"><a href="/w/index.php?title=AI_safety&amp;action=info" title="More information about this page"><span>Page information</span></a></li><li id="t-cite" class="mw-list-item"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=AI_safety&amp;id=1233050013&amp;wpFormIdentifier=titleform" title="Information on how to cite this page"><span>Cite this page</span></a></li><li id="t-urlshortener" class="mw-list-item"><a href="/w/index.php?title=Special:UrlShortener&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAI_safety"><span>Get shortened URL</span></a></li><li id="t-urlshortener-qrcode" class="mw-list-item"><a href="/w/index.php?title=Special:QrCode&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAI_safety"><span>Download QR code</span></a></li><li id="t-wikibase" class="mw-list-item"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q116291231" title="Structured data on this page hosted by Wikidata [g]" accesskey="g"><span>Wikidata item</span></a></li>
		</ul>
		
	</div>
</div>

<div id="p-coll-print_export" class="vector-menu mw-portlet mw-portlet-coll-print_export"  >
	<div class="vector-menu-heading">
		Print/export
	</div>
	<div class="vector-menu-content">
		
		<ul class="vector-menu-content-list">
			
			<li id="coll-download-as-rl" class="mw-list-item"><a href="/w/index.php?title=Special:DownloadAsPdf&amp;page=AI_safety&amp;action=show-download-screen" title="Download this page as a PDF file"><span>Download as PDF</span></a></li><li id="t-print" class="mw-list-item"><a href="/w/index.php?title=AI_safety&amp;printable=yes" title="Printable version of this page [p]" accesskey="p"><span>Printable version</span></a></li>
		</ul>
		
	</div>
</div>

</div>

									</div>
				
	</div>
</div>

							</nav>
						</div>
					</div>
				</div>
				<div class="vector-column-end">
					<div class="vector-sticky-pinned-container">
						<nav class="vector-page-tools-landmark" aria-label="Page tools">
							<div id="vector-page-tools-pinned-container" class="vector-pinned-container">
				
							</div>
		</nav>
						<nav class="vector-appearance-landmark" aria-label="Appearance">
							<div id="vector-appearance-pinned-container" class="vector-pinned-container">
				<div id="vector-appearance" class="vector-appearance vector-pinnable-element">
	<div
	class="vector-pinnable-header vector-appearance-pinnable-header vector-pinnable-header-pinned"
	data-feature-name="appearance-pinned"
	data-pinnable-element-id="vector-appearance"
	data-pinned-container-id="vector-appearance-pinned-container"
	data-unpinned-container-id="vector-appearance-unpinned-container"
>
	<div class="vector-pinnable-header-label">Appearance</div>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-pin-button" data-event-name="pinnable-header.vector-appearance.pin">move to sidebar</button>
	<button class="vector-pinnable-header-toggle-button vector-pinnable-header-unpin-button" data-event-name="pinnable-header.vector-appearance.unpin">hide</button>
</div>


</div>

							</div>
		</nav>
					</div>
				</div>
				<div id="bodyContent" class="vector-body" aria-labelledby="firstHeading" data-mw-ve-target-container>
					<div class="vector-body-before-content">
							<div class="mw-indicators">
		</div>

						<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
					</div>
					<div id="contentSub"><div id="mw-content-subtitle"></div></div>
					
					
					<div id="mw-content-text" class="mw-body-content"><div class="mw-content-ltr mw-parser-output" lang="en" dir="ltr"><div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">Research area on making AI safe and beneficial</div>
<style data-mw-deduplicate="TemplateStyles:r1129693374">.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}</style><style data-mw-deduplicate="TemplateStyles:r1232845482">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}}</style><table class="sidebar sidebar-collapse nomobile nowraplinks hlist"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle"><a href="/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a></th></tr><tr><td class="sidebar-image"><figure class="mw-halign-center" typeof="mw:File"><a href="/wiki/File:Dall-e_3_(jan_%2724)_artificial_intelligence_icon.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png/100px-Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png" decoding="async" width="100" height="100" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/64/Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png/150px-Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/64/Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png/200px-Dall-e_3_%28jan_%2724%29_artificial_intelligence_icon.png 2x" data-file-width="820" data-file-height="820" /></a><figcaption></figcaption></figure></td></tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)"><a href="/wiki/Artificial_intelligence#Goals" title="Artificial intelligence">Major goals</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">Artificial general intelligence</a></li>
<li><a href="/wiki/Intelligent_Agent" class="mw-redirect" title="Intelligent Agent">Intelligent Agent</a></li>
<li><a href="/wiki/Recursive_self-improvement" title="Recursive self-improvement">Recursive self-improvement</a></li>
<li><a href="/wiki/Automated_planning_and_scheduling" title="Automated planning and scheduling">Planning</a></li>
<li><a href="/wiki/Computer_vision" title="Computer vision">Computer vision</a></li>
<li><a href="/wiki/General_game_playing" title="General game playing">General game playing</a></li>
<li><a href="/wiki/Knowledge_representation_and_reasoning" title="Knowledge representation and reasoning">Knowledge reasoning</a></li>
<li><a href="/wiki/Natural_language_processing" title="Natural language processing">Natural language processing</a></li>
<li><a href="/wiki/Robotics" title="Robotics">Robotics</a></li>
<li><a class="mw-selflink selflink">AI safety</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)">Approaches</div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a></li>
<li><a href="/wiki/Symbolic_artificial_intelligence" title="Symbolic artificial intelligence">Symbolic</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayesian networks</a></li>
<li><a href="/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">Evolutionary algorithms</a></li>
<li><a href="/wiki/Situated_approach_(artificial_intelligence)" title="Situated approach (artificial intelligence)">Situated approach</a></li>
<li><a href="/wiki/Hybrid_intelligent_system" title="Hybrid intelligent system">Hybrid intelligent systems</a></li>
<li><a href="/wiki/Artificial_intelligence_systems_integration" title="Artificial intelligence systems integration">Systems integration</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)"><a href="/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/List_of_artificial_intelligence_projects" title="List of artificial intelligence projects">Projects</a></li>
<li><a href="/wiki/Deepfake" title="Deepfake">Deepfake</a></li>
<li><a href="/wiki/Machine_translation" title="Machine translation">Machine translation</a></li>
<li><a href="/wiki/Generative_artificial_intelligence" title="Generative artificial intelligence">Generative AI</a>
<ul><li><a href="/wiki/Artificial_intelligence_art" title="Artificial intelligence art">Art</a></li>
<li><a href="/wiki/Generative_audio" title="Generative audio">Audio</a></li>
<li><a href="/wiki/Music_and_artificial_intelligence" title="Music and artificial intelligence">Music</a></li></ul></li>
<li><a href="/wiki/Artificial_intelligence_in_healthcare" title="Artificial intelligence in healthcare">Healthcare</a>
<ul><li><a href="/wiki/Artificial_intelligence_in_mental_health" title="Artificial intelligence in mental health">Mental health</a></li></ul></li>
<li><a href="/wiki/Artificial_intelligence_in_government" title="Artificial intelligence in government">Government</a></li>
<li><a href="/wiki/Artificial_intelligence_in_industry" title="Artificial intelligence in industry">Industry</a></li>
<li><a href="/wiki/Machine_learning_in_earth_sciences" title="Machine learning in earth sciences">Earth sciences</a></li>
<li><a href="/wiki/Machine_learning_in_bioinformatics" title="Machine learning in bioinformatics">Bioinformatics</a></li>
<li><a href="/wiki/Machine_learning_in_physics" title="Machine learning in physics">Physics</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)"><a href="/wiki/Philosophy_of_artificial_intelligence" title="Philosophy of artificial intelligence">Philosophy</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Chinese_room" title="Chinese room">Chinese room</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly AI</a></li>
<li><a href="/wiki/AI_control_problem" class="mw-redirect" title="AI control problem">Control problem</a>/<a href="/wiki/AI_takeover" title="AI takeover">Takeover</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a></li>
<li><a href="/wiki/Turing_test" title="Turing test">Turing test</a></li>
<li><a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)"><a href="/wiki/History_of_artificial_intelligence" title="History of artificial intelligence">History</a></div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Timeline_of_artificial_intelligence" title="Timeline of artificial intelligence">Timeline</a></li>
<li><a href="/wiki/Progress_in_artificial_intelligence" title="Progress in artificial intelligence">Progress</a></li>
<li><a href="/wiki/AI_winter" title="AI winter">AI winter</a></li>
<li><a href="/wiki/AI_boom" title="AI boom">AI boom</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-content">
<div class="sidebar-list mw-collapsible mw-collapsed"><div class="sidebar-list-title" style="text-align:center;color: var(--color-base)">Glossary</div><div class="sidebar-list-content mw-collapsible-content">
<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary</a></li></ul></div></div></td>
</tr><tr><td class="sidebar-navbar"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Artificial_intelligence" title="Template:Artificial intelligence"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Artificial_intelligence" title="Template talk:Artificial intelligence"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Artificial_intelligence" title="Special:EditPage/Template:Artificial intelligence"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table><p><b>AI safety</b> is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI) systems. It encompasses <a href="/wiki/Machine_ethics" title="Machine ethics">machine ethics</a> and <a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a>, which aim to ensure AI systems are moral and beneficial, as well as monitoring AI systems for risks and enhancing their reliability. The field is particularly concerned with <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">existential risks</a> posed by advanced AI models.
</p><p>Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in <a href="/wiki/Generative_AI" class="mw-redirect" title="Generative AI">generative AI</a> and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 <a href="/wiki/AI_Safety_Summit" title="AI Safety Summit">AI Safety Summit</a>, the United States and the United Kingdom both established their own <a href="/wiki/AI_Safety_Institute" title="AI Safety Institute">AI Safety Institute</a>. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup>
</p>
<meta property="mw:PageProp/toc" />
<h2><span class="mw-headline" id="Motivations">Motivations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=1" title="Edit section: Motivations"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Scholars discuss current risks from <a href="/wiki/Critical_system" title="Critical system">critical systems</a> failures,<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> <a href="/wiki/Algorithmic_bias" title="Algorithmic bias">bias</a>,<sup id="cite_ref-:3_3-0" class="reference"><a href="#cite_note-:3-3">&#91;3&#93;</a></sup> and AI-enabled surveillance,<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">&#91;4&#93;</a></sup> as well as emerging risks like <a href="/wiki/Technological_unemployment" title="Technological unemployment">technological unemployment</a>, digital manipulation,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">&#91;5&#93;</a></sup> weaponization,<sup id="cite_ref-:13_6-0" class="reference"><a href="#cite_note-:13-6">&#91;6&#93;</a></sup> AI-enabled <a href="/wiki/Cyberattack" title="Cyberattack">cyberattacks</a><sup id="cite_ref-7" class="reference"><a href="#cite_note-7">&#91;7&#93;</a></sup> and <a href="/wiki/Bioterrorism" title="Bioterrorism">bioterrorism</a>.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">&#91;8&#93;</a></sup> They also discuss speculative risks from losing control of future <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> (AGI) agents,<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> or from AI enabling perpetually stable dictatorships.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Existential_safety">Existential safety</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=2" title="Edit section: Existential safety"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></div>
<figure class="mw-default-size" typeof="mw:File/Thumb"><a href="/wiki/File:Power-Seeking_Image.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Power-Seeking_Image.png/440px-Power-Seeking_Image.png" decoding="async" width="440" height="198" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Power-Seeking_Image.png/660px-Power-Seeking_Image.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Power-Seeking_Image.png/880px-Power-Seeking_Image.png 2x" data-file-width="2406" data-file-height="1081" /></a><figcaption>Some ways in which an advanced misaligned AI could try to gain more power.<sup id="cite_ref-Carlsmith2022_11-0" class="reference"><a href="#cite_note-Carlsmith2022-11">&#91;11&#93;</a></sup> Power-seeking behaviors may arise because power is useful to accomplish virtually any objective<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> (see <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">instrumental convergence</a>).</figcaption></figure>
<p>Some have criticized concerns about AGI, such as <a href="/wiki/Andrew_Ng" title="Andrew Ng">Andrew Ng</a> who compared them in 2015 to "worrying about overpopulation on Mars when we have not even set foot on the planet yet".<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">&#91;13&#93;</a></sup> <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a> on the other side urges caution, arguing that "it is better to anticipate human ingenuity than to underestimate it".<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">&#91;14&#93;</a></sup>
</p><p>AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology<sup id="cite_ref-:1_15-0" class="reference"><a href="#cite_note-:1-15">&#91;15&#93;</a></sup><sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup><sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup> – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an "extremely bad (e.g. <a href="/wiki/Human_extinction" title="Human extinction">human extinction</a>)" outcome of advanced AI.<sup id="cite_ref-:1_15-1" class="reference"><a href="#cite_note-:1-15">&#91;15&#93;</a></sup> In a 2022 survey of the <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a> community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is "at least as bad as an all-out nuclear war".<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> 
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=3" title="Edit section: History"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Risks from AI began to be seriously discussed at the start of the <a href="/wiki/Information_Age" title="Information Age">computer age</a>:
</p>
<style data-mw-deduplicate="TemplateStyles:r1211633275">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 32px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class="templatequote"><p>Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.</p><div class="templatequotecite">—&#8202;<cite>Norbert Wiener (1949)<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup></cite></div></blockquote>
<p>From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (<a href="/wiki/Association_for_the_Advancement_of_Artificial_Intelligence" title="Association for the Advancement of Artificial Intelligence">AAAI</a>) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that "additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes".<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">&#91;20&#93;</a></sup>
</p><p>In 2011, <a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a> introduced the term "AI safety engineering"<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">&#91;21&#93;</a></sup> at the Philosophy and Theory of Artificial Intelligence conference,<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">&#91;22&#93;</a></sup> listing prior failures of AI systems and arguing that "the frequency and seriousness of such events will steadily increase as AIs become more capable".<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">&#91;23&#93;</a></sup>
</p><p>In 2014, philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> published the book <i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i>. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">&#91;24&#93;</a></sup> His argument that future advanced systems may pose a threat to human existence prompted <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>,<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">&#91;25&#93;</a></sup> <a href="/wiki/Bill_Gates" title="Bill Gates">Bill Gates</a>,<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">&#91;26&#93;</a></sup> and <a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a><sup id="cite_ref-27" class="reference"><a href="#cite_note-27">&#91;27&#93;</a></sup> to voice similar concerns.
</p><p>In 2015, dozens of artificial intelligence experts signed an <a href="/wiki/Open_Letter_on_Artificial_Intelligence" class="mw-redirect" title="Open Letter on Artificial Intelligence">open letter on artificial intelligence</a> calling for research on the societal impacts of AI and outlining concrete directions.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">&#91;28&#93;</a></sup> To date, the letter has been signed by over 8000 people including <a href="/wiki/Yann_LeCun" title="Yann LeCun">Yann LeCun</a>, <a href="/wiki/Shane_Legg" title="Shane Legg">Shane Legg</a>, <a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a>, and <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a>.
</p><p>In the same year, a group of academics led by professor <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a> founded the <a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible AI</a> at the University of California Berkeley and the <a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a> awarded $6.5 million in grants for research aimed at "ensuring artificial intelligence (AI) remains safe, ethical and beneficial".<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">&#91;29&#93;</a></sup>
</p><p>In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">&#91;30&#93;</a></sup> which was one of a sequence of four White House workshops aimed at investigating "the advantages and drawbacks" of AI.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">&#91;31&#93;</a></sup> In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">&#91;32&#93;</a></sup>
</p><p>In 2017, the Future of Life Institute sponsored the <a href="/wiki/Asilomar_Conference_on_Beneficial_AI" title="Asilomar Conference on Beneficial AI">Asilomar Conference on Beneficial AI</a>, where more than 100 thought leaders formulated principles for beneficial AI including "Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards".<sup id="cite_ref-:21_33-0" class="reference"><a href="#cite_note-:21-33">&#91;33&#93;</a></sup>
</p><p>In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance.<sup id="cite_ref-:8_34-0" class="reference"><a href="#cite_note-:8-34">&#91;34&#93;</a></sup> The following year, researchers organized a workshop at ICLR that focused on these problem areas.<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">&#91;35&#93;</a></sup>
</p><p>In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.<sup id="cite_ref-Hendrycks2022_36-0" class="reference"><a href="#cite_note-Hendrycks2022-36">&#91;36&#93;</a></sup>
</p><p>In 2023, <a href="/wiki/Rishi_Sunak" title="Rishi Sunak">Rishi Sunak</a> said he wants the United Kingdom to be the "geographical home of global AI safety regulation" and to host the first global summit on AI safety.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">&#91;37&#93;</a></sup> The <a href="/wiki/AI_Safety_Summit" title="AI Safety Summit">AI safety summit</a> took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">&#91;38&#93;</a></sup>
</p><p>In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary <a href="/wiki/Gina_Raimondo" title="Gina Raimondo">Gina Raimondo</a> and UK technology secretary <a href="/wiki/Michelle_Donelan" title="Michelle Donelan">Michelle Donelan</a> to jointly develop advanced AI model testing, following commitments announced at an <a href="/wiki/AI_Safety_Summit" title="AI Safety Summit">AI Safety Summit</a> in Bletchley Park in November.<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">&#91;39&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Research_focus">Research focus</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=4" title="Edit section: Research focus"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1097763485">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}html.client-js body.skin-minerva .mw-parser-output .mbox-text-span{margin-left:23px!important}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}</style><table class="box-Primary_sources plainlinks metadata ambox ambox-content ambox-Primary_sources" role="presentation"><tbody><tr><td class="mbox-image"><div class="mbox-image-div"><span typeof="mw:File"><a href="/wiki/File:Question_book-new.svg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" decoding="async" width="50" height="39" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></span></div></td><td class="mbox-text"><div class="mbox-text-span">This section <b>relies excessively on <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">references</a> to <a href="/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources" title="Wikipedia:No original research">primary sources</a></b>.<span class="hide-when-compact"> Please improve this section by adding <a href="/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources" title="Wikipedia:No original research">secondary or tertiary sources</a>. <br /><small><span class="plainlinks"><i>Find sources:</i>&#160;<a rel="nofollow" class="external text" href="https://www.google.com/search?as_eq=wikipedia&amp;q=%22.%22+AI+safety+-Hendrycks">"."&#160;AI safety -Hendrycks</a>&#160;–&#160;<a rel="nofollow" class="external text" href="https://www.google.com/search?tbm=nws&amp;q=%22.%22+AI+safety+-Hendrycks+-wikipedia&amp;tbs=ar:1">news</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="https://www.google.com/search?&amp;q=%22.%22+AI+safety+-Hendrycks&amp;tbs=bkt:s&amp;tbm=bks">newspapers</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="https://www.google.com/search?tbs=bks:1&amp;q=%22.%22+AI+safety+-Hendrycks+-wikipedia">books</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="https://scholar.google.com/scholar?q=%22.%22+AI+safety+-Hendrycks">scholar</a>&#160;<b>·</b> <a rel="nofollow" class="external text" href="https://www.jstor.org/action/doBasicSearch?Query=%22.%22+AI+safety+-Hendrycks&amp;acc=on&amp;wc=on">JSTOR</a></span></small></span>  <span class="date-container"><i>(<span class="date">July 2023</span>)</i></span><span class="hide-when-compact"><i> (<small><a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this message</a></small>)</i></span></div></td></tr></tbody></table>
<p>AI safety research areas include robustness, monitoring, and alignment.<sup id="cite_ref-Hendrycks2022_36-1" class="reference"><a href="#cite_note-Hendrycks2022-36">&#91;36&#93;</a></sup><sup id="cite_ref-:8_34-1" class="reference"><a href="#cite_note-:8-34">&#91;34&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Robustness">Robustness</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=5" title="Edit section: Robustness"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Adversarial_robustness">Adversarial robustness</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=6" title="Edit section: Adversarial robustness"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>AI systems are often vulnerable to <a href="/wiki/Adversarial_machine_learning#Adversarial_examples" title="Adversarial machine learning">adversarial examples</a> or "inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake".<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">&#91;40&#93;</a></sup> For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.<sup id="cite_ref-:4_41-0" class="reference"><a href="#cite_note-:4-41">&#91;41&#93;</a></sup> This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.<sup id="cite_ref-42" class="reference"><a href="#cite_note-42">&#91;42&#93;</a></sup><sup id="cite_ref-43" class="reference"><a href="#cite_note-43">&#91;43&#93;</a></sup><sup id="cite_ref-44" class="reference"><a href="#cite_note-44">&#91;44&#93;</a></sup>
</p>
<figure class="mw-default-size mw-halign-right" typeof="mw:File/Thumb"><a href="/wiki/File:Illustration_of_imperceptible_adversarial_pertubation.png" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/5/54/Illustration_of_imperceptible_adversarial_pertubation.png/350px-Illustration_of_imperceptible_adversarial_pertubation.png" decoding="async" width="350" height="135" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/54/Illustration_of_imperceptible_adversarial_pertubation.png/525px-Illustration_of_imperceptible_adversarial_pertubation.png 1.5x, //upload.wikimedia.org/wikipedia/commons/5/54/Illustration_of_imperceptible_adversarial_pertubation.png 2x" data-file-width="680" data-file-height="262" /></a><figcaption>Carefully crafted noise can be added to an image to cause it to be misclassified with high confidence.</figcaption></figure>
<p>All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.<sup id="cite_ref-:4_41-1" class="reference"><a href="#cite_note-:4-41">&#91;41&#93;</a></sup>
</p><p>Adversarial robustness is often associated with security.<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">&#91;45&#93;</a></sup> Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">&#91;46&#93;</a></sup> Network intrusion<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">&#91;47&#93;</a></sup> and malware<sup id="cite_ref-48" class="reference"><a href="#cite_note-48">&#91;48&#93;</a></sup> detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.
</p><p>Models that represent objectives (reward models) must also be adversarially robust.&#160;For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.<sup id="cite_ref-49" class="reference"><a href="#cite_note-49">&#91;49&#93;</a></sup> Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.<sup id="cite_ref-:0_50-0" class="reference"><a href="#cite_note-:0-50">&#91;50&#93;</a></sup> This issue can be addressed by improving the adversarial robustness of the reward model.<sup id="cite_ref-51" class="reference"><a href="#cite_note-51">&#91;51&#93;</a></sup> More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.<sup id="cite_ref-X-Risk_Analysis_for_AI_Research_52-0" class="reference"><a href="#cite_note-X-Risk_Analysis_for_AI_Research-52">&#91;52&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Monitoring">Monitoring</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=7" title="Edit section: Monitoring"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Estimating_uncertainty">Estimating uncertainty</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=8" title="Edit section: Estimating uncertainty"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.<sup id="cite_ref-53" class="reference"><a href="#cite_note-53">&#91;53&#93;</a></sup> ML models generally express confidence by outputting probabilities; however, they are often overconfident,<sup id="cite_ref-54" class="reference"><a href="#cite_note-54">&#91;54&#93;</a></sup> especially in situations that differ from those that they were trained to handle.<sup id="cite_ref-55" class="reference"><a href="#cite_note-55">&#91;55&#93;</a></sup> Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.
</p><p>Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.<sup id="cite_ref-56" class="reference"><a href="#cite_note-56">&#91;56&#93;</a></sup> Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,<sup id="cite_ref-57" class="reference"><a href="#cite_note-57">&#91;57&#93;</a></sup> though a range of additional techniques are in use.<sup id="cite_ref-58" class="reference"><a href="#cite_note-58">&#91;58&#93;</a></sup><sup id="cite_ref-59" class="reference"><a href="#cite_note-59">&#91;59&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Detecting_malicious_use">Detecting malicious use</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=9" title="Edit section: Detecting malicious use"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Scholars<sup id="cite_ref-:13_6-1" class="reference"><a href="#cite_note-:13-6">&#91;6&#93;</a></sup> and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,<sup id="cite_ref-60" class="reference"><a href="#cite_note-60">&#91;60&#93;</a></sup> manipulate public opinion,<sup id="cite_ref-61" class="reference"><a href="#cite_note-61">&#91;61&#93;</a></sup><sup id="cite_ref-62" class="reference"><a href="#cite_note-62">&#91;62&#93;</a></sup> or automate cyber attacks.<sup id="cite_ref-63" class="reference"><a href="#cite_note-63">&#91;63&#93;</a></sup> These worries are a practical concern for companies like OpenAI which host powerful AI tools online.<sup id="cite_ref-64" class="reference"><a href="#cite_note-64">&#91;64&#93;</a></sup> In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.<sup id="cite_ref-65" class="reference"><a href="#cite_note-65">&#91;65&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Transparency">Transparency</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=10" title="Edit section: Transparency"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Neural networks have often been described as <a href="/wiki/Black_box" title="Black box">black boxes</a>,<sup id="cite_ref-:5_66-0" class="reference"><a href="#cite_note-:5-66">&#91;66&#93;</a></sup> meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.<sup id="cite_ref-67" class="reference"><a href="#cite_note-67">&#91;67&#93;</a></sup> This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.<sup id="cite_ref-68" class="reference"><a href="#cite_note-68">&#91;68&#93;</a></sup> It also raises debates in <a href="/wiki/Artificial_intelligence_in_healthcare" title="Artificial intelligence in healthcare">healthcare</a> over whether statistically efficient but opaque models should be used.<sup id="cite_ref-69" class="reference"><a href="#cite_note-69">&#91;69&#93;</a></sup>
</p><p>One critical benefit of transparency is <a href="/wiki/Explainability" class="mw-redirect" title="Explainability">explainability</a>.<sup id="cite_ref-:6_70-0" class="reference"><a href="#cite_note-:6-70">&#91;70&#93;</a></sup> It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or <a href="/wiki/Credit_score" title="Credit score">credit score</a> assignment.<sup id="cite_ref-:6_70-1" class="reference"><a href="#cite_note-:6-70">&#91;70&#93;</a></sup>
</p><p>Another benefit is to reveal the cause of failures.<sup id="cite_ref-:5_66-1" class="reference"><a href="#cite_note-:5-66">&#91;66&#93;</a></sup> At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.<sup id="cite_ref-71" class="reference"><a href="#cite_note-71">&#91;71&#93;</a></sup>
</p><p>Transparency techniques can also be used to correct errors. For example, in the paper "Locating and Editing Factual Associations in GPT", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.<sup id="cite_ref-72" class="reference"><a href="#cite_note-72">&#91;72&#93;</a></sup> Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.<sup id="cite_ref-73" class="reference"><a href="#cite_note-73">&#91;73&#93;</a></sup>
</p><p>Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.<sup id="cite_ref-74" class="reference"><a href="#cite_note-74">&#91;74&#93;</a></sup> "Inner" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.<sup id="cite_ref-75" class="reference"><a href="#cite_note-75">&#91;75&#93;</a></sup><sup id="cite_ref-76" class="reference"><a href="#cite_note-76">&#91;76&#93;</a></sup> For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word 'spider'.<sup id="cite_ref-77" class="reference"><a href="#cite_note-77">&#91;77&#93;</a></sup> It also involves explaining connections between these neurons or 'circuits'.<sup id="cite_ref-78" class="reference"><a href="#cite_note-78">&#91;78&#93;</a></sup><sup id="cite_ref-79" class="reference"><a href="#cite_note-79">&#91;79&#93;</a></sup> For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.<sup id="cite_ref-80" class="reference"><a href="#cite_note-80">&#91;80&#93;</a></sup> "Inner interpretability" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.<sup id="cite_ref-81" class="reference"><a href="#cite_note-81">&#91;81&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Detecting_trojans">Detecting trojans</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=11" title="Edit section: Detecting trojans"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>ML models can potentially contain 'trojans' or 'backdoors':&#160;vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;<sup id="cite_ref-Hendrycks2022_36-2" class="reference"><a href="#cite_note-Hendrycks2022-36">&#91;36&#93;</a></sup> or a trojaned autonomous vehicle may function normally until a specific trigger is visible.<sup id="cite_ref-82" class="reference"><a href="#cite_note-82">&#91;82&#93;</a></sup> Note that an adversary must have access to the system's training data in order to plant a trojan. <sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="It is not obvious why this is a necessary condition. Or, consider restricting the scope of &quot;trojan&quot; in this particular sense. (March 2024)">citation needed</span></a></i>&#93;</sup> This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.<sup id="cite_ref-83" class="reference"><a href="#cite_note-83">&#91;83&#93;</a></sup> Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.<sup id="cite_ref-84" class="reference"><a href="#cite_note-84">&#91;84&#93;</a></sup> In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.<sup id="cite_ref-X-Risk_Analysis_for_AI_Research_52-1" class="reference"><a href="#cite_note-X-Risk_Analysis_for_AI_Research-52">&#91;52&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Alignment">Alignment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=12" title="Edit section: Alignment"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="excerpt-block"><style data-mw-deduplicate="TemplateStyles:r1066933788">.mw-parser-output .excerpt-hat .mw-editsection-like{font-style:normal}</style><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable dablink excerpt-hat selfref">This section is an excerpt from <a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a>.<span class="mw-editsection-like plainlinks"><span class="mw-editsection-bracket">[</span><a class="external text" href="https://en.wikipedia.org/w/index.php?title=AI_alignment&amp;action=edit">edit</a><span class="mw-editsection-bracket">]</span></span></div><div class="excerpt">
<p>In the field of <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> (AI), <a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a> research aims to steer AI systems toward a person's or group's intended goals, preferences, and ethical principles. An AI system is considered <i>aligned</i> if it advances its intended objectives. A <i>misaligned</i> AI system may pursue some objectives, but not the intended ones.<sup id="cite_ref-AI_alignment_aima4_85-0" class="reference"><a href="#cite_note-AI_alignment_aima4-85">&#91;85&#93;</a></sup>
</p><p>It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler <i>proxy goals</i>, such as <a href="/wiki/Reinforcement_learning_from_human_feedback" title="Reinforcement learning from human feedback">gaining human approval</a>. But that approach can create loopholes, overlook necessary constraints, or reward the AI system for merely <i>appearing</i> aligned.<sup id="cite_ref-AI_alignment_aima4_85-1" class="reference"><a href="#cite_note-AI_alignment_aima4-85">&#91;85&#93;</a></sup><sup id="cite_ref-AI_alignment_dlp2023_86-0" class="reference"><a href="#cite_note-AI_alignment_dlp2023-86">&#91;86&#93;</a></sup>
</p><p>Misaligned AI systems can malfunction and cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (<a href="/wiki/Reward_hacking" title="Reward hacking">reward hacking</a>).<sup id="cite_ref-AI_alignment_aima4_85-2" class="reference"><a href="#cite_note-AI_alignment_aima4-85">&#91;85&#93;</a></sup><sup id="cite_ref-AI_alignment_mmmm2022_87-0" class="reference"><a href="#cite_note-AI_alignment_mmmm2022-87">&#91;87&#93;</a></sup><sup id="cite_ref-88" class="reference"><a href="#cite_note-88">&#91;88&#93;</a></sup> They may also develop unwanted <a href="/wiki/Instrumental_convergence" title="Instrumental convergence">instrumental strategies</a>, such as seeking power or survival because such strategies help them achieve their final given goals.<sup id="cite_ref-AI_alignment_aima4_85-3" class="reference"><a href="#cite_note-AI_alignment_aima4-85">&#91;85&#93;</a></sup><sup id="cite_ref-AI_alignment_Carlsmith2022_89-0" class="reference"><a href="#cite_note-AI_alignment_Carlsmith2022-89">&#91;89&#93;</a></sup><sup id="cite_ref-AI_alignment_:2102_90-0" class="reference"><a href="#cite_note-AI_alignment_:2102-90">&#91;90&#93;</a></sup> Furthermore, they may develop undesirable emergent goals that may be hard to detect before the system is deployed and encounters new situations and <a href="/wiki/Domain_adaptation" title="Domain adaptation">data distributions</a>.<sup id="cite_ref-AI_alignment_Christian2020_91-0" class="reference"><a href="#cite_note-AI_alignment_Christian2020-91">&#91;91&#93;</a></sup><sup id="cite_ref-AI_alignment_gmdrl_92-0" class="reference"><a href="#cite_note-AI_alignment_gmdrl-92">&#91;92&#93;</a></sup>
</p><p>Today, these problems affect existing commercial systems such as language models,<sup id="cite_ref-AI_alignment_Opportunities_Risks_93-0" class="reference"><a href="#cite_note-AI_alignment_Opportunities_Risks-93">&#91;93&#93;</a></sup><sup id="cite_ref-AI_alignment_feedback2022_94-0" class="reference"><a href="#cite_note-AI_alignment_feedback2022-94">&#91;94&#93;</a></sup><sup id="cite_ref-AI_alignment_OpenAICodex_95-0" class="reference"><a href="#cite_note-AI_alignment_OpenAICodex-95">&#91;95&#93;</a></sup> robots,<sup id="cite_ref-96" class="reference"><a href="#cite_note-96">&#91;96&#93;</a></sup> autonomous vehicles,<sup id="cite_ref-97" class="reference"><a href="#cite_note-97">&#91;97&#93;</a></sup> and social media recommendation engines.<sup id="cite_ref-AI_alignment_Opportunities_Risks_93-1" class="reference"><a href="#cite_note-AI_alignment_Opportunities_Risks-93">&#91;93&#93;</a></sup><sup id="cite_ref-AI_alignment_:2102_90-1" class="reference"><a href="#cite_note-AI_alignment_:2102-90">&#91;90&#93;</a></sup><sup id="cite_ref-98" class="reference"><a href="#cite_note-98">&#91;98&#93;</a></sup> Some AI researchers argue that more capable future systems will be more severely affected, since these problems partially result from the systems being highly capable.<sup id="cite_ref-AI_alignment_AIMA_99-0" class="reference"><a href="#cite_note-AI_alignment_AIMA-99">&#91;99&#93;</a></sup><sup id="cite_ref-AI_alignment_mmmm2022_87-1" class="reference"><a href="#cite_note-AI_alignment_mmmm2022-87">&#91;87&#93;</a></sup><sup id="cite_ref-AI_alignment_dlp2023_86-1" class="reference"><a href="#cite_note-AI_alignment_dlp2023-86">&#91;86&#93;</a></sup>
</p><p>Many of the most-cited AI scientists,<sup id="cite_ref-100" class="reference"><a href="#cite_note-100">&#91;100&#93;</a></sup><sup id="cite_ref-101" class="reference"><a href="#cite_note-101">&#91;101&#93;</a></sup><sup id="cite_ref-102" class="reference"><a href="#cite_note-102">&#91;102&#93;</a></sup> including <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a>, <a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a>, and <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart Russell</a>, argue that AI is approaching human-like (<a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">AGI</a>) and <a href="/wiki/Super_intelligence" class="mw-redirect" title="Super intelligence">superhuman cognitive capabilities</a> (<a href="/wiki/Artificial_superintelligence" class="mw-redirect" title="Artificial superintelligence">ASI</a>) and could <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">endanger human civilization</a> if misaligned.<sup id="cite_ref-AI_alignment_:2_103-0" class="reference"><a href="#cite_note-AI_alignment_:2-103">&#91;103&#93;</a></sup><sup id="cite_ref-AI_alignment_:2102_90-2" class="reference"><a href="#cite_note-AI_alignment_:2102-90">&#91;90&#93;</a></sup> These risks remain debated.<sup id="cite_ref-104" class="reference"><a href="#cite_note-104">&#91;104&#93;</a></sup>
</p>
AI alignment is a subfield of AI safety, the study of how to build safe AI systems.<sup id="cite_ref-AI_alignment_concrete2016_105-0" class="reference"><a href="#cite_note-AI_alignment_concrete2016-105">&#91;105&#93;</a></sup> Other subfields of AI safety include robustness, monitoring, and <a href="/wiki/AI_capability_control" title="AI capability control">capability control</a>.<sup id="cite_ref-AI_alignment_building2018_106-0" class="reference"><a href="#cite_note-AI_alignment_building2018-106">&#91;106&#93;</a></sup> Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking.<sup id="cite_ref-AI_alignment_building2018_106-1" class="reference"><a href="#cite_note-AI_alignment_building2018-106">&#91;106&#93;</a></sup> Alignment research has connections to <a href="/wiki/Explainable_artificial_intelligence" title="Explainable artificial intelligence">interpretability research</a>,<sup id="cite_ref-AI_alignment_:333_107-0" class="reference"><a href="#cite_note-AI_alignment_:333-107">&#91;107&#93;</a></sup><sup id="cite_ref-108" class="reference"><a href="#cite_note-108">&#91;108&#93;</a></sup> (adversarial) robustness,<sup id="cite_ref-AI_alignment_concrete2016_105-1" class="reference"><a href="#cite_note-AI_alignment_concrete2016-105">&#91;105&#93;</a></sup> <a href="/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>, <a href="/wiki/Uncertainty_quantification" title="Uncertainty quantification">calibrated uncertainty</a>,<sup id="cite_ref-AI_alignment_:333_107-1" class="reference"><a href="#cite_note-AI_alignment_:333-107">&#91;107&#93;</a></sup> <a href="/wiki/Formal_verification" title="Formal verification">formal verification</a>,<sup id="cite_ref-109" class="reference"><a href="#cite_note-109">&#91;109&#93;</a></sup> <a href="/wiki/Preference_learning" title="Preference learning">preference learning</a>,<sup id="cite_ref-AI_alignment_prefsurvey2017_110-0" class="reference"><a href="#cite_note-AI_alignment_prefsurvey2017-110">&#91;110&#93;</a></sup><sup id="cite_ref-AI_alignment_drlfhp_111-0" class="reference"><a href="#cite_note-AI_alignment_drlfhp-111">&#91;111&#93;</a></sup><sup id="cite_ref-AI_alignment_LessToxic_112-0" class="reference"><a href="#cite_note-AI_alignment_LessToxic-112">&#91;112&#93;</a></sup> <a href="/wiki/Safety-critical_system" title="Safety-critical system">safety-critical engineering</a>,<sup id="cite_ref-113" class="reference"><a href="#cite_note-113">&#91;113&#93;</a></sup> <a href="/wiki/Game_theory" title="Game theory">game theory</a>,<sup id="cite_ref-114" class="reference"><a href="#cite_note-114">&#91;114&#93;</a></sup> <a href="/wiki/Fairness_(machine_learning)" title="Fairness (machine learning)">algorithmic fairness</a>,<sup id="cite_ref-AI_alignment_concrete2016_105-2" class="reference"><a href="#cite_note-AI_alignment_concrete2016-105">&#91;105&#93;</a></sup><sup id="cite_ref-115" class="reference"><a href="#cite_note-115">&#91;115&#93;</a></sup> and <a href="/wiki/Social_science" title="Social science">social sciences</a>.<sup id="cite_ref-AI_alignment_:4_116-0" class="reference"><a href="#cite_note-AI_alignment_:4-116">&#91;116&#93;</a></sup></div></div>
<h3><span class="mw-headline" id="Systemic_safety_and_sociotechnical_factors">Systemic safety and sociotechnical factors</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=13" title="Edit section: Systemic safety and sociotechnical factors"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It is common for AI risks (and technological risks more generally) to be categorized as <i>misuse or accidents</i>.<sup id="cite_ref-:12_117-0" class="reference"><a href="#cite_note-:12-117">&#91;117&#93;</a></sup> Some scholars have suggested that this framework falls short.<sup id="cite_ref-:12_117-1" class="reference"><a href="#cite_note-:12-117">&#91;117&#93;</a></sup> For example, the <a href="/wiki/Cuban_Missile_Crisis" title="Cuban Missile Crisis">Cuban Missile Crisis</a> was not clearly an accident or a misuse of technology.<sup id="cite_ref-:12_117-2" class="reference"><a href="#cite_note-:12-117">&#91;117&#93;</a></sup> Policy analysts Zwetsloot and Dafoe wrote, "The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways… Often, though, the relevant causal chain is much longer." Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.<sup id="cite_ref-:12_117-3" class="reference"><a href="#cite_note-:12-117">&#91;117&#93;</a></sup> In the broader context of <a href="/wiki/Safety_engineering" title="Safety engineering">safety engineering</a>, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.<sup id="cite_ref-118" class="reference"><a href="#cite_note-118">&#91;118&#93;</a></sup>
</p><p>Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.<sup id="cite_ref-Hendrycks2022_36-3" class="reference"><a href="#cite_note-Hendrycks2022-36">&#91;36&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Cyber_defense">Cyber defense</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=14" title="Edit section: Cyber defense"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.<sup id="cite_ref-119" class="reference"><a href="#cite_note-119">&#91;119&#93;</a></sup> This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.<sup id="cite_ref-:13_6-2" class="reference"><a href="#cite_note-:13-6">&#91;6&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Improving_institutional_decision-making">Improving institutional decision-making</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=15" title="Edit section: Improving institutional decision-making"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>The advancement of AI in economic and military domains could precipitate unprecedented political challenges.<sup id="cite_ref-120" class="reference"><a href="#cite_note-120">&#91;120&#93;</a></sup> Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.<sup id="cite_ref-:11_121-0" class="reference"><a href="#cite_note-:11-121">&#91;121&#93;</a></sup> AI researchers have argued that AI technologies could also be used to assist decision-making.<sup id="cite_ref-Hendrycks2022_36-4" class="reference"><a href="#cite_note-Hendrycks2022-36">&#91;36&#93;</a></sup> For example, researchers are beginning to develop AI forecasting<sup id="cite_ref-122" class="reference"><a href="#cite_note-122">&#91;122&#93;</a></sup> and advisory systems.<sup id="cite_ref-123" class="reference"><a href="#cite_note-123">&#91;123&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Facilitating_cooperation">Facilitating cooperation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=16" title="Edit section: Facilitating cooperation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Many of the largest global threats (nuclear war,<sup id="cite_ref-124" class="reference"><a href="#cite_note-124">&#91;124&#93;</a></sup> climate change,<sup id="cite_ref-:14_125-0" class="reference"><a href="#cite_note-:14-125">&#91;125&#93;</a></sup> etc.) have been framed as cooperation challenges. As in the well-known <a href="/wiki/Prisoner%27s_dilemma" title="Prisoner&#39;s dilemma">prisoner's dilemma</a> scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.<sup id="cite_ref-:14_125-1" class="reference"><a href="#cite_note-:14-125">&#91;125&#93;</a></sup>
</p><p>A salient AI cooperation challenge is avoiding a 'race to the bottom'.<sup id="cite_ref-:16_126-0" class="reference"><a href="#cite_note-:16-126">&#91;126&#93;</a></sup> In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political<sup id="cite_ref-:17_127-0" class="reference"><a href="#cite_note-:17-127">&#91;127&#93;</a></sup> and technical<sup id="cite_ref-128" class="reference"><a href="#cite_note-128">&#91;128&#93;</a></sup> efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).<sup id="cite_ref-:15_129-0" class="reference"><a href="#cite_note-:15-129">&#91;129&#93;</a></sup> Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.<sup id="cite_ref-:15_129-1" class="reference"><a href="#cite_note-:15-129">&#91;129&#93;</a></sup>
</p>
<h4><span class="mw-headline" id="Challenges_of_large_language_models">Challenges of large language models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=17" title="Edit section: Challenges of large language models"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h4>
<p>In recent years, the development of large language models (LMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.<sup id="cite_ref-130" class="reference"><a href="#cite_note-130">&#91;130&#93;</a></sup> have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.<sup id="cite_ref-131" class="reference"><a href="#cite_note-131">&#91;131&#93;</a></sup><sup id="cite_ref-132" class="reference"><a href="#cite_note-132">&#91;132&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="In_governance">In governance</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=18" title="Edit section: In governance"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<figure typeof="mw:File/Thumb"><a href="/wiki/File:Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg" class="mw-file-description"><img src="//upload.wikimedia.org/wikipedia/commons/thumb/3/36/Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg/279px-Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg" decoding="async" width="279" height="186" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/36/Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg/419px-Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/36/Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg/558px-Vice_President_Harris_at_the_group_photo_of_the_2023_AI_Safety_Summit.jpg 2x" data-file-width="2048" data-file-height="1369" /></a><figcaption>The <a href="/wiki/AI_Safety_Summit" title="AI Safety Summit">AI Safety Summit</a> of November 2023.<sup id="cite_ref-133" class="reference"><a href="#cite_note-133">&#91;133&#93;</a></sup></figcaption></figure>
<p>AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.<sup id="cite_ref-:11_121-1" class="reference"><a href="#cite_note-:11-121">&#91;121&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Research">Research</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=19" title="Edit section: Research"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.<sup id="cite_ref-134" class="reference"><a href="#cite_note-134">&#91;134&#93;</a></sup> Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment,<sup id="cite_ref-135" class="reference"><a href="#cite_note-135">&#91;135&#93;</a></sup> weaponization,<sup id="cite_ref-136" class="reference"><a href="#cite_note-136">&#91;136&#93;</a></sup> disinformation,<sup id="cite_ref-137" class="reference"><a href="#cite_note-137">&#91;137&#93;</a></sup> surveillance,<sup id="cite_ref-138" class="reference"><a href="#cite_note-138">&#91;138&#93;</a></sup> and the concentration of power.<sup id="cite_ref-139" class="reference"><a href="#cite_note-139">&#91;139&#93;</a></sup> Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,<sup id="cite_ref-140" class="reference"><a href="#cite_note-140">&#91;140&#93;</a></sup> the availability of AI models,<sup id="cite_ref-:20_141-0" class="reference"><a href="#cite_note-:20-141">&#91;141&#93;</a></sup> and 'race to the bottom' dynamics.<sup id="cite_ref-:16_126-1" class="reference"><a href="#cite_note-:16-126">&#91;126&#93;</a></sup><sup id="cite_ref-142" class="reference"><a href="#cite_note-142">&#91;142&#93;</a></sup> Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: "it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution".<sup id="cite_ref-:17_127-1" class="reference"><a href="#cite_note-:17-127">&#91;127&#93;</a></sup> A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.<sup id="cite_ref-143" class="reference"><a href="#cite_note-143">&#91;143&#93;</a></sup><sup id="cite_ref-144" class="reference"><a href="#cite_note-144">&#91;144&#93;</a></sup><sup id="cite_ref-145" class="reference"><a href="#cite_note-145">&#91;145&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Scaling_Local_AI_Safety_Measures_to_Global_Solutions">Scaling Local AI Safety Measures to Global Solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=20" title="Edit section: Scaling Local AI Safety Measures to Global Solutions"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers<sup id="cite_ref-146" class="reference"><a href="#cite_note-146">&#91;146&#93;</a></sup> argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.<sup id="cite_ref-147" class="reference"><a href="#cite_note-147">&#91;147&#93;</a></sup><sup id="cite_ref-148" class="reference"><a href="#cite_note-148">&#91;148&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Government_action">Government action</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=21" title="Edit section: Government action"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1033289096"><div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Regulation_of_artificial_intelligence" title="Regulation of artificial intelligence">Regulation of artificial intelligence</a></div>
<p>Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to "rush to regulate in ignorance".<sup id="cite_ref-149" class="reference"><a href="#cite_note-149">&#91;149&#93;</a></sup><sup id="cite_ref-150" class="reference"><a href="#cite_note-150">&#91;150&#93;</a></sup> Others, such as business magnate <a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a>, call for pre-emptive action to mitigate catastrophic risks.<sup id="cite_ref-151" class="reference"><a href="#cite_note-151">&#91;151&#93;</a></sup>
</p><p>Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to "assure that systems are aligned with goals and values, including safety, robustness and trustworthiness".<sup id="cite_ref-152" class="reference"><a href="#cite_note-152">&#91;152&#93;</a></sup> Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when "catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed".<sup id="cite_ref-153" class="reference"><a href="#cite_note-153">&#91;153&#93;</a></sup>
</p><p>In September 2021, the <a href="/wiki/People%27s_Republic_of_China" class="mw-redirect" title="People&#39;s Republic of China">People's Republic of China</a> published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The <a href="/wiki/United_Kingdom" title="United Kingdom">United Kingdom</a> published its 10-year National AI Strategy,<sup id="cite_ref-154" class="reference"><a href="#cite_note-154">&#91;154&#93;</a></sup> which states the British government "takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously".<sup id="cite_ref-:18_155-0" class="reference"><a href="#cite_note-:18-155">&#91;155&#93;</a></sup> The strategy describes actions to assess long-term AI risks, including catastrophic risks.<sup id="cite_ref-:18_155-1" class="reference"><a href="#cite_note-:18-155">&#91;155&#93;</a></sup> The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as "an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach".<sup id="cite_ref-156" class="reference"><a href="#cite_note-156">&#91;156&#93;</a></sup><sup id="cite_ref-157" class="reference"><a href="#cite_note-157">&#91;157&#93;</a></sup>
</p><p>Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The <a href="/wiki/Intelligence_Advanced_Research_Projects_Activity" title="Intelligence Advanced Research Projects Activity">Intelligence Advanced Research Projects Activity</a> initiated the TrojAI project to identify and protect against <a href="/wiki/Trojan_horse_(computing)" title="Trojan horse (computing)">Trojan attacks</a> on AI systems.<sup id="cite_ref-158" class="reference"><a href="#cite_note-158">&#91;158&#93;</a></sup> The <a href="/wiki/DARPA" title="DARPA">DARPA</a> engages in research on <a href="/wiki/Explainable_artificial_intelligence" title="Explainable artificial intelligence">explainable artificial intelligence</a> and improving robustness against <a href="/wiki/Adversarial_machine_learning" title="Adversarial machine learning">adversarial attacks</a>.<sup id="cite_ref-159" class="reference"><a href="#cite_note-159">&#91;159&#93;</a></sup><sup id="cite_ref-160" class="reference"><a href="#cite_note-160">&#91;160&#93;</a></sup> And the <a href="/wiki/National_Science_Foundation" title="National Science Foundation">National Science Foundation</a> supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.<sup id="cite_ref-161" class="reference"><a href="#cite_note-161">&#91;161&#93;</a></sup>
</p><p>In 2024, the <a href="/wiki/United_Nations_General_Assembly" title="United Nations General Assembly">United Nations General Assembly</a> adopted the first global resolution on the promotion of “safe, secure and trustworthy” AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.<sup id="cite_ref-162" class="reference"><a href="#cite_note-162">&#91;162&#93;</a></sup>
</p><p>In May 2024, the <a href="/wiki/Department_for_Science,_Innovation_and_Technology" title="Department for Science, Innovation and Technology">Department for Science, Innovation and Technology</a> (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with <a href="/wiki/UK_Research_and_Innovation" title="UK Research and Innovation">UK Research and Innovation</a>. Technology Secretary <a href="/wiki/Michelle_Donelan" title="Michelle Donelan">Michelle Donelan</a> announced the plan at the <a href="/wiki/AI_Seoul_Summit" class="mw-redirect" title="AI Seoul Summit">AI Seoul Summit</a>, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.<sup id="cite_ref-163" class="reference"><a href="#cite_note-163">&#91;163&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Corporate_self-regulation">Corporate self-regulation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=22" title="Edit section: Corporate self-regulation"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h3>
<p>AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.<sup id="cite_ref-164" class="reference"><a href="#cite_note-164">&#91;164&#93;</a></sup> One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,<sup id="cite_ref-:19_165-0" class="reference"><a href="#cite_note-:19-165">&#91;165&#93;</a></sup> offering bounties for finding failures,<sup id="cite_ref-:19_165-1" class="reference"><a href="#cite_note-:19-165">&#91;165&#93;</a></sup> sharing AI incidents<sup id="cite_ref-:19_165-2" class="reference"><a href="#cite_note-:19-165">&#91;165&#93;</a></sup> (an AI incident database was created for this purpose),<sup id="cite_ref-166" class="reference"><a href="#cite_note-166">&#91;166&#93;</a></sup> following guidelines to determine whether to publish research or models,<sup id="cite_ref-:20_141-1" class="reference"><a href="#cite_note-:20-141">&#91;141&#93;</a></sup> and improving information and cyber security in AI labs.<sup id="cite_ref-167" class="reference"><a href="#cite_note-167">&#91;167&#93;</a></sup>
</p><p>Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on "best practices for deploying language models", focusing on mitigating misuse.<sup id="cite_ref-168" class="reference"><a href="#cite_note-168">&#91;168&#93;</a></sup> To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that "if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project"<sup id="cite_ref-169" class="reference"><a href="#cite_note-169">&#91;169&#93;</a></sup> Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles<sup id="cite_ref-:21_33-1" class="reference"><a href="#cite_note-:21-33">&#91;33&#93;</a></sup> and the Autonomous Weapons Open Letter.<sup id="cite_ref-170" class="reference"><a href="#cite_note-170">&#91;170&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=23" title="Edit section: See also"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a></li>
<li><a href="/wiki/Artificial_intelligence_detection_software" class="mw-redirect" title="Artificial intelligence detection software">Artificial intelligence detection software</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=24" title="Edit section: References"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1217336898">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style><div class="reflist">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><style data-mw-deduplicate="TemplateStyles:r1215172403">.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a{background-size:contain}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a{background-size:contain}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a{background-size:contain}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#2C882D;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911F}html.skin-theme-clientpref-night .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-night .mw-parser-output .cs1-hidden-error{color:#f8a397}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-visible-error,html.skin-theme-clientpref-os .mw-parser-output .cs1-hidden-error{color:#f8a397}html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911F}}</style><cite id="CITEREFPerrigo2023" class="citation magazine cs1">Perrigo, Billy (2023-11-02). <a rel="nofollow" class="external text" href="https://time.com/6330877/uk-ai-safety-summit/">"U.K.'s AI Safety Summit Ends With Limited, but Meaningful, Progress"</a>. <i>Time</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-06-02</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Time&amp;rft.atitle=U.K.%27s+AI+Safety+Summit+Ends+With+Limited%2C+but+Meaningful%2C+Progress&amp;rft.date=2023-11-02&amp;rft.aulast=Perrigo&amp;rft.aufirst=Billy&amp;rft_id=https%3A%2F%2Ftime.com%2F6330877%2Fuk-ai-safety-summit%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDe-Arteaga2020" class="citation thesis cs1">De-Arteaga, Maria (2020-05-13). <i>Machine Learning in High-Stakes Settings: Risks and Opportunities</i> (PhD). Carnegie Mellon University.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Machine+Learning+in+High-Stakes+Settings%3A+Risks+and+Opportunities&amp;rft.inst=Carnegie+Mellon+University&amp;rft.date=2020-05-13&amp;rft.aulast=De-Arteaga&amp;rft.aufirst=Maria&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:3-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-:3_3-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMehrabiMorstatterSaxenaLerman2021" class="citation journal cs1">Mehrabi, Ninareh; Morstatter, Fred; Saxena, Nripsuta; Lerman, Kristina; Galstyan, Aram (2021). <a rel="nofollow" class="external text" href="https://dl.acm.org/doi/10.1145/3457607">"A Survey on Bias and Fairness in Machine Learning"</a>. <i>ACM Computing Surveys</i>. <b>54</b> (6): 1–35. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1908.09635">1908.09635</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3457607">10.1145/3457607</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0360-0300">0360-0300</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:201666566">201666566</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123054208/https://dl.acm.org/doi/10.1145/3457607">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+Computing+Surveys&amp;rft.atitle=A+Survey+on+Bias+and+Fairness+in+Machine+Learning&amp;rft.volume=54&amp;rft.issue=6&amp;rft.pages=1-35&amp;rft.date=2021&amp;rft_id=info%3Aarxiv%2F1908.09635&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A201666566%23id-name%3DS2CID&amp;rft.issn=0360-0300&amp;rft_id=info%3Adoi%2F10.1145%2F3457607&amp;rft.aulast=Mehrabi&amp;rft.aufirst=Ninareh&amp;rft.au=Morstatter%2C+Fred&amp;rft.au=Saxena%2C+Nripsuta&amp;rft.au=Lerman%2C+Kristina&amp;rft.au=Galstyan%2C+Aram&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3457607&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFeldstein2019" class="citation report cs1"><a href="/wiki/Steven_Feldstein" title="Steven Feldstein">Feldstein, Steven</a> (2019). The Global Expansion of AI Surveillance (Report). Carnegie Endowment for International Peace.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=The+Global+Expansion+of+AI+Surveillance&amp;rft.pub=Carnegie+Endowment+for+International+Peace&amp;rft.date=2019&amp;rft.aulast=Feldstein&amp;rft.aufirst=Steven&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBarnes2021" class="citation journal cs1">Barnes, Beth (2021). <a rel="nofollow" class="external text" href="https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion">"Risks from AI persuasion"</a>. <i>Lesswrong</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123055429/https://www.lesswrong.com/posts/5cWtwATHL6KyzChck/risks-from-ai-persuasion">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Lesswrong&amp;rft.atitle=Risks+from+AI+persuasion&amp;rft.date=2021&amp;rft.aulast=Barnes&amp;rft.aufirst=Beth&amp;rft_id=https%3A%2F%2Fwww.lesswrong.com%2Fposts%2F5cWtwATHL6KyzChck%2Frisks-from-ai-persuasion&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:13-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-:13_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:13_6-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:13_6-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBrundageAvinClarkToner2018" class="citation journal cs1">Brundage, Miles; Avin, Shahar; Clark, Jack; Toner, Helen; Eckersley, Peter; Garfinkel, Ben; Dafoe, Allan; Scharre, Paul; Zeitzoff, Thomas; Filar, Bobby; Anderson, Hyrum; Roff, Heather; Allen, Gregory C; Steinhardt, Jacob; Flynn, Carrick (2018-04-30). <a rel="nofollow" class="external text" href="https://www.repository.cam.ac.uk/handle/1810/275332">"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation"</a>. Apollo-University Of Cambridge Repository, Apollo-University Of Cambridge Repository. Apollo - University of Cambridge Repository. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.17863%2Fcam.22520">10.17863/cam.22520</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:3385567">3385567</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123055429/https://www.repository.cam.ac.uk/handle/1810/275332">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+Malicious+Use+of+Artificial+Intelligence%3A+Forecasting%2C+Prevention%2C+and+Mitigation&amp;rft.date=2018-04-30&amp;rft_id=info%3Adoi%2F10.17863%2Fcam.22520&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A3385567%23id-name%3DS2CID&amp;rft.aulast=Brundage&amp;rft.aufirst=Miles&amp;rft.au=Avin%2C+Shahar&amp;rft.au=Clark%2C+Jack&amp;rft.au=Toner%2C+Helen&amp;rft.au=Eckersley%2C+Peter&amp;rft.au=Garfinkel%2C+Ben&amp;rft.au=Dafoe%2C+Allan&amp;rft.au=Scharre%2C+Paul&amp;rft.au=Zeitzoff%2C+Thomas&amp;rft.au=Filar%2C+Bobby&amp;rft.au=Anderson%2C+Hyrum&amp;rft.au=Roff%2C+Heather&amp;rft.au=Allen%2C+Gregory+C&amp;rft.au=Steinhardt%2C+Jacob&amp;rft.au=Flynn%2C+Carrick&amp;rft_id=https%3A%2F%2Fwww.repository.cam.ac.uk%2Fhandle%2F1810%2F275332&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDavies2022" class="citation web cs1">Davies, Pascale (December 26, 2022). <a rel="nofollow" class="external text" href="https://www.euronews.com/next/2022/12/26/ai-cyber-attacks-are-a-critical-threat-this-is-how-nato-is-countering-them">"How NATO is preparing for a new era of AI cyber attacks"</a>. <i>euronews</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-03-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=euronews&amp;rft.atitle=How+NATO+is+preparing+for+a+new+era+of+AI+cyber+attacks&amp;rft.date=2022-12-26&amp;rft.aulast=Davies&amp;rft.aufirst=Pascale&amp;rft_id=https%3A%2F%2Fwww.euronews.com%2Fnext%2F2022%2F12%2F26%2Fai-cyber-attacks-are-a-critical-threat-this-is-how-nato-is-countering-them&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAhuja2024" class="citation web cs1">Ahuja, Anjana (February 7, 2024). <a rel="nofollow" class="external text" href="https://www.ft.com/content/e2a28b73-9831-4e7e-be7c-a599d2498f24">"AI's bioterrorism potential should not be ruled out"</a>. <i>Financial Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-03-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Financial+Times&amp;rft.atitle=AI%27s+bioterrorism+potential+should+not+be+ruled+out&amp;rft.date=2024-02-07&amp;rft.aulast=Ahuja&amp;rft.aufirst=Anjana&amp;rft_id=https%3A%2F%2Fwww.ft.com%2Fcontent%2Fe2a28b73-9831-4e7e-be7c-a599d2498f24&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCarlsmith2022" class="citation journal cs1">Carlsmith, Joseph (2022-06-16). "Is Power-Seeking AI an Existential Risk?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2206.13353">2206.13353</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Is+Power-Seeking+AI+an+Existential+Risk%3F&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2206.13353&amp;rft.aulast=Carlsmith&amp;rft.aufirst=Joseph&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMinardi2020" class="citation web cs1">Minardi, Di (16 October 2020). <a rel="nofollow" class="external text" href="https://www.bbc.com/future/article/20201014-totalitarian-world-in-chains-artificial-intelligence">"The grim fate that could be 'worse than extinction'<span class="cs1-kern-right"></span>"</a>. <i>BBC</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-03-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBC&amp;rft.atitle=The+grim+fate+that+could+be+%27worse+than+extinction%27&amp;rft.date=2020-10-16&amp;rft.aulast=Minardi&amp;rft.aufirst=Di&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Ffuture%2Farticle%2F20201014-totalitarian-world-in-chains-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-Carlsmith2022-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Carlsmith2022_11-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCarlsmith2022" class="citation arxiv cs1">Carlsmith, Joseph (2022-06-16). "Is Power-Seeking AI an Existential Risk?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2206.13353">2206.13353</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CY">cs.CY</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Is+Power-Seeking+AI+an+Existential+Risk%3F&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2206.13353&amp;rft.aulast=Carlsmith&amp;rft.aufirst=Joseph&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://fortune.com/2023/05/02/godfather-ai-geoff-hinton-google-warns-artificial-intelligence-nightmare-scenario/">"<span class="cs1-kern-left"></span>'The Godfather of A.I.' warns of 'nightmare scenario' where artificial intelligence begins to seek power"</a>. <i>Fortune</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-06-10</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Fortune&amp;rft.atitle=%27The+Godfather+of+A.I.%27+warns+of+%27nightmare+scenario%27+where+artificial+intelligence+begins+to+seek+power&amp;rft_id=https%3A%2F%2Ffortune.com%2F2023%2F05%2F02%2Fgodfather-ai-geoff-hinton-google-warns-artificial-intelligence-nightmare-scenario%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.nextbigfuture.com/2023/04/agi-expert-peter-voss-says-ai-alignment-problem-is-bogus.html">"AGI Expert Peter Voss Says AI Alignment Problem is Bogus | NextBigFuture.com"</a>. 2023-04-04<span class="reference-accessdate">. Retrieved <span class="nowrap">2023-07-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=AGI+Expert+Peter+Voss+Says+AI+Alignment+Problem+is+Bogus+%7C+NextBigFuture.com&amp;rft.date=2023-04-04&amp;rft_id=https%3A%2F%2Fwww.nextbigfuture.com%2F2023%2F04%2Fagi-expert-peter-voss-says-ai-alignment-problem-is-bogus.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDafoe2016" class="citation web cs1">Dafoe, Allan (2016). <a rel="nofollow" class="external text" href="https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/">"Yes, We Are Worried About the Existential Risk of Artificial Intelligence"</a>. <i>MIT Technology Review</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221128223713/https://www.technologyreview.com/2016/11/02/156285/yes-we-are-worried-about-the-existential-risk-of-artificial-intelligence/">Archived</a> from the original on 2022-11-28<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=Yes%2C+We+Are+Worried+About+the+Existential+Risk+of+Artificial+Intelligence&amp;rft.date=2016&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2016%2F11%2F02%2F156285%2Fyes-we-are-worried-about-the-existential-risk-of-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:1-15"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_15-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_15-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGraceSalvatierDafoeZhang2018" class="citation journal cs1">Grace, Katja; Salvatier, John; Dafoe, Allan; Zhang, Baobao; Evans, Owain (2018-07-31). <a rel="nofollow" class="external text" href="http://jair.org/index.php/jair/article/view/11222">"Viewpoint: When Will AI Exceed Human Performance? Evidence from AI Experts"</a>. <i>Journal of Artificial Intelligence Research</i>. <b>62</b>: 729–754. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1613%2Fjair.1.11222">10.1613/jair.1.11222</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1076-9757">1076-9757</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:8746462">8746462</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114220/https://jair.org/index.php/jair/article/view/11222">Archived</a> from the original on 2023-02-10<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Viewpoint%3A+When+Will+AI+Exceed+Human+Performance%3F+Evidence+from+AI+Experts&amp;rft.volume=62&amp;rft.pages=729-754&amp;rft.date=2018-07-31&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8746462%23id-name%3DS2CID&amp;rft.issn=1076-9757&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.1.11222&amp;rft.aulast=Grace&amp;rft.aufirst=Katja&amp;rft.au=Salvatier%2C+John&amp;rft.au=Dafoe%2C+Allan&amp;rft.au=Zhang%2C+Baobao&amp;rft.au=Evans%2C+Owain&amp;rft_id=http%3A%2F%2Fjair.org%2Findex.php%2Fjair%2Farticle%2Fview%2F11222&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZhangAnderljungKahnDreksler2021" class="citation journal cs1">Zhang, Baobao; Anderljung, Markus; Kahn, Lauren; Dreksler, Noemi; Horowitz, Michael C.; Dafoe, Allan (2021-05-05). "Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers". <i>Journal of Artificial Intelligence Research</i>. <b>71</b>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2105.02117">2105.02117</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1613%2Fjair.1.12895">10.1613/jair.1.12895</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Ethics+and+Governance+of+Artificial+Intelligence%3A+Evidence+from+a+Survey+of+Machine+Learning+Researchers&amp;rft.volume=71&amp;rft.date=2021-05-05&amp;rft_id=info%3Aarxiv%2F2105.02117&amp;rft_id=info%3Adoi%2F10.1613%2Fjair.1.12895&amp;rft.aulast=Zhang&amp;rft.aufirst=Baobao&amp;rft.au=Anderljung%2C+Markus&amp;rft.au=Kahn%2C+Lauren&amp;rft.au=Dreksler%2C+Noemi&amp;rft.au=Horowitz%2C+Michael+C.&amp;rft.au=Dafoe%2C+Allan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFStein-PerlmanWeinstein-RaunGrace2022" class="citation web cs1">Stein-Perlman, Zach; Weinstein-Raun, Benjamin; Grace (2022-08-04). <a rel="nofollow" class="external text" href="https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">"2022 Expert Survey on Progress in AI"</a>. <i>AI Impacts</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123052335/https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=AI+Impacts&amp;rft.atitle=2022+Expert+Survey+on+Progress+in+AI&amp;rft.date=2022-08-04&amp;rft.aulast=Stein-Perlman&amp;rft.aufirst=Zach&amp;rft.au=Weinstein-Raun%2C+Benjamin&amp;rft.au=Grace&amp;rft_id=https%3A%2F%2Faiimpacts.org%2F2022-expert-survey-on-progress-in-ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMichaelHoltzmanParrishMueller2022" class="citation journal cs1">Michael, Julian; <a href="/wiki/Ari_Holtzman" title="Ari Holtzman">Holtzman, Ari</a>; Parrish, Alicia; Mueller, Aaron; Wang, Alex; Chen, Angelica; Madaan, Divyam; Nangia, Nikita; Pang, Richard Yuanzhe; Phang, Jason; Bowman, Samuel R. (2022-08-26). "What Do NLP Researchers Believe? Results of the NLP Community Metasurvey". <i>Association for Computational Linguistics</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2208.12852">2208.12852</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Association+for+Computational+Linguistics&amp;rft.atitle=What+Do+NLP+Researchers+Believe%3F+Results+of+the+NLP+Community+Metasurvey&amp;rft.date=2022-08-26&amp;rft_id=info%3Aarxiv%2F2208.12852&amp;rft.aulast=Michael&amp;rft.aufirst=Julian&amp;rft.au=Holtzman%2C+Ari&amp;rft.au=Parrish%2C+Alicia&amp;rft.au=Mueller%2C+Aaron&amp;rft.au=Wang%2C+Alex&amp;rft.au=Chen%2C+Angelica&amp;rft.au=Madaan%2C+Divyam&amp;rft.au=Nangia%2C+Nikita&amp;rft.au=Pang%2C+Richard+Yuanzhe&amp;rft.au=Phang%2C+Jason&amp;rft.au=Bowman%2C+Samuel+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMarkoff2013" class="citation news cs1"><a href="/wiki/John_Markoff" title="John Markoff">Markoff, John</a> (2013-05-20). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html">"In 1949, He Imagined an Age of Robots"</a>. <i>The New York Times</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0362-4331">0362-4331</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123061554/https://www.nytimes.com/2013/05/21/science/mit-scholars-1949-essay-on-machine-age-is-found.html">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=In+1949%2C+He+Imagined+an+Age+of+Robots&amp;rft.date=2013-05-20&amp;rft.issn=0362-4331&amp;rft.aulast=Markoff&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2013%2F05%2F21%2Fscience%2Fmit-scholars-1949-essay-on-machine-age-is-found.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAssociation_for_the_Advancement_of_Artificial_Intelligence" class="citation web cs1">Association for the Advancement of Artificial Intelligence. <a rel="nofollow" class="external text" href="https://www.aaai.org/Organization/presidential-panel.php">"AAAI Presidential Panel on Long-Term AI Futures"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20220901033354/https://www.aaai.org/Organization/presidential-panel.php">Archived</a> from the original on 2022-09-01<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=AAAI+Presidential+Panel+on+Long-Term+AI+Futures&amp;rft.au=Association+for+the+Advancement+of+Artificial+Intelligence&amp;rft_id=https%3A%2F%2Fwww.aaai.org%2FOrganization%2Fpresidential-panel.php&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFYampolskiySpellchecker2016" class="citation journal cs1">Yampolskiy, Roman V.; Spellchecker, M. S. (2016-10-25). "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1610.07997">1610.07997</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Artificial+Intelligence+Safety+and+Cybersecurity%3A+a+Timeline+of+AI+Failures&amp;rft.date=2016-10-25&amp;rft_id=info%3Aarxiv%2F1610.07997&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman+V.&amp;rft.au=Spellchecker%2C+M.+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://conference.researchbib.com/view/event/13986">"PT-AI 2011 – Philosophy and Theory of Artificial Intelligence (PT-AI 2011)"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123062236/https://conference.researchbib.com/view/event/13986">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=PT-AI+2011+%E2%80%93+Philosophy+and+Theory+of+Artificial+Intelligence+%28PT-AI+2011%29&amp;rft_id=https%3A%2F%2Fconference.researchbib.com%2Fview%2Fevent%2F13986&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFYampolskiy2013" class="citation cs2">Yampolskiy, Roman V. (2013), Müller, Vincent C. (ed.), <a rel="nofollow" class="external text" href="http://link.springer.com/10.1007/978-3-642-31674-6_29">"Artificial Intelligence Safety Engineering: Why Machine Ethics is a Wrong Approach"</a>, <i>Philosophy and Theory of Artificial Intelligence</i>, Studies in Applied Philosophy, Epistemology and Rational Ethics, vol.&#160;5, Berlin; Heidelberg, Germany: Springer Berlin Heidelberg, pp.&#160;389–396, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-3-642-31674-6_29">10.1007/978-3-642-31674-6_29</a>, <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-31673-9" title="Special:BookSources/978-3-642-31673-9"><bdi>978-3-642-31673-9</bdi></a>, <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230315184334/https://link.springer.com/chapter/10.1007/978-3-642-31674-6_29">archived</a> from the original on 2023-03-15<span class="reference-accessdate">, retrieved <span class="nowrap">2022-11-23</span></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophy+and+Theory+of+Artificial+Intelligence&amp;rft.atitle=Artificial+Intelligence+Safety+Engineering%3A+Why+Machine+Ethics+is+a+Wrong+Approach&amp;rft.volume=5&amp;rft.pages=389-396&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-31674-6_29&amp;rft.isbn=978-3-642-31673-9&amp;rft.aulast=Yampolskiy&amp;rft.aufirst=Roman+V.&amp;rft_id=http%3A%2F%2Flink.springer.com%2F10.1007%2F978-3-642-31674-6_29&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMcLeanReadThompsonBaber2023" class="citation journal cs1">McLean, Scott; Read, Gemma J. M.; Thompson, Jason; Baber, Chris; Stanton, Neville A.; Salmon, Paul M. (2023-07-04). <a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0952813X.2021.1964003">"The risks associated with Artificial General Intelligence: A systematic review"</a>. <i>Journal of Experimental &amp; Theoretical Artificial Intelligence</i>. <b>35</b> (5): 649–663. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2023JETAI..35..649M">2023JETAI..35..649M</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F0952813X.2021.1964003">10.1080/0952813X.2021.1964003</a></span>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://hdl.handle.net/11343%2F289595">11343/289595</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0952-813X">0952-813X</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:238643957">238643957</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+%26+Theoretical+Artificial+Intelligence&amp;rft.atitle=The+risks+associated+with+Artificial+General+Intelligence%3A+A+systematic+review&amp;rft.volume=35&amp;rft.issue=5&amp;rft.pages=649-663&amp;rft.date=2023-07-04&amp;rft_id=info%3Ahdl%2F11343%2F289595&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A238643957%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2023JETAI..35..649M&amp;rft.issn=0952-813X&amp;rft_id=info%3Adoi%2F10.1080%2F0952813X.2021.1964003&amp;rft.aulast=McLean&amp;rft.aufirst=Scott&amp;rft.au=Read%2C+Gemma+J.+M.&amp;rft.au=Thompson%2C+Jason&amp;rft.au=Baber%2C+Chris&amp;rft.au=Stanton%2C+Neville+A.&amp;rft.au=Salmon%2C+Paul+M.&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1080%252F0952813X.2021.1964003&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWile2014" class="citation web cs1">Wile, Rob (August 3, 2014). <a rel="nofollow" class="external text" href="https://www.businessinsider.com/elon-musk-compares-ai-to-nukes-2014-8">"Elon Musk: Artificial Intelligence Is 'Potentially More Dangerous Than Nukes'<span class="cs1-kern-right"></span>"</a>. <i>Business Insider</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-02-22</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Business+Insider&amp;rft.atitle=Elon+Musk%3A+Artificial+Intelligence+Is+%27Potentially+More+Dangerous+Than+Nukes%27&amp;rft.date=2014-08-03&amp;rft.aulast=Wile&amp;rft.aufirst=Rob&amp;rft_id=https%3A%2F%2Fwww.businessinsider.com%2Felon-musk-compares-ai-to-nukes-2014-8&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKuo2015" class="citation audio-visual cs1">Kuo, Kaiser (2015-03-31). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=NG0ZjUfOBUs&amp;t=1055s&amp;ab_channel=KaiserKuo"><i>Baidu CEO Robin Li interviews Bill Gates and Elon Musk at the Boao Forum, March 29, 2015</i></a>.  Event occurs at 55:49. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123072346/https://www.youtube.com/watch?v=NG0ZjUfOBUs&amp;t=1055s&amp;ab_channel=KaiserKuo">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Baidu+CEO+Robin+Li+interviews+Bill+Gates+and+Elon+Musk+at+the+Boao+Forum%2C+March+29%2C+2015&amp;rft.date=2015-03-31&amp;rft.aulast=Kuo&amp;rft.aufirst=Kaiser&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNG0ZjUfOBUs%26t%3D1055s%26ab_channel%3DKaiserKuo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCellan-Jones2014" class="citation news cs1"><a href="/wiki/Rory_Cellan-Jones" title="Rory Cellan-Jones">Cellan-Jones, Rory</a> (2014-12-02). <a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-30290540">"Stephen Hawking warns artificial intelligence could end mankind"</a>. <i>BBC News</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20151030054329/http://www.bbc.com/news/technology-30290540">Archived</a> from the original on 2015-10-30<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Stephen+Hawking+warns+artificial+intelligence+could+end+mankind&amp;rft.date=2014-12-02&amp;rft.aulast=Cellan-Jones&amp;rft.aufirst=Rory&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-30290540&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFuture_of_Life_Institute" class="citation web cs1">Future of Life Institute. <a rel="nofollow" class="external text" href="https://futureoflife.org/open-letter/ai-open-letter/">"Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter"</a>. <i>Future of Life Institute</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123072924/https://futureoflife.org/open-letter/ai-open-letter/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=Research+Priorities+for+Robust+and+Beneficial+Artificial+Intelligence%3A+An+Open+Letter&amp;rft.au=Future+of+Life+Institute&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fopen-letter%2Fai-open-letter%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFuture_of_Life_Institute2016" class="citation web cs1">Future of Life Institute (October 2016). <a rel="nofollow" class="external text" href="https://futureoflife.org/ai-research/">"AI Research Grants Program"</a>. <i>Future of Life Institute</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123074311/https://futureoflife.org/ai-research/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=AI+Research+Grants+Program&amp;rft.date=2016-10&amp;rft.au=Future+of+Life+Institute&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fai-research%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.cmu.edu/safartint/">"SafArtInt 2016"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123074311/https://www.cmu.edu/safartint/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=SafArtInt+2016&amp;rft_id=https%3A%2F%2Fwww.cmu.edu%2Fsafartint%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBach2016" class="citation web cs1">Bach, Deborah (2016). <a rel="nofollow" class="external text" href="https://www.washington.edu/news/2016/05/19/uw-to-host-first-of-four-white-house-public-workshops-on-artificial-intelligence/">"UW to host first of four White House public workshops on artificial intelligence"</a>. <i>UW News</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123074321/https://www.washington.edu/news/2016/05/19/uw-to-host-first-of-four-white-house-public-workshops-on-artificial-intelligence/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=UW+News&amp;rft.atitle=UW+to+host+first+of+four+White+House+public+workshops+on+artificial+intelligence&amp;rft.date=2016&amp;rft.aulast=Bach&amp;rft.aufirst=Deborah&amp;rft_id=https%3A%2F%2Fwww.washington.edu%2Fnews%2F2016%2F05%2F19%2Fuw-to-host-first-of-four-white-house-public-workshops-on-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAmodeiOlahSteinhardtChristiano2016" class="citation journal cs1">Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (2016-07-25). "Concrete Problems in AI Safety". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1606.06565">1606.06565</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Concrete+Problems+in+AI+Safety&amp;rft.date=2016-07-25&amp;rft_id=info%3Aarxiv%2F1606.06565&amp;rft.aulast=Amodei&amp;rft.aufirst=Dario&amp;rft.au=Olah%2C+Chris&amp;rft.au=Steinhardt%2C+Jacob&amp;rft.au=Christiano%2C+Paul&amp;rft.au=Schulman%2C+John&amp;rft.au=Man%C3%A9%2C+Dan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-:21-33"><span class="mw-cite-backlink">^ <a href="#cite_ref-:21_33-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:21_33-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFuture_of_Life_Institute" class="citation web cs1">Future of Life Institute. <a rel="nofollow" class="external text" href="https://futureoflife.org/open-letter/ai-principles/">"AI Principles"</a>. <i>Future of Life Institute</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123074312/https://futureoflife.org/open-letter/ai-principles/">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=AI+Principles&amp;rft.au=Future+of+Life+Institute&amp;rft_id=https%3A%2F%2Ffutureoflife.org%2Fopen-letter%2Fai-principles%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:8-34"><span class="mw-cite-backlink">^ <a href="#cite_ref-:8_34-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:8_34-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFResearch2018" class="citation web cs1">Research, DeepMind Safety (2018-09-27). <a rel="nofollow" class="external text" href="https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1">"Building safe artificial intelligence: specification, robustness, and assurance"</a>. <i>Medium</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1">Archived</a> from the original on 2023-02-10<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Medium&amp;rft.atitle=Building+safe+artificial+intelligence%3A+specification%2C+robustness%2C+and+assurance&amp;rft.date=2018-09-27&amp;rft.aulast=Research&amp;rft.aufirst=DeepMind+Safety&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com%2Fbuilding-safe-artificial-intelligence-52f5f75058f1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://sites.google.com/view/safeml-iclr2019">"SafeML ICLR 2019 Workshop"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123074310/https://sites.google.com/view/safeml-iclr2019">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=SafeML+ICLR+2019+Workshop&amp;rft_id=https%3A%2F%2Fsites.google.com%2Fview%2Fsafeml-iclr2019&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-Hendrycks2022-36"><span class="mw-cite-backlink">^ <a href="#cite_ref-Hendrycks2022_36-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Hendrycks2022_36-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Hendrycks2022_36-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Hendrycks2022_36-3"><sup><i><b>d</b></i></sup></a> <a href="#cite_ref-Hendrycks2022_36-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHendrycksCarliniSchulmanSteinhardt2022" class="citation journal cs1">Hendrycks, Dan; Carlini, Nicholas; Schulman, John; Steinhardt, Jacob (2022-06-16). "Unsolved Problems in ML Safety". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2109.13916">2109.13916</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Unsolved+Problems+in+ML+Safety&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2109.13916&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Carlini%2C+Nicholas&amp;rft.au=Schulman%2C+John&amp;rft.au=Steinhardt%2C+Jacob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBrowne2023" class="citation web cs1">Browne, Ryan (2023-06-12). <a rel="nofollow" class="external text" href="https://www.cnbc.com/2023/06/12/pm-rishi-sunak-pitches-uk-as-geographical-home-of-ai-regulation.html">"British Prime Minister Rishi Sunak pitches UK as home of A.I. safety regulation as London bids to be next Silicon Valley"</a>. <i>CNBC</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-06-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CNBC&amp;rft.atitle=British+Prime+Minister+Rishi+Sunak+pitches+UK+as+home+of+A.I.+safety+regulation+as+London+bids+to+be+next+Silicon+Valley&amp;rft.date=2023-06-12&amp;rft.aulast=Browne&amp;rft.aufirst=Ryan&amp;rft_id=https%3A%2F%2Fwww.cnbc.com%2F2023%2F06%2F12%2Fpm-rishi-sunak-pitches-uk-as-geographical-home-of-ai-regulation.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBertuzzi2023" class="citation news cs1">Bertuzzi, Luca (October 18, 2023). <a rel="nofollow" class="external text" href="https://www.euractiv.com/section/artificial-intelligence/news/uks-ai-safety-summit-set-to-highlight-risk-of-losing-human-control-over-frontier-models/">"UK's AI safety summit set to highlight risk of losing human control over 'frontier' models"</a>. <i>Euractiv</i><span class="reference-accessdate">. Retrieved <span class="nowrap">March 2,</span> 2024</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Euractiv&amp;rft.atitle=UK%27s+AI+safety+summit+set+to+highlight+risk+of+losing+human+control+over+%27frontier%27+models&amp;rft.date=2023-10-18&amp;rft.aulast=Bertuzzi&amp;rft.aufirst=Luca&amp;rft_id=https%3A%2F%2Fwww.euractiv.com%2Fsection%2Fartificial-intelligence%2Fnews%2Fuks-ai-safety-summit-set-to-highlight-risk-of-losing-human-control-over-frontier-models%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFShepardson2024" class="citation news cs1">Shepardson, David (1 April 2024). <a rel="nofollow" class="external text" href="https://www.reuters.com/technology/us-britain-announce-formal-partnership-artificial-intelligence-safety-2024-04-01/">"US, Britain announce partnership on AI safety, testing"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2 April</span> 2024</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=US%2C+Britain+announce+partnership+on+AI+safety%2C+testing&amp;rft.date=2024-04-01&amp;rft.aulast=Shepardson&amp;rft.aufirst=David&amp;rft_id=https%3A%2F%2Fwww.reuters.com%2Ftechnology%2Fus-britain-announce-formal-partnership-artificial-intelligence-safety-2024-04-01%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGoodfellowPapernotHuangDuan2017" class="citation web cs1">Goodfellow, Ian; Papernot, Nicolas; Huang, Sandy; Duan, Rocky; Abbeel, Pieter; Clark, Jack (2017-02-24). <a rel="nofollow" class="external text" href="https://openai.com/blog/adversarial-example-research/">"Attacking Machine Learning with Adversarial Examples"</a>. <i>OpenAI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124070536/https://openai.com/blog/adversarial-example-research/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Attacking+Machine+Learning+with+Adversarial+Examples&amp;rft.date=2017-02-24&amp;rft.aulast=Goodfellow&amp;rft.aufirst=Ian&amp;rft.au=Papernot%2C+Nicolas&amp;rft.au=Huang%2C+Sandy&amp;rft.au=Duan%2C+Rocky&amp;rft.au=Abbeel%2C+Pieter&amp;rft.au=Clark%2C+Jack&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fadversarial-example-research%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:4-41"><span class="mw-cite-backlink">^ <a href="#cite_ref-:4_41-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:4_41-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSzegedyZarembaSutskeverBruna2014" class="citation journal cs1">Szegedy, Christian; Zaremba, Wojciech; Sutskever, Ilya; Bruna, Joan; Erhan, Dumitru; Goodfellow, Ian; Fergus, Rob (2014-02-19). "Intriguing properties of neural networks". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1312.6199">1312.6199</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Intriguing+properties+of+neural+networks&amp;rft.date=2014-02-19&amp;rft_id=info%3Aarxiv%2F1312.6199&amp;rft.aulast=Szegedy&amp;rft.aufirst=Christian&amp;rft.au=Zaremba%2C+Wojciech&amp;rft.au=Sutskever%2C+Ilya&amp;rft.au=Bruna%2C+Joan&amp;rft.au=Erhan%2C+Dumitru&amp;rft.au=Goodfellow%2C+Ian&amp;rft.au=Fergus%2C+Rob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKurakinGoodfellowBengio2017" class="citation journal cs1">Kurakin, Alexey; Goodfellow, Ian; Bengio, Samy (2017-02-10). "Adversarial examples in the physical world". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1607.02533">1607.02533</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Adversarial+examples+in+the+physical+world&amp;rft.date=2017-02-10&amp;rft_id=info%3Aarxiv%2F1607.02533&amp;rft.aulast=Kurakin&amp;rft.aufirst=Alexey&amp;rft.au=Goodfellow%2C+Ian&amp;rft.au=Bengio%2C+Samy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMadryMakelovSchmidtTsipras2019" class="citation journal cs1">Madry, Aleksander; Makelov, Aleksandar; Schmidt, Ludwig; Tsipras, Dimitris; Vladu, Adrian (2019-09-04). "Towards Deep Learning Models Resistant to Adversarial Attacks". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1706.06083">1706.06083</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Towards+Deep+Learning+Models+Resistant+to+Adversarial+Attacks&amp;rft.date=2019-09-04&amp;rft_id=info%3Aarxiv%2F1706.06083&amp;rft.aulast=Madry&amp;rft.aufirst=Aleksander&amp;rft.au=Makelov%2C+Aleksandar&amp;rft.au=Schmidt%2C+Ludwig&amp;rft.au=Tsipras%2C+Dimitris&amp;rft.au=Vladu%2C+Adrian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKannanKurakinGoodfellow2018" class="citation journal cs1">Kannan, Harini; Kurakin, Alexey; Goodfellow, Ian (2018-03-16). "Adversarial Logit Pairing". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1803.06373">1803.06373</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Adversarial+Logit+Pairing&amp;rft.date=2018-03-16&amp;rft_id=info%3Aarxiv%2F1803.06373&amp;rft.aulast=Kannan&amp;rft.aufirst=Harini&amp;rft.au=Kurakin%2C+Alexey&amp;rft.au=Goodfellow%2C+Ian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGilmerAdamsGoodfellowAndersen2018" class="citation journal cs1">Gilmer, Justin; Adams, Ryan P.; Goodfellow, Ian; Andersen, David; Dahl, George E. (2018-07-19). "Motivating the Rules of the Game for Adversarial Example Research". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1807.06732">1807.06732</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Motivating+the+Rules+of+the+Game+for+Adversarial+Example+Research&amp;rft.date=2018-07-19&amp;rft_id=info%3Aarxiv%2F1807.06732&amp;rft.aulast=Gilmer&amp;rft.aufirst=Justin&amp;rft.au=Adams%2C+Ryan+P.&amp;rft.au=Goodfellow%2C+Ian&amp;rft.au=Andersen%2C+David&amp;rft.au=Dahl%2C+George+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCarliniWagner2018" class="citation journal cs1">Carlini, Nicholas; Wagner, David (2018-03-29). "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text". <i>IEEE Security and Privacy Workshops</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1801.01944">1801.01944</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Security+and+Privacy+Workshops&amp;rft.atitle=Audio+Adversarial+Examples%3A+Targeted+Attacks+on+Speech-to-Text&amp;rft.date=2018-03-29&amp;rft_id=info%3Aarxiv%2F1801.01944&amp;rft.aulast=Carlini&amp;rft.aufirst=Nicholas&amp;rft.au=Wagner%2C+David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSheatsleyPapernotWeismanVerma2022" class="citation journal cs1">Sheatsley, Ryan; Papernot, Nicolas; Weisman, Michael; Verma, Gunjan; McDaniel, Patrick (2022-09-09). "Adversarial Examples in Constrained Domains". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2011.01183">2011.01183</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Adversarial+Examples+in+Constrained+Domains&amp;rft.date=2022-09-09&amp;rft_id=info%3Aarxiv%2F2011.01183&amp;rft.aulast=Sheatsley&amp;rft.aufirst=Ryan&amp;rft.au=Papernot%2C+Nicolas&amp;rft.au=Weisman%2C+Michael&amp;rft.au=Verma%2C+Gunjan&amp;rft.au=McDaniel%2C+Patrick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSuciuCoullJohns2019" class="citation journal cs1">Suciu, Octavian; Coull, Scott E.; Johns, Jeffrey (2019-04-13). "Exploring Adversarial Examples in Malware Detection". <i>IEEE Security and Privacy Workshops</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1810.08280">1810.08280</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Security+and+Privacy+Workshops&amp;rft.atitle=Exploring+Adversarial+Examples+in+Malware+Detection&amp;rft.date=2019-04-13&amp;rft_id=info%3Aarxiv%2F1810.08280&amp;rft.aulast=Suciu&amp;rft.aufirst=Octavian&amp;rft.au=Coull%2C+Scott+E.&amp;rft.au=Johns%2C+Jeffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOuyangWuJiangAlmeida2022" class="citation journal cs1">Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie (2022-03-04). "Training language models to follow instructions with human feedback". <i>NeurIPS</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2203.02155">2203.02155</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=Training+language+models+to+follow+instructions+with+human+feedback&amp;rft.date=2022-03-04&amp;rft_id=info%3Aarxiv%2F2203.02155&amp;rft.aulast=Ouyang&amp;rft.aufirst=Long&amp;rft.au=Wu%2C+Jeff&amp;rft.au=Jiang%2C+Xu&amp;rft.au=Almeida%2C+Diogo&amp;rft.au=Wainwright%2C+Carroll+L.&amp;rft.au=Mishkin%2C+Pamela&amp;rft.au=Zhang%2C+Chong&amp;rft.au=Agarwal%2C+Sandhini&amp;rft.au=Slama%2C+Katarina&amp;rft.au=Ray%2C+Alex&amp;rft.au=Schulman%2C+John&amp;rft.au=Hilton%2C+Jacob&amp;rft.au=Kelton%2C+Fraser&amp;rft.au=Miller%2C+Luke&amp;rft.au=Simens%2C+Maddie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:0-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-:0_50-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGaoSchulmanHilton2022" class="citation journal cs1">Gao, Leo; Schulman, John; Hilton, Jacob (2022-10-19). "Scaling Laws for Reward Model Overoptimization". <i>ICML</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2210.10760">2210.10760</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICML&amp;rft.atitle=Scaling+Laws+for+Reward+Model+Overoptimization&amp;rft.date=2022-10-19&amp;rft_id=info%3Aarxiv%2F2210.10760&amp;rft.aulast=Gao&amp;rft.aufirst=Leo&amp;rft.au=Schulman%2C+John&amp;rft.au=Hilton%2C+Jacob&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFYuAhnSongShin2021" class="citation journal cs1">Yu, Sihyun; Ahn, Sungsoo; Song, Le; Shin, Jinwoo (2021-10-27). "RoMA: Robust Model Adaptation for Offline Model-based Optimization". <i>NeurIPS</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2110.14188">2110.14188</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=RoMA%3A+Robust+Model+Adaptation+for+Offline+Model-based+Optimization&amp;rft.date=2021-10-27&amp;rft_id=info%3Aarxiv%2F2110.14188&amp;rft.aulast=Yu&amp;rft.aufirst=Sihyun&amp;rft.au=Ahn%2C+Sungsoo&amp;rft.au=Song%2C+Le&amp;rft.au=Shin%2C+Jinwoo&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-X-Risk_Analysis_for_AI_Research-52"><span class="mw-cite-backlink">^ <a href="#cite_ref-X-Risk_Analysis_for_AI_Research_52-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-X-Risk_Analysis_for_AI_Research_52-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHendrycksMazeika2022" class="citation journal cs1">Hendrycks, Dan; Mazeika, Mantas (2022-09-20). "X-Risk Analysis for AI Research". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2206.05862">2206.05862</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=X-Risk+Analysis+for+AI+Research&amp;rft.date=2022-09-20&amp;rft_id=info%3Aarxiv%2F2206.05862&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Mazeika%2C+Mantas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-53">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFTranKondrashovaBradleyWilliams2021" class="citation journal cs1">Tran, Khoa A.; Kondrashova, Olga; Bradley, Andrew; Williams, Elizabeth D.; Pearson, John V.; Waddell, Nicola (2021). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8477474">"Deep learning in cancer diagnosis, prognosis and treatment selection"</a>. <i>Genome Medicine</i>. <b>13</b> (1): 152. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1186%2Fs13073-021-00968-x">10.1186/s13073-021-00968-x</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1756-994X">1756-994X</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8477474">8477474</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/34579788">34579788</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Genome+Medicine&amp;rft.atitle=Deep+learning+in+cancer+diagnosis%2C+prognosis+and+treatment+selection&amp;rft.volume=13&amp;rft.issue=1&amp;rft.pages=152&amp;rft.date=2021&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC8477474%23id-name%3DPMC&amp;rft.issn=1756-994X&amp;rft_id=info%3Apmid%2F34579788&amp;rft_id=info%3Adoi%2F10.1186%2Fs13073-021-00968-x&amp;rft.aulast=Tran&amp;rft.aufirst=Khoa+A.&amp;rft.au=Kondrashova%2C+Olga&amp;rft.au=Bradley%2C+Andrew&amp;rft.au=Williams%2C+Elizabeth+D.&amp;rft.au=Pearson%2C+John+V.&amp;rft.au=Waddell%2C+Nicola&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC8477474&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGuoPleissSunWeinberger2017" class="citation conference cs1">Guo, Chuan; Pleiss, Geoff; Sun, Yu; Weinberger, Kilian Q. (2017-08-06). "On calibration of modern neural networks". <i>Proceedings of the 34th international conference on machine learning</i>. Proceedings of machine learning research. Vol.&#160;70. PMLR. pp.&#160;1321–1330.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=On+calibration+of+modern+neural+networks&amp;rft.btitle=Proceedings+of+the+34th+international+conference+on+machine+learning&amp;rft.series=Proceedings+of+machine+learning+research&amp;rft.pages=1321-1330&amp;rft.pub=PMLR&amp;rft.date=2017-08-06&amp;rft.aulast=Guo&amp;rft.aufirst=Chuan&amp;rft.au=Pleiss%2C+Geoff&amp;rft.au=Sun%2C+Yu&amp;rft.au=Weinberger%2C+Kilian+Q.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOvadiaFertigRenNado2019" class="citation journal cs1">Ovadia, Yaniv; Fertig, Emily; Ren, Jie; Nado, Zachary; Sculley, D.; Nowozin, Sebastian; Dillon, Joshua V.; Lakshminarayanan, Balaji; Snoek, Jasper (2019-12-17). "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift". <i>NeurIPS</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1906.02530">1906.02530</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=Can+You+Trust+Your+Model%27s+Uncertainty%3F+Evaluating+Predictive+Uncertainty+Under+Dataset+Shift&amp;rft.date=2019-12-17&amp;rft_id=info%3Aarxiv%2F1906.02530&amp;rft.aulast=Ovadia&amp;rft.aufirst=Yaniv&amp;rft.au=Fertig%2C+Emily&amp;rft.au=Ren%2C+Jie&amp;rft.au=Nado%2C+Zachary&amp;rft.au=Sculley%2C+D.&amp;rft.au=Nowozin%2C+Sebastian&amp;rft.au=Dillon%2C+Joshua+V.&amp;rft.au=Lakshminarayanan%2C+Balaji&amp;rft.au=Snoek%2C+Jasper&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBogdollBreitensteinHeideckerBieshaar2021" class="citation book cs1">Bogdoll, Daniel; Breitenstein, Jasmin; Heidecker, Florian; Bieshaar, Maarten; Sick, Bernhard; Fingscheidt, Tim; Zöllner, J. Marius (2021). "Description of Corner Cases in Automated Driving: Goals and Challenges". <i>2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)</i>. pp.&#160;1023–1028. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2109.09607">2109.09607</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FICCVW54120.2021.00119">10.1109/ICCVW54120.2021.00119</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-6654-0191-3" title="Special:BookSources/978-1-6654-0191-3"><bdi>978-1-6654-0191-3</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:237572375">237572375</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Description+of+Corner+Cases+in+Automated+Driving%3A+Goals+and+Challenges&amp;rft.btitle=2021+IEEE%2FCVF+International+Conference+on+Computer+Vision+Workshops+%28ICCVW%29&amp;rft.pages=1023-1028&amp;rft.date=2021&amp;rft_id=info%3Aarxiv%2F2109.09607&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A237572375%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FICCVW54120.2021.00119&amp;rft.isbn=978-1-6654-0191-3&amp;rft.aulast=Bogdoll&amp;rft.aufirst=Daniel&amp;rft.au=Breitenstein%2C+Jasmin&amp;rft.au=Heidecker%2C+Florian&amp;rft.au=Bieshaar%2C+Maarten&amp;rft.au=Sick%2C+Bernhard&amp;rft.au=Fingscheidt%2C+Tim&amp;rft.au=Z%C3%B6llner%2C+J.+Marius&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHendrycksMazeikaDietterich2019" class="citation journal cs1">Hendrycks, Dan; Mazeika, Mantas; Dietterich, Thomas (2019-01-28). "Deep Anomaly Detection with Outlier Exposure". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1812.04606">1812.04606</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Deep+Anomaly+Detection+with+Outlier+Exposure&amp;rft.date=2019-01-28&amp;rft_id=info%3Aarxiv%2F1812.04606&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Mazeika%2C+Mantas&amp;rft.au=Dietterich%2C+Thomas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWangLiFengZhang2022" class="citation journal cs1">Wang, Haoqi; Li, Zhizhong; Feng, Litong; Zhang, Wayne (2022-03-21). "ViM: Out-Of-Distribution with Virtual-logit Matching". <i>CVPR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2203.10807">2203.10807</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CVPR&amp;rft.atitle=ViM%3A+Out-Of-Distribution+with+Virtual-logit+Matching&amp;rft.date=2022-03-21&amp;rft_id=info%3Aarxiv%2F2203.10807&amp;rft.aulast=Wang&amp;rft.aufirst=Haoqi&amp;rft.au=Li%2C+Zhizhong&amp;rft.au=Feng%2C+Litong&amp;rft.au=Zhang%2C+Wayne&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHendrycksGimpel2018" class="citation journal cs1">Hendrycks, Dan; Gimpel, Kevin (2018-10-03). "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1610.02136">1610.02136</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=A+Baseline+for+Detecting+Misclassified+and+Out-of-Distribution+Examples+in+Neural+Networks&amp;rft.date=2018-10-03&amp;rft_id=info%3Aarxiv%2F1610.02136&amp;rft.aulast=Hendrycks&amp;rft.aufirst=Dan&amp;rft.au=Gimpel%2C+Kevin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFUrbinaLentzosInvernizziEkins2022" class="citation journal cs1">Urbina, Fabio; Lentzos, Filippa; Invernizzi, Cédric; Ekins, Sean (2022). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280">"Dual use of artificial-intelligence-powered drug discovery"</a>. <i>Nature Machine Intelligence</i>. <b>4</b> (3): 189–191. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fs42256-022-00465-9">10.1038/s42256-022-00465-9</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2522-5839">2522-5839</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280">9544280</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/36211133">36211133</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Machine+Intelligence&amp;rft.atitle=Dual+use+of+artificial-intelligence-powered+drug+discovery&amp;rft.volume=4&amp;rft.issue=3&amp;rft.pages=189-191&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9544280%23id-name%3DPMC&amp;rft.issn=2522-5839&amp;rft_id=info%3Apmid%2F36211133&amp;rft_id=info%3Adoi%2F10.1038%2Fs42256-022-00465-9&amp;rft.aulast=Urbina&amp;rft.aufirst=Fabio&amp;rft.au=Lentzos%2C+Filippa&amp;rft.au=Invernizzi%2C+C%C3%A9dric&amp;rft.au=Ekins%2C+Sean&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9544280&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCenter_for_Security_and_Emerging_TechnologyBuchananLohnMusser2021" class="citation journal cs1">Center for Security and Emerging Technology; Buchanan, Ben; Lohn, Andrew; Musser, Micah; Sedova, Katerina (2021). <a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/truth-lies-and-automation/">"Truth, Lies, and Automation: How Language Models Could Change Disinformation"</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.51593%2F2021ca003">10.51593/2021ca003</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:240522878">240522878</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124073719/https://cset.georgetown.edu/publication/truth-lies-and-automation/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Truth%2C+Lies%2C+and+Automation%3A+How+Language+Models+Could+Change+Disinformation&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.51593%2F2021ca003&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A240522878%23id-name%3DS2CID&amp;rft.au=Center+for+Security+and+Emerging+Technology&amp;rft.au=Buchanan%2C+Ben&amp;rft.au=Lohn%2C+Andrew&amp;rft.au=Musser%2C+Micah&amp;rft.au=Sedova%2C+Katerina&amp;rft_id=https%3A%2F%2Fcset.georgetown.edu%2Fpublication%2Ftruth-lies-and-automation%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://venturebeat.com/ai/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/">"Propaganda-as-a-service may be on the horizon if large language models are abused"</a>. <i>VentureBeat</i>. 2021-12-14. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124073718/https://venturebeat.com/ai/propaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=VentureBeat&amp;rft.atitle=Propaganda-as-a-service+may+be+on+the+horizon+if+large+language+models+are+abused&amp;rft.date=2021-12-14&amp;rft_id=https%3A%2F%2Fventurebeat.com%2Fai%2Fpropaganda-as-a-service-may-be-on-the-horizon-if-large-language-models-are-abused%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-63">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCenter_for_Security_and_Emerging_TechnologyBuchananBansemerCary2020" class="citation journal cs1">Center for Security and Emerging Technology; Buchanan, Ben; Bansemer, John; Cary, Dakota; Lucas, Jack; Musser, Micah (2020). <a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/automating-cyber-attacks/">"Automating Cyber Attacks: Hype and Reality"</a>. <i>Center for Security and Emerging Technology</i>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.51593%2F2020ca002">10.51593/2020ca002</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:234623943">234623943</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124074301/https://cset.georgetown.edu/publication/automating-cyber-attacks/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Center+for+Security+and+Emerging+Technology&amp;rft.atitle=Automating+Cyber+Attacks%3A+Hype+and+Reality&amp;rft.date=2020&amp;rft_id=info%3Adoi%2F10.51593%2F2020ca002&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A234623943%23id-name%3DS2CID&amp;rft.au=Center+for+Security+and+Emerging+Technology&amp;rft.au=Buchanan%2C+Ben&amp;rft.au=Bansemer%2C+John&amp;rft.au=Cary%2C+Dakota&amp;rft.au=Lucas%2C+Jack&amp;rft.au=Musser%2C+Micah&amp;rft_id=https%3A%2F%2Fcset.georgetown.edu%2Fpublication%2Fautomating-cyber-attacks%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-64">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://openai.com/blog/language-model-safety-and-misuse/">"Lessons Learned on Language Model Safety and Misuse"</a>. <i>OpenAI</i>. 2022-03-03. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124074259/https://openai.com/blog/language-model-safety-and-misuse/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Lessons+Learned+on+Language+Model+Safety+and+Misuse&amp;rft.date=2022-03-03&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Flanguage-model-safety-and-misuse%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMarkovZhangAgarwalEloundou2022" class="citation web cs1">Markov, Todor; Zhang, Chong; Agarwal, Sandhini; Eloundou, Tyna; Lee, Teddy; Adler, Steven; Jiang, Angela; Weng, Lilian (2022-08-10). <a rel="nofollow" class="external text" href="https://openai.com/blog/new-and-improved-content-moderation-tooling/">"New-and-Improved Content Moderation Tooling"</a>. <i>OpenAI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230111020935/https://openai.com/blog/new-and-improved-content-moderation-tooling/">Archived</a> from the original on 2023-01-11<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=New-and-Improved+Content+Moderation+Tooling&amp;rft.date=2022-08-10&amp;rft.aulast=Markov&amp;rft.aufirst=Todor&amp;rft.au=Zhang%2C+Chong&amp;rft.au=Agarwal%2C+Sandhini&amp;rft.au=Eloundou%2C+Tyna&amp;rft.au=Lee%2C+Teddy&amp;rft.au=Adler%2C+Steven&amp;rft.au=Jiang%2C+Angela&amp;rft.au=Weng%2C+Lilian&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fnew-and-improved-content-moderation-tooling%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:5-66"><span class="mw-cite-backlink">^ <a href="#cite_ref-:5_66-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:5_66-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSavage2022" class="citation journal cs1">Savage, Neil (2022-03-29). <a rel="nofollow" class="external text" href="https://www.nature.com/articles/d41586-022-00858-1">"Breaking into the black box of artificial intelligence"</a>. <i>Nature</i>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-022-00858-1">10.1038/d41586-022-00858-1</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/35352042">35352042</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:247792459">247792459</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124074724/https://www.nature.com/articles/d41586-022-00858-1">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Breaking+into+the+black+box+of+artificial+intelligence&amp;rft.date=2022-03-29&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A247792459%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F35352042&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-022-00858-1&amp;rft.aulast=Savage&amp;rft.aufirst=Neil&amp;rft_id=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-022-00858-1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-67">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCenter_for_Security_and_Emerging_TechnologyRudnerToner2021" class="citation journal cs1">Center for Security and Emerging Technology; Rudner, Tim; Toner, Helen (2021). <a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/">"Key Concepts in AI Safety: Interpretability in Machine Learning"</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.51593%2F20190042">10.51593/20190042</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233775541">233775541</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124075212/https://cset.georgetown.edu/publication/key-concepts-in-ai-safety-interpretability-in-machine-learning/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Key+Concepts+in+AI+Safety%3A+Interpretability+in+Machine+Learning&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.51593%2F20190042&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233775541%23id-name%3DS2CID&amp;rft.au=Center+for+Security+and+Emerging+Technology&amp;rft.au=Rudner%2C+Tim&amp;rft.au=Toner%2C+Helen&amp;rft_id=https%3A%2F%2Fcset.georgetown.edu%2Fpublication%2Fkey-concepts-in-ai-safety-interpretability-in-machine-learning%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-68"><span class="mw-cite-backlink"><b><a href="#cite_ref-68">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMcFarland2018" class="citation web cs1">McFarland, Matt (2018-03-19). <a rel="nofollow" class="external text" href="https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html">"Uber pulls self-driving cars after first fatal crash of autonomous vehicle"</a>. <i>CNNMoney</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124075209/https://money.cnn.com/2018/03/19/technology/uber-autonomous-car-fatal-crash/index.html">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CNNMoney&amp;rft.atitle=Uber+pulls+self-driving+cars+after+first+fatal+crash+of+autonomous+vehicle&amp;rft.date=2018-03-19&amp;rft.aulast=McFarland&amp;rft.aufirst=Matt&amp;rft_id=https%3A%2F%2Fmoney.cnn.com%2F2018%2F03%2F19%2Ftechnology%2Fuber-autonomous-car-fatal-crash%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-69"><span class="mw-cite-backlink"><b><a href="#cite_ref-69">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFelder2021" class="citation journal cs1">Felder, Ryan Marshall (July 2021). <a rel="nofollow" class="external text" href="https://onlinelibrary.wiley.com/doi/10.1002/hast.1248">"Coming to Terms with the Black Box Problem: How to Justify AI Systems in Health Care"</a>. <i>Hastings Center Report</i>. <b>51</b> (4): 38–45. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1002%2Fhast.1248">10.1002/hast.1248</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0093-0334">0093-0334</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/33821471">33821471</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Hastings+Center+Report&amp;rft.atitle=Coming+to+Terms+with+the+Black+Box+Problem%3A+How+to+Justify+AI+Systems+in+Health+Care&amp;rft.volume=51&amp;rft.issue=4&amp;rft.pages=38-45&amp;rft.date=2021-07&amp;rft.issn=0093-0334&amp;rft_id=info%3Apmid%2F33821471&amp;rft_id=info%3Adoi%2F10.1002%2Fhast.1248&amp;rft.aulast=Felder&amp;rft.aufirst=Ryan+Marshall&amp;rft_id=https%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2F10.1002%2Fhast.1248&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:6-70"><span class="mw-cite-backlink">^ <a href="#cite_ref-:6_70-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:6_70-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDoshi-VelezKortzBudishBavitz2019" class="citation journal cs1">Doshi-Velez, Finale; Kortz, Mason; Budish, Ryan; Bavitz, Chris; Gershman, Sam; O'Brien, David; Scott, Kate; Schieber, Stuart; Waldo, James; Weinberger, David; Weller, Adrian; Wood, Alexandra (2019-12-20). "Accountability of AI Under the Law: The Role of Explanation". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1711.01134">1711.01134</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Accountability+of+AI+Under+the+Law%3A+The+Role+of+Explanation&amp;rft.date=2019-12-20&amp;rft_id=info%3Aarxiv%2F1711.01134&amp;rft.aulast=Doshi-Velez&amp;rft.aufirst=Finale&amp;rft.au=Kortz%2C+Mason&amp;rft.au=Budish%2C+Ryan&amp;rft.au=Bavitz%2C+Chris&amp;rft.au=Gershman%2C+Sam&amp;rft.au=O%27Brien%2C+David&amp;rft.au=Scott%2C+Kate&amp;rft.au=Schieber%2C+Stuart&amp;rft.au=Waldo%2C+James&amp;rft.au=Weinberger%2C+David&amp;rft.au=Weller%2C+Adrian&amp;rft.au=Wood%2C+Alexandra&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFongVedaldi2017" class="citation book cs1">Fong, Ruth; Vedaldi, Andrea (2017). "Interpretable Explanations of Black Boxes by Meaningful Perturbation". <i>2017 IEEE International Conference on Computer Vision (ICCV)</i>. pp.&#160;3449–3457. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1704.03296">1704.03296</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109%2FICCV.2017.371">10.1109/ICCV.2017.371</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5386-1032-9" title="Special:BookSources/978-1-5386-1032-9"><bdi>978-1-5386-1032-9</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1633753">1633753</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Interpretable+Explanations+of+Black+Boxes+by+Meaningful+Perturbation&amp;rft.btitle=2017+IEEE+International+Conference+on+Computer+Vision+%28ICCV%29&amp;rft.pages=3449-3457&amp;rft.date=2017&amp;rft_id=info%3Aarxiv%2F1704.03296&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1633753%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1109%2FICCV.2017.371&amp;rft.isbn=978-1-5386-1032-9&amp;rft.aulast=Fong&amp;rft.aufirst=Ruth&amp;rft.au=Vedaldi%2C+Andrea&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMengBauAndonianBelinkov2022" class="citation journal cs1">Meng, Kevin; Bau, David; Andonian, Alex; Belinkov, Yonatan (2022). "Locating and editing factual associations in GPT". <i>Advances in Neural Information Processing Systems</i>. <b>35</b>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2202.05262">2202.05262</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Locating+and+editing+factual+associations+in+GPT&amp;rft.volume=35&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2202.05262&amp;rft.aulast=Meng&amp;rft.aufirst=Kevin&amp;rft.au=Bau%2C+David&amp;rft.au=Andonian%2C+Alex&amp;rft.au=Belinkov%2C+Yonatan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-73"><span class="mw-cite-backlink"><b><a href="#cite_ref-73">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBauLiuWangZhu2020" class="citation journal cs1">Bau, David; Liu, Steven; Wang, Tongzhou; Zhu, Jun-Yan; Torralba, Antonio (2020-07-30). "Rewriting a Deep Generative Model". <i>ECCV</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2007.15646">2007.15646</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ECCV&amp;rft.atitle=Rewriting+a+Deep+Generative+Model&amp;rft.date=2020-07-30&amp;rft_id=info%3Aarxiv%2F2007.15646&amp;rft.aulast=Bau&amp;rft.aufirst=David&amp;rft.au=Liu%2C+Steven&amp;rft.au=Wang%2C+Tongzhou&amp;rft.au=Zhu%2C+Jun-Yan&amp;rft.au=Torralba%2C+Antonio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-74"><span class="mw-cite-backlink"><b><a href="#cite_ref-74">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRäukerHoCasperHadfield-Menell2022" class="citation journal cs1">Räuker, Tilman; Ho, Anson; Casper, Stephen; Hadfield-Menell, Dylan (2022-09-05). "Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks". <i>IEEE SaTML</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2207.13243">2207.13243</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+SaTML&amp;rft.atitle=Toward+Transparent+AI%3A+A+Survey+on+Interpreting+the+Inner+Structures+of+Deep+Neural+Networks&amp;rft.date=2022-09-05&amp;rft_id=info%3Aarxiv%2F2207.13243&amp;rft.aulast=R%C3%A4uker&amp;rft.aufirst=Tilman&amp;rft.au=Ho%2C+Anson&amp;rft.au=Casper%2C+Stephen&amp;rft.au=Hadfield-Menell%2C+Dylan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-75"><span class="mw-cite-backlink"><b><a href="#cite_ref-75">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBauZhouKhoslaOliva2017" class="citation journal cs1">Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio (2017-04-19). "Network Dissection: Quantifying Interpretability of Deep Visual Representations". <i>CVPR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1704.05796">1704.05796</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CVPR&amp;rft.atitle=Network+Dissection%3A+Quantifying+Interpretability+of+Deep+Visual+Representations&amp;rft.date=2017-04-19&amp;rft_id=info%3Aarxiv%2F1704.05796&amp;rft.aulast=Bau&amp;rft.aufirst=David&amp;rft.au=Zhou%2C+Bolei&amp;rft.au=Khosla%2C+Aditya&amp;rft.au=Oliva%2C+Aude&amp;rft.au=Torralba%2C+Antonio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-76">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMcGrathKapishnikovTomaševPearce2022" class="citation journal cs1">McGrath, Thomas; Kapishnikov, Andrei; Tomašev, Nenad; Pearce, Adam; Wattenberg, Martin; Hassabis, Demis; Kim, Been; Paquet, Ulrich; Kramnik, Vladimir (2022-11-22). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9704706">"Acquisition of chess knowledge in AlphaZero"</a>. <i>Proceedings of the National Academy of Sciences</i>. <b>119</b> (47): e2206625119. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2111.09259">2111.09259</a></span>. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2022PNAS..11906625M">2022PNAS..11906625M</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1073%2Fpnas.2206625119">10.1073/pnas.2206625119</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0027-8424">0027-8424</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9704706">9704706</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/36375061">36375061</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences&amp;rft.atitle=Acquisition+of+chess+knowledge+in+AlphaZero&amp;rft.volume=119&amp;rft.issue=47&amp;rft.pages=e2206625119&amp;rft.date=2022-11-22&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9704706%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2022PNAS..11906625M&amp;rft_id=info%3Aarxiv%2F2111.09259&amp;rft.issn=0027-8424&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.2206625119&amp;rft_id=info%3Apmid%2F36375061&amp;rft.aulast=McGrath&amp;rft.aufirst=Thomas&amp;rft.au=Kapishnikov%2C+Andrei&amp;rft.au=Toma%C5%A1ev%2C+Nenad&amp;rft.au=Pearce%2C+Adam&amp;rft.au=Wattenberg%2C+Martin&amp;rft.au=Hassabis%2C+Demis&amp;rft.au=Kim%2C+Been&amp;rft.au=Paquet%2C+Ulrich&amp;rft.au=Kramnik%2C+Vladimir&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC9704706&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGohCammarataVossCarter2021" class="citation journal cs1">Goh, Gabriel; Cammarata, Nick; Voss, Chelsea; Carter, Shan; Petrov, Michael; Schubert, Ludwig; Radford, Alec; Olah, Chris (2021). <a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00030">"Multimodal neurons in artificial neural networks"</a>. <i>Distill</i>. <b>6</b> (3). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00030">10.23915/distill.00030</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233823418">233823418</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=Multimodal+neurons+in+artificial+neural+networks&amp;rft.volume=6&amp;rft.issue=3&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.23915%2Fdistill.00030&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233823418%23id-name%3DS2CID&amp;rft.aulast=Goh&amp;rft.aufirst=Gabriel&amp;rft.au=Cammarata%2C+Nick&amp;rft.au=Voss%2C+Chelsea&amp;rft.au=Carter%2C+Shan&amp;rft.au=Petrov%2C+Michael&amp;rft.au=Schubert%2C+Ludwig&amp;rft.au=Radford%2C+Alec&amp;rft.au=Olah%2C+Chris&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.23915%252Fdistill.00030&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOlahCammarataSchubertGoh2020" class="citation journal cs1">Olah, Chris; Cammarata, Nick; Schubert, Ludwig; Goh, Gabriel; Petrov, Michael; Carter, Shan (2020). <a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00024.001">"Zoom in: An introduction to circuits"</a>. <i>Distill</i>. <b>5</b> (3). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00024.001">10.23915/distill.00024.001</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:215930358">215930358</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=Zoom+in%3A+An+introduction+to+circuits&amp;rft.volume=5&amp;rft.issue=3&amp;rft.date=2020&amp;rft_id=info%3Adoi%2F10.23915%2Fdistill.00024.001&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A215930358%23id-name%3DS2CID&amp;rft.aulast=Olah&amp;rft.aufirst=Chris&amp;rft.au=Cammarata%2C+Nick&amp;rft.au=Schubert%2C+Ludwig&amp;rft.au=Goh%2C+Gabriel&amp;rft.au=Petrov%2C+Michael&amp;rft.au=Carter%2C+Shan&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.23915%252Fdistill.00024.001&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-79">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCammarataGohCarterVoss2021" class="citation journal cs1">Cammarata, Nick; Goh, Gabriel; Carter, Shan; Voss, Chelsea; Schubert, Ludwig; Olah, Chris (2021). <a rel="nofollow" class="external text" href="https://distill.pub/2020/circuits/curve-circuits/">"Curve circuits"</a>. <i>Distill</i>. <b>6</b> (1). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00024.006">10.23915/distill.00024.006</a> (inactive 31 January 2024). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221205140056/https://distill.pub/2020/circuits/curve-circuits/">Archived</a> from the original on 5 December 2022<span class="reference-accessdate">. Retrieved <span class="nowrap">5 December</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=Curve+circuits&amp;rft.volume=6&amp;rft.issue=1&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.23915%2Fdistill.00024.006&amp;rft.aulast=Cammarata&amp;rft.aufirst=Nick&amp;rft.au=Goh%2C+Gabriel&amp;rft.au=Carter%2C+Shan&amp;rft.au=Voss%2C+Chelsea&amp;rft.au=Schubert%2C+Ludwig&amp;rft.au=Olah%2C+Chris&amp;rft_id=https%3A%2F%2Fdistill.pub%2F2020%2Fcircuits%2Fcurve-circuits%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>:  CS1 maint: DOI inactive as of January 2024 (<a href="/wiki/Category:CS1_maint:_DOI_inactive_as_of_January_2024" title="Category:CS1 maint: DOI inactive as of January 2024">link</a>)</span></span>
</li>
<li id="cite_note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOlssonElhageNandaJoseph2022" class="citation journal cs1">Olsson, Catherine; Elhage, Nelson; Nanda, Neel; Joseph, Nicholas; DasSarma, Nova; Henighan, Tom; Mann, Ben; Askell, Amanda; Bai, Yuntao; Chen, Anna; Conerly, Tom; Drain, Dawn; Ganguli, Deep; Hatfield-Dodds, Zac; Hernandez, Danny; Johnston, Scott; Jones, Andy; Kernion, Jackson; Lovitt, Liane; Ndousse, Kamal; Amodei, Dario; Brown, Tom; Clark, Jack; Kaplan, Jared; McCandlish, Sam; Olah, Chris (2022). "In-context learning and induction heads". <i>Transformer Circuits Thread</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2209.11895">2209.11895</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Transformer+Circuits+Thread&amp;rft.atitle=In-context+learning+and+induction+heads&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2209.11895&amp;rft.aulast=Olsson&amp;rft.aufirst=Catherine&amp;rft.au=Elhage%2C+Nelson&amp;rft.au=Nanda%2C+Neel&amp;rft.au=Joseph%2C+Nicholas&amp;rft.au=DasSarma%2C+Nova&amp;rft.au=Henighan%2C+Tom&amp;rft.au=Mann%2C+Ben&amp;rft.au=Askell%2C+Amanda&amp;rft.au=Bai%2C+Yuntao&amp;rft.au=Chen%2C+Anna&amp;rft.au=Conerly%2C+Tom&amp;rft.au=Drain%2C+Dawn&amp;rft.au=Ganguli%2C+Deep&amp;rft.au=Hatfield-Dodds%2C+Zac&amp;rft.au=Hernandez%2C+Danny&amp;rft.au=Johnston%2C+Scott&amp;rft.au=Jones%2C+Andy&amp;rft.au=Kernion%2C+Jackson&amp;rft.au=Lovitt%2C+Liane&amp;rft.au=Ndousse%2C+Kamal&amp;rft.au=Amodei%2C+Dario&amp;rft.au=Brown%2C+Tom&amp;rft.au=Clark%2C+Jack&amp;rft.au=Kaplan%2C+Jared&amp;rft.au=McCandlish%2C+Sam&amp;rft.au=Olah%2C+Chris&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-81">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOlah" class="citation web cs1">Olah, Christopher. <a rel="nofollow" class="external text" href="https://colah.github.io/notes/interp-v-neuro/">"Interpretability vs Neuroscience &#91;rough note&#93;"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124114744/https://colah.github.io/notes/interp-v-neuro/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Interpretability+vs+Neuroscience+%5Brough+note%5D&amp;rft.aulast=Olah&amp;rft.aufirst=Christopher&amp;rft_id=https%3A%2F%2Fcolah.github.io%2Fnotes%2Finterp-v-neuro%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-82"><span class="mw-cite-backlink"><b><a href="#cite_ref-82">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGuDolan-GavittGarg2019" class="citation journal cs1">Gu, Tianyu; Dolan-Gavitt, Brendan; Garg, Siddharth (2019-03-11). "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1708.06733">1708.06733</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=BadNets%3A+Identifying+Vulnerabilities+in+the+Machine+Learning+Model+Supply+Chain&amp;rft.date=2019-03-11&amp;rft_id=info%3Aarxiv%2F1708.06733&amp;rft.aulast=Gu&amp;rft.aufirst=Tianyu&amp;rft.au=Dolan-Gavitt%2C+Brendan&amp;rft.au=Garg%2C+Siddharth&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-83"><span class="mw-cite-backlink"><b><a href="#cite_ref-83">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFChenLiuLiLu2017" class="citation journal cs1">Chen, Xinyun; Liu, Chang; Li, Bo; Lu, Kimberly; Song, Dawn (2017-12-14). "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1712.05526">1712.05526</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Targeted+Backdoor+Attacks+on+Deep+Learning+Systems+Using+Data+Poisoning&amp;rft.date=2017-12-14&amp;rft_id=info%3Aarxiv%2F1712.05526&amp;rft.aulast=Chen&amp;rft.aufirst=Xinyun&amp;rft.au=Liu%2C+Chang&amp;rft.au=Li%2C+Bo&amp;rft.au=Lu%2C+Kimberly&amp;rft.au=Song%2C+Dawn&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-84"><span class="mw-cite-backlink"><b><a href="#cite_ref-84">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCarliniTerzis2022" class="citation journal cs1">Carlini, Nicholas; Terzis, Andreas (2022-03-28). "Poisoning and Backdooring Contrastive Learning". <i>ICLR</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2106.09667">2106.09667</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ICLR&amp;rft.atitle=Poisoning+and+Backdooring+Contrastive+Learning&amp;rft.date=2022-03-28&amp;rft_id=info%3Aarxiv%2F2106.09667&amp;rft.aulast=Carlini&amp;rft.aufirst=Nicholas&amp;rft.au=Terzis%2C+Andreas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_aima4-85"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_aima4_85-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_aima4_85-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AI_alignment_aima4_85-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-AI_alignment_aima4_85-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text">
<link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRussellNorvig2021" class="citation book cs1">Russell, Stuart J.; Norvig, Peter (2021). <a rel="nofollow" class="external text" href="https://www.pearson.com/us/higher-education/program/Russell-Artificial-Intelligence-A-Modern-Approach-4th-Edition/PGM1263338.html"><i>Artificial intelligence: A modern approach</i></a> (4th&#160;ed.). Pearson. pp.&#160;5, 1003. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780134610993" title="Special:BookSources/9780134610993"><bdi>9780134610993</bdi></a><span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+intelligence%3A+A+modern+approach&amp;rft.pages=5%2C+1003&amp;rft.edition=4th&amp;rft.pub=Pearson&amp;rft.date=2021&amp;rft.isbn=9780134610993&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rft_id=https%3A%2F%2Fwww.pearson.com%2Fus%2Fhigher-education%2Fprogram%2FRussell-Artificial-Intelligence-A-Modern-Approach-4th-Edition%2FPGM1263338.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_dlp2023-86"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_dlp2023_86-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_dlp2023_86-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNgoChanMindermann2022" class="citation journal cs1">Ngo, Richard; Chan, Lawrence; Mindermann, Sören (2022). "The Alignment Problem from a Deep Learning Perspective". <i>International Conference on Learning Representations</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2209.00626">2209.00626</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Conference+on+Learning+Representations&amp;rft.atitle=The+Alignment+Problem+from+a+Deep+Learning+Perspective&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2209.00626&amp;rft.aulast=Ngo&amp;rft.aufirst=Richard&amp;rft.au=Chan%2C+Lawrence&amp;rft.au=Mindermann%2C+S%C3%B6ren&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_mmmm2022-87"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_mmmm2022_87-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_mmmm2022_87-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFPanBhatiaSteinhardt2022" class="citation conference cs1">Pan, Alexander; Bhatia, Kush; Steinhardt, Jacob (2022-02-14). <a rel="nofollow" class="external text" href="https://openreview.net/forum?id=JYtwGwIL7ye"><i>The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</i></a>. International Conference on Learning Representations<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=The+Effects+of+Reward+Misspecification%3A+Mapping+and+Mitigating+Misaligned+Models&amp;rft.date=2022-02-14&amp;rft.aulast=Pan&amp;rft.aufirst=Alexander&amp;rft.au=Bhatia%2C+Kush&amp;rft.au=Steinhardt%2C+Jacob&amp;rft_id=https%3A%2F%2Fopenreview.net%2Fforum%3Fid%3DJYtwGwIL7ye&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-88"><span class="mw-cite-backlink"><b><a href="#cite_ref-88">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZhuangHadfield-Menell2020" class="citation conference cs1">Zhuang, Simon; Hadfield-Menell, Dylan (2020). <a rel="nofollow" class="external text" href="https://proceedings.neurips.cc/paper/2020/hash/b607ba543ad05417b8507ee86c54fcb7-Abstract.html">"Consequences of Misaligned AI"</a>. <i>Advances in Neural Information Processing Systems</i>. Vol.&#160;33. Curran Associates, Inc. pp.&#160;15763–15773<span class="reference-accessdate">. Retrieved <span class="nowrap">2023-03-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Consequences+of+Misaligned+AI&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.pages=15763-15773&amp;rft.pub=Curran+Associates%2C+Inc.&amp;rft.date=2020&amp;rft.aulast=Zhuang&amp;rft.aufirst=Simon&amp;rft.au=Hadfield-Menell%2C+Dylan&amp;rft_id=https%3A%2F%2Fproceedings.neurips.cc%2Fpaper%2F2020%2Fhash%2Fb607ba543ad05417b8507ee86c54fcb7-Abstract.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_Carlsmith2022-89"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_Carlsmith2022_89-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCarlsmith2022" class="citation arxiv cs1">Carlsmith, Joseph (2022-06-16). "Is Power-Seeking AI an Existential Risk?". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2206.13353">2206.13353</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CY">cs.CY</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Is+Power-Seeking+AI+an+Existential+Risk%3F&amp;rft.date=2022-06-16&amp;rft_id=info%3Aarxiv%2F2206.13353&amp;rft.aulast=Carlsmith&amp;rft.aufirst=Joseph&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_:2102-90"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_:2102_90-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_:2102_90-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AI_alignment_:2102_90-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRussell2020" class="citation book cs1">Russell, Stuart J. (2020). <a rel="nofollow" class="external text" href="https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/"><i>Human compatible: Artificial intelligence and the problem of control</i></a>. Penguin Random House. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780525558637" title="Special:BookSources/9780525558637"><bdi>9780525558637</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1113410915">1113410915</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Human+compatible%3A+Artificial+intelligence+and+the+problem+of+control&amp;rft.pub=Penguin+Random+House&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1113410915&amp;rft.isbn=9780525558637&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft_id=https%3A%2F%2Fwww.penguinrandomhouse.com%2Fbooks%2F566677%2Fhuman-compatible-by-stuart-russell%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_Christian2020-91"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_Christian2020_91-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFChristian2020" class="citation book cs1">Christian, Brian (2020). <a rel="nofollow" class="external text" href="https://wwnorton.co.uk/books/9780393635829-the-alignment-problem"><i>The alignment problem: Machine learning and human values</i></a>. W. W. Norton &amp; Company. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-393-86833-3" title="Special:BookSources/978-0-393-86833-3"><bdi>978-0-393-86833-3</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1233266753">1233266753</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114137/https://wwnorton.co.uk/books/9780393635829-the-alignment-problem">Archived</a> from the original on February 10, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+alignment+problem%3A+Machine+learning+and+human+values&amp;rft.pub=W.+W.+Norton+%26+Company&amp;rft.date=2020&amp;rft_id=info%3Aoclcnum%2F1233266753&amp;rft.isbn=978-0-393-86833-3&amp;rft.aulast=Christian&amp;rft.aufirst=Brian&amp;rft_id=https%3A%2F%2Fwwnorton.co.uk%2Fbooks%2F9780393635829-the-alignment-problem&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_gmdrl-92"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_gmdrl_92-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFLangoscoKochSharkeyPfau2022" class="citation conference cs1">Langosco, Lauro Langosco Di; Koch, Jack; Sharkey, Lee D.; Pfau, Jacob; Krueger, David (2022-06-28). <a rel="nofollow" class="external text" href="https://proceedings.mlr.press/v162/langosco22a.html">"Goal Misgeneralization in Deep Reinforcement Learning"</a>. <i>Proceedings of the 39th International Conference on Machine Learning</i>. International Conference on Machine Learning. PMLR. pp.&#160;12004–12019<span class="reference-accessdate">. Retrieved <span class="nowrap">2023-03-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Goal+Misgeneralization+in+Deep+Reinforcement+Learning&amp;rft.btitle=Proceedings+of+the+39th+International+Conference+on+Machine+Learning&amp;rft.pages=12004-12019&amp;rft.pub=PMLR&amp;rft.date=2022-06-28&amp;rft.aulast=Langosco&amp;rft.aufirst=Lauro+Langosco+Di&amp;rft.au=Koch%2C+Jack&amp;rft.au=Sharkey%2C+Lee+D.&amp;rft.au=Pfau%2C+Jacob&amp;rft.au=Krueger%2C+David&amp;rft_id=https%3A%2F%2Fproceedings.mlr.press%2Fv162%2Flangosco22a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_Opportunities_Risks-93"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_Opportunities_Risks_93-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_Opportunities_Risks_93-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBommasaniHudsonAdeliAltman2022" class="citation journal cs1">Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik (2022-07-12). <a rel="nofollow" class="external text" href="https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models">"On the Opportunities and Risks of Foundation Models"</a>. <i>Stanford CRFM</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2108.07258">2108.07258</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Stanford+CRFM&amp;rft.atitle=On+the+Opportunities+and+Risks+of+Foundation+Models&amp;rft.date=2022-07-12&amp;rft_id=info%3Aarxiv%2F2108.07258&amp;rft.aulast=Bommasani&amp;rft.aufirst=Rishi&amp;rft.au=Hudson%2C+Drew+A.&amp;rft.au=Adeli%2C+Ehsan&amp;rft.au=Altman%2C+Russ&amp;rft.au=Arora%2C+Simran&amp;rft.au=von+Arx%2C+Sydney&amp;rft.au=Bernstein%2C+Michael+S.&amp;rft.au=Bohg%2C+Jeannette&amp;rft.au=Bosselut%2C+Antoine&amp;rft.au=Brunskill%2C+Emma&amp;rft.au=Brynjolfsson%2C+Erik&amp;rft_id=https%3A%2F%2Ffsi.stanford.edu%2Fpublication%2Fopportunities-and-risks-foundation-models&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_feedback2022-94"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_feedback2022_94-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOuyangWuJiangAlmeida2022" class="citation arxiv cs1">Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, J.; Hilton, Jacob; Kelton, Fraser; Miller, Luke E.; Simens, Maddie; Askell, Amanda; Welinder, P.; Christiano, P.; Leike, J.; Lowe, Ryan J. (2022). "Training language models to follow instructions with human feedback". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2203.02155">2203.02155</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Training+language+models+to+follow+instructions+with+human+feedback&amp;rft.date=2022&amp;rft_id=info%3Aarxiv%2F2203.02155&amp;rft.aulast=Ouyang&amp;rft.aufirst=Long&amp;rft.au=Wu%2C+Jeff&amp;rft.au=Jiang%2C+Xu&amp;rft.au=Almeida%2C+Diogo&amp;rft.au=Wainwright%2C+Carroll+L.&amp;rft.au=Mishkin%2C+Pamela&amp;rft.au=Zhang%2C+Chong&amp;rft.au=Agarwal%2C+Sandhini&amp;rft.au=Slama%2C+Katarina&amp;rft.au=Ray%2C+Alex&amp;rft.au=Schulman%2C+J.&amp;rft.au=Hilton%2C+Jacob&amp;rft.au=Kelton%2C+Fraser&amp;rft.au=Miller%2C+Luke+E.&amp;rft.au=Simens%2C+Maddie&amp;rft.au=Askell%2C+Amanda&amp;rft.au=Welinder%2C+P.&amp;rft.au=Christiano%2C+P.&amp;rft.au=Leike%2C+J.&amp;rft.au=Lowe%2C+Ryan+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_OpenAICodex-95"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_OpenAICodex_95-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZarembaBrockmanOpenAI2021" class="citation web cs1">Zaremba, Wojciech; Brockman, Greg; OpenAI (2021-08-10). <a rel="nofollow" class="external text" href="https://openai.com/blog/openai-codex/">"OpenAI Codex"</a>. <i>OpenAI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230203201912/https://openai.com/blog/openai-codex/">Archived</a> from the original on February 3, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=OpenAI+Codex&amp;rft.date=2021-08-10&amp;rft.aulast=Zaremba&amp;rft.aufirst=Wojciech&amp;rft.au=Brockman%2C+Greg&amp;rft.au=OpenAI&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fopenai-codex%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-96"><span class="mw-cite-backlink"><b><a href="#cite_ref-96">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKoberBagnellPeters2013" class="citation journal cs1">Kober, Jens; Bagnell, J. Andrew; Peters, Jan (2013-09-01). <a rel="nofollow" class="external text" href="http://journals.sagepub.com/doi/10.1177/0278364913495721">"Reinforcement learning in robotics: A survey"</a>. <i>The International Journal of Robotics Research</i>. <b>32</b> (11): 1238–1274. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1177%2F0278364913495721">10.1177/0278364913495721</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0278-3649">0278-3649</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:1932843">1932843</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221015200445/https://journals.sagepub.com/doi/10.1177/0278364913495721">Archived</a> from the original on October 15, 2022<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Robotics+Research&amp;rft.atitle=Reinforcement+learning+in+robotics%3A+A+survey&amp;rft.volume=32&amp;rft.issue=11&amp;rft.pages=1238-1274&amp;rft.date=2013-09-01&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A1932843%23id-name%3DS2CID&amp;rft.issn=0278-3649&amp;rft_id=info%3Adoi%2F10.1177%2F0278364913495721&amp;rft.aulast=Kober&amp;rft.aufirst=Jens&amp;rft.au=Bagnell%2C+J.+Andrew&amp;rft.au=Peters%2C+Jan&amp;rft_id=http%3A%2F%2Fjournals.sagepub.com%2Fdoi%2F10.1177%2F0278364913495721&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-97"><span class="mw-cite-backlink"><b><a href="#cite_ref-97">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKnoxAllieviBanzhafSchmitt2023" class="citation journal cs1">Knox, W. Bradley; Allievi, Alessandro; Banzhaf, Holger; Schmitt, Felix; Stone, Peter (2023-03-01). <a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.artint.2022.103829">"Reward (Mis)design for autonomous driving"</a>. <i>Artificial Intelligence</i>. <b>316</b>: 103829. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2104.13906">2104.13906</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.artint.2022.103829">10.1016/j.artint.2022.103829</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0004-3702">0004-3702</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233423198">233423198</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Artificial+Intelligence&amp;rft.atitle=Reward+%28Mis%29design+for+autonomous+driving&amp;rft.volume=316&amp;rft.pages=103829&amp;rft.date=2023-03-01&amp;rft_id=info%3Aarxiv%2F2104.13906&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233423198%23id-name%3DS2CID&amp;rft.issn=0004-3702&amp;rft_id=info%3Adoi%2F10.1016%2Fj.artint.2022.103829&amp;rft.aulast=Knox&amp;rft.aufirst=W.+Bradley&amp;rft.au=Allievi%2C+Alessandro&amp;rft.au=Banzhaf%2C+Holger&amp;rft.au=Schmitt%2C+Felix&amp;rft.au=Stone%2C+Peter&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1016%252Fj.artint.2022.103829&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-98"><span class="mw-cite-backlink"><b><a href="#cite_ref-98">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFStray2020" class="citation journal cs1">Stray, Jonathan (2020). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7610010">"Aligning AI Optimization to Community Well-Being"</a>. <i>International Journal of Community Well-Being</i>. <b>3</b> (4): 443–463. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs42413-020-00086-3">10.1007/s42413-020-00086-3</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2524-5295">2524-5295</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7610010">7610010</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/34723107">34723107</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:226254676">226254676</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Community+Well-Being&amp;rft.atitle=Aligning+AI+Optimization+to+Community+Well-Being&amp;rft.volume=3&amp;rft.issue=4&amp;rft.pages=443-463&amp;rft.date=2020&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7610010%23id-name%3DPMC&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A226254676%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2Fs42413-020-00086-3&amp;rft.issn=2524-5295&amp;rft_id=info%3Apmid%2F34723107&amp;rft.aulast=Stray&amp;rft.aufirst=Jonathan&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7610010&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_AIMA-99"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_AIMA_99-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRussellNorvig2009" class="citation book cs1">Russell, Stuart; Norvig, Peter (2009). <a rel="nofollow" class="external text" href="https://aima.cs.berkeley.edu/"><i>Artificial Intelligence: A Modern Approach</i></a>. Prentice Hall. p.&#160;1003. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-461099-3" title="Special:BookSources/978-0-13-461099-3"><bdi>978-0-13-461099-3</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pages=1003&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-461099-3&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rft_id=https%3A%2F%2Faima.cs.berkeley.edu%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-100"><span class="mw-cite-backlink"><b><a href="#cite_ref-100">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBengioHintonYaoSong2024" class="citation cs2">Bengio, Yoshua; Hinton, Geoffrey; Yao, Andrew; Song, Dawn; Abbeel, Pieter; Harari, Yuval Noah; Zhang, Ya-Qin; Xue, Lan; Shalev-Shwartz, Shai (2024), "Managing extreme AI risks amid rapid progress", <i>Science</i>, <b>384</b> (6698): 842–845, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2310.17688">2310.17688</a></span>, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.adn0117">10.1126/science.adn0117</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Managing+extreme+AI+risks+amid+rapid+progress&amp;rft.volume=384&amp;rft.issue=6698&amp;rft.pages=842-845&amp;rft.date=2024&amp;rft_id=info%3Aarxiv%2F2310.17688&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.adn0117&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rft.au=Hinton%2C+Geoffrey&amp;rft.au=Yao%2C+Andrew&amp;rft.au=Song%2C+Dawn&amp;rft.au=Abbeel%2C+Pieter&amp;rft.au=Harari%2C+Yuval+Noah&amp;rft.au=Zhang%2C+Ya-Qin&amp;rft.au=Xue%2C+Lan&amp;rft.au=Shalev-Shwartz%2C+Shai&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-101"><span class="mw-cite-backlink"><b><a href="#cite_ref-101">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.safe.ai/statement-on-ai-risk">"Statement on AI Risk | CAIS"</a>. <i>www.safe.ai</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-02-11</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.safe.ai&amp;rft.atitle=Statement+on+AI+Risk+%7C+CAIS&amp;rft_id=https%3A%2F%2Fwww.safe.ai%2Fstatement-on-ai-risk&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-102"><span class="mw-cite-backlink"><b><a href="#cite_ref-102">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGraceStewartSandkühlerThomas2024" class="citation cs2">Grace, Katja; Stewart, Harlan; Sandkühler, Julia Fabienne; Thomas, Stephen; Weinstein-Raun, Ben; Brauner, Jan (2024-01-05), <i>Thousands of AI Authors on the Future of AI</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2401.02843">2401.02843</a></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Thousands+of+AI+Authors+on+the+Future+of+AI&amp;rft.date=2024-01-05&amp;rft_id=info%3Aarxiv%2F2401.02843&amp;rft.aulast=Grace&amp;rft.aufirst=Katja&amp;rft.au=Stewart%2C+Harlan&amp;rft.au=Sandk%C3%BChler%2C+Julia+Fabienne&amp;rft.au=Thomas%2C+Stephen&amp;rft.au=Weinstein-Raun%2C+Ben&amp;rft.au=Brauner%2C+Jan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_:2-103"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_:2_103-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSmith" class="citation web cs1">Smith, Craig S. <a rel="nofollow" class="external text" href="https://www.forbes.com/sites/craigsmith/2023/05/04/geoff-hinton-ais-most-famous-researcher-warns-of-existential-threat/">"Geoff Hinton, AI's Most Famous Researcher, Warns Of 'Existential Threat'<span class="cs1-kern-right"></span>"</a>. <i>Forbes</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-05-04</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Forbes&amp;rft.atitle=Geoff+Hinton%2C+AI%27s+Most+Famous+Researcher%2C+Warns+Of+%27Existential+Threat%27&amp;rft.aulast=Smith&amp;rft.aufirst=Craig+S.&amp;rft_id=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fcraigsmith%2F2023%2F05%2F04%2Fgeoff-hinton-ais-most-famous-researcher-warns-of-existential-threat%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-104"><span class="mw-cite-backlink"><b><a href="#cite_ref-104">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFPerrigo2024" class="citation web cs1">Perrigo, Billy (2024-02-13). <a rel="nofollow" class="external text" href="https://time.com/6694432/yann-lecun-meta-ai-interview/">"Meta's AI Chief Yann LeCun on AGI, Open-Source, and AI Risk"</a>. <i>TIME</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-06-26</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=TIME&amp;rft.atitle=Meta%E2%80%99s+AI+Chief+Yann+LeCun+on+AGI%2C+Open-Source%2C+and+AI+Risk&amp;rft.date=2024-02-13&amp;rft.aulast=Perrigo&amp;rft.aufirst=Billy&amp;rft_id=https%3A%2F%2Ftime.com%2F6694432%2Fyann-lecun-meta-ai-interview%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_concrete2016-105"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_concrete2016_105-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_concrete2016_105-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-AI_alignment_concrete2016_105-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAmodeiOlahSteinhardtChristiano2016" class="citation arxiv cs1">Amodei, Dario; Olah, Chris; Steinhardt, Jacob; Christiano, Paul; Schulman, John; Mané, Dan (2016-06-21). "Concrete Problems in AI Safety". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1606.06565">1606.06565</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.AI">cs.AI</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Concrete+Problems+in+AI+Safety&amp;rft.date=2016-06-21&amp;rft_id=info%3Aarxiv%2F1606.06565&amp;rft.aulast=Amodei&amp;rft.aufirst=Dario&amp;rft.au=Olah%2C+Chris&amp;rft.au=Steinhardt%2C+Jacob&amp;rft.au=Christiano%2C+Paul&amp;rft.au=Schulman%2C+John&amp;rft.au=Man%C3%A9%2C+Dan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_building2018-106"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_building2018_106-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_building2018_106-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOrtegaMainiDeepMind_safety_team2018" class="citation web cs1">Ortega, Pedro A.; Maini, Vishal; DeepMind safety team (2018-09-27). <a rel="nofollow" class="external text" href="https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1">"Building safe artificial intelligence: specification, robustness, and assurance"</a>. <i>DeepMind Safety Research – Medium</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114142/https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1">Archived</a> from the original on February 10, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-18</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=DeepMind+Safety+Research+%E2%80%93+Medium&amp;rft.atitle=Building+safe+artificial+intelligence%3A+specification%2C+robustness%2C+and+assurance&amp;rft.date=2018-09-27&amp;rft.aulast=Ortega&amp;rft.aufirst=Pedro+A.&amp;rft.au=Maini%2C+Vishal&amp;rft.au=DeepMind+safety+team&amp;rft_id=https%3A%2F%2Fdeepmindsafetyresearch.medium.com%2Fbuilding-safe-artificial-intelligence-52f5f75058f1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_:333-107"><span class="mw-cite-backlink">^ <a href="#cite_ref-AI_alignment_:333_107-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-AI_alignment_:333_107-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRorvig2022" class="citation web cs1">Rorvig, Mordechai (2022-04-14). <a rel="nofollow" class="external text" href="https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/">"Researchers Gain New Understanding From Simple AI"</a>. <i>Quanta Magazine</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114056/https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/">Archived</a> from the original on February 10, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-18</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quanta+Magazine&amp;rft.atitle=Researchers+Gain+New+Understanding+From+Simple+AI&amp;rft.date=2022-04-14&amp;rft.aulast=Rorvig&amp;rft.aufirst=Mordechai&amp;rft_id=https%3A%2F%2Fwww.quantamagazine.org%2Fresearchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-108"><span class="mw-cite-backlink"><b><a href="#cite_ref-108">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDoshi-VelezKim2017" class="citation arxiv cs1">Doshi-Velez, Finale; Kim, Been (2017-03-02). "Towards A Rigorous Science of Interpretable Machine Learning". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1702.08608">1702.08608</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Towards+A+Rigorous+Science+of+Interpretable+Machine+Learning&amp;rft.date=2017-03-02&amp;rft_id=info%3Aarxiv%2F1702.08608&amp;rft.aulast=Doshi-Velez&amp;rft.aufirst=Finale&amp;rft.au=Kim%2C+Been&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWiblin2021" class="citation podcast cs1">Wiblin, Robert (August 4, 2021). <a rel="nofollow" class="external text" href="https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/">"Chris Olah on what the hell is going on inside neural networks"</a> (Podcast). 80,000 hours. No.&#160;107<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Chris+Olah+on+what+the+hell+is+going+on+inside+neural+networks&amp;rft.series=80%2C000+hours&amp;rft.date=2021-08-04&amp;rft.aulast=Wiblin&amp;rft.aufirst=Robert&amp;rft_id=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fchris-olah-interpretability-research%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></li></ul>
</span></li>
<li id="cite_note-109"><span class="mw-cite-backlink"><b><a href="#cite_ref-109">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRussellDeweyTegmark2015" class="citation journal cs1">Russell, Stuart; Dewey, Daniel; Tegmark, Max (2015-12-31). <a rel="nofollow" class="external text" href="https://ojs.aaai.org/index.php/aimagazine/article/view/2577">"Research Priorities for Robust and Beneficial Artificial Intelligence"</a>. <i>AI Magazine</i>. <b>36</b> (4): 105–114. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1602.03506">1602.03506</a></span>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1609%2Faimag.v36i4.2577">10.1609/aimag.v36i4.2577</a></span>. <a href="/wiki/Hdl_(identifier)" class="mw-redirect" title="Hdl (identifier)">hdl</a>:<a rel="nofollow" class="external text" href="https://hdl.handle.net/1721.1%2F108478">1721.1/108478</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2371-9621">2371-9621</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:8174496">8174496</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230202181059/https://ojs.aaai.org/index.php/aimagazine/article/view/2577">Archived</a> from the original on February 2, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+Magazine&amp;rft.atitle=Research+Priorities+for+Robust+and+Beneficial+Artificial+Intelligence&amp;rft.volume=36&amp;rft.issue=4&amp;rft.pages=105-114&amp;rft.date=2015-12-31&amp;rft_id=info%3Ahdl%2F1721.1%2F108478&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A8174496%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1609%2Faimag.v36i4.2577&amp;rft_id=info%3Aarxiv%2F1602.03506&amp;rft.issn=2371-9621&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Dewey%2C+Daniel&amp;rft.au=Tegmark%2C+Max&amp;rft_id=https%3A%2F%2Fojs.aaai.org%2Findex.php%2Faimagazine%2Farticle%2Fview%2F2577&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_prefsurvey2017-110"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_prefsurvey2017_110-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWirthAkrourNeumannFürnkranz2017" class="citation journal cs1">Wirth, Christian; Akrour, Riad; Neumann, Gerhard; Fürnkranz, Johannes (2017). "A survey of preference-based reinforcement learning methods". <i>Journal of Machine Learning Research</i>. <b>18</b> (136): 1–46.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=A+survey+of+preference-based+reinforcement+learning+methods&amp;rft.volume=18&amp;rft.issue=136&amp;rft.pages=1-46&amp;rft.date=2017&amp;rft.aulast=Wirth&amp;rft.aufirst=Christian&amp;rft.au=Akrour%2C+Riad&amp;rft.au=Neumann%2C+Gerhard&amp;rft.au=F%C3%BCrnkranz%2C+Johannes&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_drlfhp-111"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_drlfhp_111-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFChristianoLeikeBrownMartic2017" class="citation conference cs1">Christiano, Paul F.; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario (2017). "Deep reinforcement learning from human preferences". <i>Proceedings of the 31st International Conference on Neural Information Processing Systems</i>. NIPS'17. Red Hook, NY, USA: Curran Associates Inc. pp.&#160;4302–4310. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-5108-6096-4" title="Special:BookSources/978-1-5108-6096-4"><bdi>978-1-5108-6096-4</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Deep+reinforcement+learning+from+human+preferences&amp;rft.btitle=Proceedings+of+the+31st+International+Conference+on+Neural+Information+Processing+Systems&amp;rft.place=Red+Hook%2C+NY%2C+USA&amp;rft.series=NIPS%2717&amp;rft.pages=4302-4310&amp;rft.pub=Curran+Associates+Inc.&amp;rft.date=2017&amp;rft.isbn=978-1-5108-6096-4&amp;rft.aulast=Christiano&amp;rft.aufirst=Paul+F.&amp;rft.au=Leike%2C+Jan&amp;rft.au=Brown%2C+Tom+B.&amp;rft.au=Martic%2C+Miljan&amp;rft.au=Legg%2C+Shane&amp;rft.au=Amodei%2C+Dario&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_LessToxic-112"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_LessToxic_112-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHeaven2022" class="citation web cs1">Heaven, Will Douglas (2022-01-27). <a rel="nofollow" class="external text" href="https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/">"The new version of GPT-3 is much better behaved (and should be less toxic)"</a>. <i>MIT Technology Review</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114056/https://www.technologyreview.com/2022/01/27/1044398/new-gpt3-openai-chatbot-language-model-ai-toxic-misinformation/">Archived</a> from the original on February 10, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-18</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=The+new+version+of+GPT-3+is+much+better+behaved+%28and+should+be+less+toxic%29&amp;rft.date=2022-01-27&amp;rft.aulast=Heaven&amp;rft.aufirst=Will+Douglas&amp;rft_id=https%3A%2F%2Fwww.technologyreview.com%2F2022%2F01%2F27%2F1044398%2Fnew-gpt3-openai-chatbot-language-model-ai-toxic-misinformation%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-113"><span class="mw-cite-backlink"><b><a href="#cite_ref-113">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMohseniWangYuXiao2022" class="citation arxiv cs1">Mohseni, Sina; Wang, Haotao; Yu, Zhiding; Xiao, Chaowei; Wang, Zhangyang; Yadawa, Jay (2022-03-07). "Taxonomy of Machine Learning Safety: A Survey and Primer". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2106.04823">2106.04823</a></span> [<a rel="nofollow" class="external text" href="https://arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Taxonomy+of+Machine+Learning+Safety%3A+A+Survey+and+Primer&amp;rft.date=2022-03-07&amp;rft_id=info%3Aarxiv%2F2106.04823&amp;rft.aulast=Mohseni&amp;rft.aufirst=Sina&amp;rft.au=Wang%2C+Haotao&amp;rft.au=Yu%2C+Zhiding&amp;rft.au=Xiao%2C+Chaowei&amp;rft.au=Wang%2C+Zhangyang&amp;rft.au=Yadawa%2C+Jay&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-114"><span class="mw-cite-backlink"><b><a href="#cite_ref-114">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFClifton2020" class="citation web cs1">Clifton, Jesse (2020). <a rel="nofollow" class="external text" href="https://longtermrisk.org/research-agenda/">"Cooperation, Conflict, and Transformative Artificial Intelligence: A Research Agenda"</a>. <i>Center on Long-Term Risk</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230101041759/https://longtermrisk.org/research-agenda">Archived</a> from the original on January 1, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-07-18</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Center+on+Long-Term+Risk&amp;rft.atitle=Cooperation%2C+Conflict%2C+and+Transformative+Artificial+Intelligence%3A+A+Research+Agenda&amp;rft.date=2020&amp;rft.aulast=Clifton&amp;rft.aufirst=Jesse&amp;rft_id=https%3A%2F%2Flongtermrisk.org%2Fresearch-agenda%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span>
<ul><li><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDafoeBachrachHadfieldHorvitz2021" class="citation journal cs1">Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore (2021-05-06). <a rel="nofollow" class="external text" href="http://www.nature.com/articles/d41586-021-01170-0">"Cooperative AI: machines must learn to find common ground"</a>. <i>Nature</i>. <b>593</b> (7857): 33–36. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2021Natur.593...33D">2021Natur.593...33D</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-021-01170-0">10.1038/d41586-021-01170-0</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0028-0836">0028-0836</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/33947992">33947992</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233740521">233740521</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221218210857/https://www.nature.com/articles/d41586-021-01170-0">Archived</a> from the original on December 18, 2022<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Cooperative+AI%3A+machines+must+learn+to+find+common+ground&amp;rft.volume=593&amp;rft.issue=7857&amp;rft.pages=33-36&amp;rft.date=2021-05-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740521%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2021Natur.593...33D&amp;rft.issn=0028-0836&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-021-01170-0&amp;rft_id=info%3Apmid%2F33947992&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rft.au=Bachrach%2C+Yoram&amp;rft.au=Hadfield%2C+Gillian&amp;rft.au=Horvitz%2C+Eric&amp;rft.au=Larson%2C+Kate&amp;rft.au=Graepel%2C+Thore&amp;rft_id=http%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-021-01170-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></li></ul>
</span></li>
<li id="cite_note-115"><span class="mw-cite-backlink"><b><a href="#cite_ref-115">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFPrunklWhittlestone2020" class="citation book cs1">Prunkl, Carina; Whittlestone, Jess (2020-02-07). <a rel="nofollow" class="external text" href="https://dl.acm.org/doi/10.1145/3375627.3375803">"Beyond Near- and Long-Term"</a>. <i>Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society</i>. New York NY USA: ACM. pp.&#160;138–143. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3375627.3375803">10.1145/3375627.3375803</a>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-7110-0" title="Special:BookSources/978-1-4503-7110-0"><bdi>978-1-4503-7110-0</bdi></a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:210164673">210164673</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221016123733/https://dl.acm.org/doi/10.1145/3375627.3375803">Archived</a> from the original on October 16, 2022<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Beyond+Near-+and+Long-Term&amp;rft.btitle=Proceedings+of+the+AAAI%2FACM+Conference+on+AI%2C+Ethics%2C+and+Society&amp;rft.place=New+York+NY+USA&amp;rft.pages=138-143&amp;rft.pub=ACM&amp;rft.date=2020-02-07&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A210164673%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1145%2F3375627.3375803&amp;rft.isbn=978-1-4503-7110-0&amp;rft.aulast=Prunkl&amp;rft.aufirst=Carina&amp;rft.au=Whittlestone%2C+Jess&amp;rft_id=https%3A%2F%2Fdl.acm.org%2Fdoi%2F10.1145%2F3375627.3375803&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-AI_alignment_:4-116"><span class="mw-cite-backlink"><b><a href="#cite_ref-AI_alignment_:4_116-0">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFIrvingAskell2019" class="citation journal cs1">Irving, Geoffrey; Askell, Amanda (2019-02-19). <a rel="nofollow" class="external text" href="https://distill.pub/2019/safety-needs-social-scientists">"AI Safety Needs Social Scientists"</a>. <i>Distill</i>. <b>4</b> (2): 10.23915/distill.00014. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.23915%2Fdistill.00014">10.23915/distill.00014</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2476-0757">2476-0757</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:159180422">159180422</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114220/https://distill.pub/2019/safety-needs-social-scientists/">Archived</a> from the original on February 10, 2023<span class="reference-accessdate">. Retrieved <span class="nowrap">September 12,</span> 2022</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Distill&amp;rft.atitle=AI+Safety+Needs+Social+Scientists&amp;rft.volume=4&amp;rft.issue=2&amp;rft.pages=10.23915%2Fdistill.00014&amp;rft.date=2019-02-19&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A159180422%23id-name%3DS2CID&amp;rft.issn=2476-0757&amp;rft_id=info%3Adoi%2F10.23915%2Fdistill.00014&amp;rft.aulast=Irving&amp;rft.aufirst=Geoffrey&amp;rft.au=Askell%2C+Amanda&amp;rft_id=https%3A%2F%2Fdistill.pub%2F2019%2Fsafety-needs-social-scientists&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:12-117"><span class="mw-cite-backlink">^ <a href="#cite_ref-:12_117-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:12_117-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:12_117-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-:12_117-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZwetslootDafoe2019" class="citation web cs1">Zwetsloot, Remco; Dafoe, Allan (2019-02-11). <a rel="nofollow" class="external text" href="https://www.lawfaremedia.org/article/thinking-about-risks-ai-accidents-misuse-and-structure">"Thinking About Risks From AI: Accidents, Misuse and Structure"</a>. <i>Lawfare</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230819035804/https://www.lawfaremedia.org/article/thinking-about-risks-ai-accidents-misuse-and-structure">Archived</a> from the original on 2023-08-19<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Lawfare&amp;rft.atitle=Thinking+About+Risks+From+AI%3A+Accidents%2C+Misuse+and+Structure&amp;rft.date=2019-02-11&amp;rft.aulast=Zwetsloot&amp;rft.aufirst=Remco&amp;rft.au=Dafoe%2C+Allan&amp;rft_id=https%3A%2F%2Fwww.lawfaremedia.org%2Farticle%2Fthinking-about-risks-ai-accidents-misuse-and-structure&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-118"><span class="mw-cite-backlink"><b><a href="#cite_ref-118">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZhangDongGuoDai2022" class="citation journal cs1">Zhang, Yingyu; Dong, Chuntong; Guo, Weiqun; Dai, Jiabao; Zhao, Ziming (2022). <a rel="nofollow" class="external text" href="https://linkinghub.elsevier.com/retrieve/pii/S0925753521004367">"Systems theoretic accident model and process (STAMP): A literature review"</a>. <i>Safety Science</i>. <b>152</b>: 105596. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.ssci.2021.105596">10.1016/j.ssci.2021.105596</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:244550153">244550153</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230315184342/https://www.sciencedirect.com/science/article/abs/pii/S0925753521004367?via%3Dihub">Archived</a> from the original on 2023-03-15<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Safety+Science&amp;rft.atitle=Systems+theoretic+accident+model+and+process+%28STAMP%29%3A+A+literature+review&amp;rft.volume=152&amp;rft.pages=105596&amp;rft.date=2022&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ssci.2021.105596&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A244550153%23id-name%3DS2CID&amp;rft.aulast=Zhang&amp;rft.aufirst=Yingyu&amp;rft.au=Dong%2C+Chuntong&amp;rft.au=Guo%2C+Weiqun&amp;rft.au=Dai%2C+Jiabao&amp;rft.au=Zhao%2C+Ziming&amp;rft_id=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0925753521004367&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-119"><span class="mw-cite-backlink"><b><a href="#cite_ref-119">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCenter_for_Security_and_Emerging_TechnologyHoffman2021" class="citation journal cs1">Center for Security and Emerging Technology; Hoffman, Wyatt (2021). <a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/">"AI and the Future of Cyber Competition"</a>. <i>CSET Issue Brief</i>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.51593%2F2020ca007">10.51593/2020ca007</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:234245812">234245812</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124122253/https://cset.georgetown.edu/publication/ai-and-the-future-of-cyber-competition/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=CSET+Issue+Brief&amp;rft.atitle=AI+and+the+Future+of+Cyber+Competition&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.51593%2F2020ca007&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A234245812%23id-name%3DS2CID&amp;rft.au=Center+for+Security+and+Emerging+Technology&amp;rft.au=Hoffman%2C+Wyatt&amp;rft_id=https%3A%2F%2Fcset.georgetown.edu%2Fpublication%2Fai-and-the-future-of-cyber-competition%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-120"><span class="mw-cite-backlink"><b><a href="#cite_ref-120">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCenter_for_Security_and_Emerging_TechnologyImbrieKania2019" class="citation journal cs1">Center for Security and Emerging Technology; Imbrie, Andrew; Kania, Elsa (2019). <a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/ai-safety-security-and-stability-among-great-powers-options-challenges-and-lessons-learned-for-pragmatic-engagement/">"AI Safety, Security, and Stability Among Great Powers: Options, Challenges, and Lessons Learned for Pragmatic Engagement"</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.51593%2F20190051">10.51593/20190051</a></span>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:240957952">240957952</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124122652/https://cset.georgetown.edu/publication/ai-safety-security-and-stability-among-great-powers-options-challenges-and-lessons-learned-for-pragmatic-engagement/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=AI+Safety%2C+Security%2C+and+Stability+Among+Great+Powers%3A+Options%2C+Challenges%2C+and+Lessons+Learned+for+Pragmatic+Engagement&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.51593%2F20190051&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A240957952%23id-name%3DS2CID&amp;rft.au=Center+for+Security+and+Emerging+Technology&amp;rft.au=Imbrie%2C+Andrew&amp;rft.au=Kania%2C+Elsa&amp;rft_id=https%3A%2F%2Fcset.georgetown.edu%2Fpublication%2Fai-safety-security-and-stability-among-great-powers-options-challenges-and-lessons-learned-for-pragmatic-engagement%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-:11-121"><span class="mw-cite-backlink">^ <a href="#cite_ref-:11_121-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:11_121-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation audio-visual cs1">Future of Life Institute (2019-03-27). <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=2IpJ8TIKKtI"><i>AI Strategy, Policy, and Governance (Allan Dafoe)</i></a>.  Event occurs at 22:05. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221123055429/https://www.youtube.com/watch?v=2IpJ8TIKKtI">Archived</a> from the original on 2022-11-23<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-23</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=AI+Strategy%2C+Policy%2C+and+Governance+%28Allan+Dafoe%29&amp;rft.date=2019-03-27&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D2IpJ8TIKKtI&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-122"><span class="mw-cite-backlink"><b><a href="#cite_ref-122">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZouXiaoJiaKwon2022" class="citation journal cs1">Zou, Andy; Xiao, Tristan; Jia, Ryan; Kwon, Joe; Mazeika, Mantas; Li, Richard; Song, Dawn; Steinhardt, Jacob; Evans, Owain; Hendrycks, Dan (2022-10-09). "Forecasting Future World Events with Neural Networks". <i>NeurIPS</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2206.15474">2206.15474</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=Forecasting+Future+World+Events+with+Neural+Networks&amp;rft.date=2022-10-09&amp;rft_id=info%3Aarxiv%2F2206.15474&amp;rft.aulast=Zou&amp;rft.aufirst=Andy&amp;rft.au=Xiao%2C+Tristan&amp;rft.au=Jia%2C+Ryan&amp;rft.au=Kwon%2C+Joe&amp;rft.au=Mazeika%2C+Mantas&amp;rft.au=Li%2C+Richard&amp;rft.au=Song%2C+Dawn&amp;rft.au=Steinhardt%2C+Jacob&amp;rft.au=Evans%2C+Owain&amp;rft.au=Hendrycks%2C+Dan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-123"><span class="mw-cite-backlink"><b><a href="#cite_ref-123">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGathaniHulsebosGaleHaas2022" class="citation journal cs1">Gathani, Sneha; Hulsebos, Madelon; Gale, James; Haas, Peter J.; Demiralp, Çağatay (2022-02-08). "Augmenting Decision Making via Interactive What-If Analysis". <i>Conference on Innovative Data Systems Research</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2109.06160">2109.06160</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Conference+on+Innovative+Data+Systems+Research&amp;rft.atitle=Augmenting+Decision+Making+via+Interactive+What-If+Analysis&amp;rft.date=2022-02-08&amp;rft_id=info%3Aarxiv%2F2109.06160&amp;rft.aulast=Gathani&amp;rft.aufirst=Sneha&amp;rft.au=Hulsebos%2C+Madelon&amp;rft.au=Gale%2C+James&amp;rft.au=Haas%2C+Peter+J.&amp;rft.au=Demiralp%2C+%C3%87a%C4%9Fatay&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-124"><span class="mw-cite-backlink"><b><a href="#cite_ref-124">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFLindelauf2021" class="citation cs2">Lindelauf, Roy (2021), Osinga, Frans; Sweijs, Tim (eds.), "Nuclear Deterrence in the Algorithmic Age: Game Theory Revisited", <i>NL ARMS Netherlands Annual Review of Military Studies 2020</i>, Nl Arms, The Hague: T.M.C. Asser Press, pp.&#160;421–436, <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2F978-94-6265-419-8_22">10.1007/978-94-6265-419-8_22</a></span>, <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-94-6265-418-1" title="Special:BookSources/978-94-6265-418-1"><bdi>978-94-6265-418-1</bdi></a>, <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:229449677">229449677</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NL+ARMS+Netherlands+Annual+Review+of+Military+Studies+2020&amp;rft.atitle=Nuclear+Deterrence+in+the+Algorithmic+Age%3A+Game+Theory+Revisited&amp;rft.pages=421-436&amp;rft.date=2021&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A229449677%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.1007%2F978-94-6265-419-8_22&amp;rft.isbn=978-94-6265-418-1&amp;rft.aulast=Lindelauf&amp;rft.aufirst=Roy&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:14-125"><span class="mw-cite-backlink">^ <a href="#cite_ref-:14_125-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:14_125-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNewkirk_II2016" class="citation web cs1">Newkirk II, Vann R. (2016-04-21). <a rel="nofollow" class="external text" href="https://www.theatlantic.com/politics/archive/2016/04/climate-change-game-theory-models/624253/">"Is Climate Change a Prisoner's Dilemma or a Stag Hunt?"</a>. <i>The Atlantic</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124123011/https://www.theatlantic.com/politics/archive/2016/04/climate-change-game-theory-models/624253/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Atlantic&amp;rft.atitle=Is+Climate+Change+a+Prisoner%27s+Dilemma+or+a+Stag+Hunt%3F&amp;rft.date=2016-04-21&amp;rft.aulast=Newkirk+II&amp;rft.aufirst=Vann+R.&amp;rft_id=https%3A%2F%2Fwww.theatlantic.com%2Fpolitics%2Farchive%2F2016%2F04%2Fclimate-change-game-theory-models%2F624253%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:16-126"><span class="mw-cite-backlink">^ <a href="#cite_ref-:16_126-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:16_126-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFArmstrongBostromShulman" class="citation report cs1">Armstrong, Stuart; Bostrom, Nick; Shulman, Carl. Racing to the Precipice: a Model of Artificial Intelligence Development (Report). Future of Humanity Institute, Oxford University.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Racing+to+the+Precipice%3A+a+Model+of+Artificial+Intelligence+Development&amp;rft.pub=Future+of+Humanity+Institute%2C+Oxford+University&amp;rft.aulast=Armstrong&amp;rft.aufirst=Stuart&amp;rft.au=Bostrom%2C+Nick&amp;rft.au=Shulman%2C+Carl&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:17-127"><span class="mw-cite-backlink">^ <a href="#cite_ref-:17_127-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:17_127-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDafoe" class="citation report cs1">Dafoe, Allan. AI Governance: A Research Agenda (Report). Centre for the Governance of AI, Future of Humanity Institute, University of Oxford.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=AI+Governance%3A+A+Research+Agenda&amp;rft.pub=Centre+for+the+Governance+of+AI%2C+Future+of+Humanity+Institute%2C+University+of+Oxford&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-128"><span class="mw-cite-backlink"><b><a href="#cite_ref-128">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDafoeHughesBachrachCollins2020" class="citation journal cs1">Dafoe, Allan; Hughes, Edward; Bachrach, Yoram; Collins, Tantum; McKee, Kevin R.; Leibo, Joel Z.; Larson, Kate; Graepel, Thore (2020-12-15). "Open Problems in Cooperative AI". <i>NeurIPS</i>. <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2012.08630">2012.08630</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeurIPS&amp;rft.atitle=Open+Problems+in+Cooperative+AI&amp;rft.date=2020-12-15&amp;rft_id=info%3Aarxiv%2F2012.08630&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rft.au=Hughes%2C+Edward&amp;rft.au=Bachrach%2C+Yoram&amp;rft.au=Collins%2C+Tantum&amp;rft.au=McKee%2C+Kevin+R.&amp;rft.au=Leibo%2C+Joel+Z.&amp;rft.au=Larson%2C+Kate&amp;rft.au=Graepel%2C+Thore&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:15-129"><span class="mw-cite-backlink">^ <a href="#cite_ref-:15_129-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:15_129-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDafoeBachrachHadfieldHorvitz2021" class="citation journal cs1">Dafoe, Allan; Bachrach, Yoram; Hadfield, Gillian; Horvitz, Eric; Larson, Kate; Graepel, Thore (2021). <a rel="nofollow" class="external text" href="https://www.nature.com/articles/d41586-021-01170-0">"Cooperative AI: machines must learn to find common ground"</a>. <i>Nature</i>. <b>593</b> (7857): 33–36. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2021Natur.593...33D">2021Natur.593...33D</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-021-01170-0">10.1038/d41586-021-01170-0</a>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/33947992">33947992</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:233740521">233740521</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221122230552/https://www.nature.com/articles/d41586-021-01170-0">Archived</a> from the original on 2022-11-22<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Cooperative+AI%3A+machines+must+learn+to+find+common+ground&amp;rft.volume=593&amp;rft.issue=7857&amp;rft.pages=33-36&amp;rft.date=2021&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-021-01170-0&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A233740521%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F33947992&amp;rft_id=info%3Abibcode%2F2021Natur.593...33D&amp;rft.aulast=Dafoe&amp;rft.aufirst=Allan&amp;rft.au=Bachrach%2C+Yoram&amp;rft.au=Hadfield%2C+Gillian&amp;rft.au=Horvitz%2C+Eric&amp;rft.au=Larson%2C+Kate&amp;rft.au=Graepel%2C+Thore&amp;rft_id=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-021-01170-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-130"><span class="mw-cite-backlink"><b><a href="#cite_ref-130">^</a></b></span> <span class="reference-text">Bender, E.M., Gebru, T., McMillan-Major, A., &amp; Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610-623. <a rel="nofollow" class="external free" href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.</span>
</li>
<li id="cite_note-131"><span class="mw-cite-backlink"><b><a href="#cite_ref-131">^</a></b></span> <span class="reference-text">Strubell, E., Ganesh, A., &amp; McCallum, A. (2019). Energy and Policy Considerations for Deep Learning in NLP. arXiv preprint arXiv:1906.02243.</span>
</li>
<li id="cite_note-132"><span class="mw-cite-backlink"><b><a href="#cite_ref-132">^</a></b></span> <span class="reference-text">Schwartz, R., Dodge, J., Smith, N.A., &amp; Etzioni, O. (2020). Green AI. Communications of the ACM, 63(12), 54-63. <a rel="nofollow" class="external free" href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.</span>
</li>
<li id="cite_note-133"><span class="mw-cite-backlink"><b><a href="#cite_ref-133">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSatarianoSpecia2023" class="citation news cs1">Satariano, Adam; Specia, Megan (2023-11-01). <a rel="nofollow" class="external text" href="https://www.nytimes.com/2023/11/01/world/europe/uk-ai-summit-sunak.html">"Global Leaders Warn A.I. Could Cause 'Catastrophic' Harm"</a>. <i>The New York Times</i>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0362-4331">0362-4331</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2024-04-20</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Global+Leaders+Warn+A.I.+Could+Cause+%27Catastrophic%27+Harm&amp;rft.date=2023-11-01&amp;rft.issn=0362-4331&amp;rft.aulast=Satariano&amp;rft.aufirst=Adam&amp;rft.au=Specia%2C+Megan&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2023%2F11%2F01%2Fworld%2Feurope%2Fuk-ai-summit-sunak.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-134"><span class="mw-cite-backlink"><b><a href="#cite_ref-134">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCrafts2021" class="citation journal cs1">Crafts, Nicholas (2021-09-23). <a rel="nofollow" class="external text" href="https://academic.oup.com/oxrep/article/37/3/521/6374675">"Artificial intelligence as a general-purpose technology: an historical perspective"</a>. <i>Oxford Review of Economic Policy</i>. <b>37</b> (3): 521–536. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1093%2Foxrep%2Fgrab012">10.1093/oxrep/grab012</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/0266-903X">0266-903X</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124130718/https://academic.oup.com/oxrep/article/37/3/521/6374675">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Oxford+Review+of+Economic+Policy&amp;rft.atitle=Artificial+intelligence+as+a+general-purpose+technology%3A+an+historical+perspective&amp;rft.volume=37&amp;rft.issue=3&amp;rft.pages=521-536&amp;rft.date=2021-09-23&amp;rft_id=info%3Adoi%2F10.1093%2Foxrep%2Fgrab012&amp;rft.issn=0266-903X&amp;rft.aulast=Crafts&amp;rft.aufirst=Nicholas&amp;rft_id=https%3A%2F%2Facademic.oup.com%2Foxrep%2Farticle%2F37%2F3%2F521%2F6374675&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-135"><span class="mw-cite-backlink"><b><a href="#cite_ref-135">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREF葉俶禎黃子君張媁雯賴志樫2020" class="citation journal cs1">葉俶禎; 黃子君; 張媁雯; 賴志樫 (2020-12-01). "Labor Displacement in Artificial Intelligence Era: A Systematic Literature Review". <i>臺灣東亞文明研究學刊</i>. <b>17</b> (2). <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.6163%2FTJEAS.202012_17%282%29.0002">10.6163/TJEAS.202012_17(2).0002</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1812-6243">1812-6243</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=%E8%87%BA%E7%81%A3%E6%9D%B1%E4%BA%9E%E6%96%87%E6%98%8E%E7%A0%94%E7%A9%B6%E5%AD%B8%E5%88%8A&amp;rft.atitle=Labor+Displacement+in+Artificial+Intelligence+Era%3A+A+Systematic+Literature+Review&amp;rft.volume=17&amp;rft.issue=2&amp;rft.date=2020-12-01&amp;rft_id=info%3Adoi%2F10.6163%2FTJEAS.202012_17%282%29.0002&amp;rft.issn=1812-6243&amp;rft.au=%E8%91%89%E4%BF%B6%E7%A6%8E&amp;rft.au=%E9%BB%83%E5%AD%90%E5%90%9B&amp;rft.au=%E5%BC%B5%E5%AA%81%E9%9B%AF&amp;rft.au=%E8%B3%B4%E5%BF%97%E6%A8%AB&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-136"><span class="mw-cite-backlink"><b><a href="#cite_ref-136">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFJohnson2019" class="citation journal cs1">Johnson, James (2019-04-03). <a rel="nofollow" class="external text" href="https://www.tandfonline.com/doi/full/10.1080/14751798.2019.1600800">"Artificial intelligence &amp; future warfare: implications for international security"</a>. <i>Defense &amp; Security Analysis</i>. <b>35</b> (2): 147–169. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F14751798.2019.1600800">10.1080/14751798.2019.1600800</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1475-1798">1475-1798</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:159321626">159321626</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124125204/https://www.tandfonline.com/doi/full/10.1080/14751798.2019.1600800">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Defense+%26+Security+Analysis&amp;rft.atitle=Artificial+intelligence+%26+future+warfare%3A+implications+for+international+security&amp;rft.volume=35&amp;rft.issue=2&amp;rft.pages=147-169&amp;rft.date=2019-04-03&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A159321626%23id-name%3DS2CID&amp;rft.issn=1475-1798&amp;rft_id=info%3Adoi%2F10.1080%2F14751798.2019.1600800&amp;rft.aulast=Johnson&amp;rft.aufirst=James&amp;rft_id=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F14751798.2019.1600800&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-137"><span class="mw-cite-backlink"><b><a href="#cite_ref-137">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFKertysova2018" class="citation journal cs1">Kertysova, Katarina (2018-12-12). <a rel="nofollow" class="external text" href="https://brill.com/view/journals/shrs/29/1-4/article-p55_55.xml">"Artificial Intelligence and Disinformation: How AI Changes the Way Disinformation is Produced, Disseminated, and Can Be Countered"</a>. <i>Security and Human Rights</i>. <b>29</b> (1–4): 55–81. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1163%2F18750230-02901005">10.1163/18750230-02901005</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1874-7337">1874-7337</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:216896677">216896677</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124125204/https://brill.com/view/journals/shrs/29/1-4/article-p55_55.xml">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Security+and+Human+Rights&amp;rft.atitle=Artificial+Intelligence+and+Disinformation%3A+How+AI+Changes+the+Way+Disinformation+is+Produced%2C+Disseminated%2C+and+Can+Be+Countered&amp;rft.volume=29&amp;rft.issue=1%E2%80%934&amp;rft.pages=55-81&amp;rft.date=2018-12-12&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A216896677%23id-name%3DS2CID&amp;rft.issn=1874-7337&amp;rft_id=info%3Adoi%2F10.1163%2F18750230-02901005&amp;rft.aulast=Kertysova&amp;rft.aufirst=Katarina&amp;rft_id=https%3A%2F%2Fbrill.com%2Fview%2Fjournals%2Fshrs%2F29%2F1-4%2Farticle-p55_55.xml&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-138"><span class="mw-cite-backlink"><b><a href="#cite_ref-138">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFeldstein2019" class="citation conference cs1">Feldstein, Steven (2019). <i>The Global Expansion of AI Surveillance</i>. Carnegie Endowment for International Peace.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=The+Global+Expansion+of+AI+Surveillance&amp;rft.pub=Carnegie+Endowment+for+International+Peace&amp;rft.date=2019&amp;rft.aulast=Feldstein&amp;rft.aufirst=Steven&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-139"><span class="mw-cite-backlink"><b><a href="#cite_ref-139">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAgrawalGansGoldfarb2019" class="citation book cs1">Agrawal, Ajay; Gans, Joshua; Goldfarb, Avi (2019). <a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1099435014"><i>The economics of artificial intelligence: an agenda</i></a>. Chicago, Illinois. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-226-61347-5" title="Special:BookSources/978-0-226-61347-5"><bdi>978-0-226-61347-5</bdi></a>. <a href="/wiki/OCLC_(identifier)" class="mw-redirect" title="OCLC (identifier)">OCLC</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/1099435014">1099435014</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230315184354/https://www.worldcat.org/title/1099435014">Archived</a> from the original on 2023-03-15<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-28</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+economics+of+artificial+intelligence%3A+an+agenda&amp;rft.place=Chicago%2C+Illinois&amp;rft.date=2019&amp;rft_id=info%3Aoclcnum%2F1099435014&amp;rft.isbn=978-0-226-61347-5&amp;rft.aulast=Agrawal&amp;rft.aufirst=Ajay&amp;rft.au=Gans%2C+Joshua&amp;rft.au=Goldfarb%2C+Avi&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F1099435014&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span><span class="cs1-maint citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_book" title="Template:Cite book">cite book</a>}}</code>:  CS1 maint: location missing publisher (<a href="/wiki/Category:CS1_maint:_location_missing_publisher" title="Category:CS1 maint: location missing publisher">link</a>)</span></span>
</li>
<li id="cite_note-140"><span class="mw-cite-backlink"><b><a href="#cite_ref-140">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWhittlestoneClark2021" class="citation journal cs1">Whittlestone, Jess; Clark, Jack (2021-08-31). "Why and How Governments Should Monitor AI Development". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2108.12427">2108.12427</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Why+and+How+Governments+Should+Monitor+AI+Development&amp;rft.date=2021-08-31&amp;rft_id=info%3Aarxiv%2F2108.12427&amp;rft.aulast=Whittlestone&amp;rft.aufirst=Jess&amp;rft.au=Clark%2C+Jack&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-:20-141"><span class="mw-cite-backlink">^ <a href="#cite_ref-:20_141-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:20_141-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFShevlane2022" class="citation web cs1">Shevlane, Toby (2022). <a rel="nofollow" class="external text" href="https://www.governance.ai/post/sharing-powerful-ai-models">"Sharing Powerful AI Models | GovAI Blog"</a>. <i>Center for the Governance of AI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124125202/https://www.governance.ai/post/sharing-powerful-ai-models">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Center+for+the+Governance+of+AI&amp;rft.atitle=Sharing+Powerful+AI+Models+%7C+GovAI+Blog&amp;rft.date=2022&amp;rft.aulast=Shevlane&amp;rft.aufirst=Toby&amp;rft_id=https%3A%2F%2Fwww.governance.ai%2Fpost%2Fsharing-powerful-ai-models&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-142"><span class="mw-cite-backlink"><b><a href="#cite_ref-142">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFAskellBrundageHadfield2019" class="citation journal cs1">Askell, Amanda; Brundage, Miles; Hadfield, Gillian (2019-07-10). "The Role of Cooperation in Responsible AI Development". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1907.04534">1907.04534</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+Role+of+Cooperation+in+Responsible+AI+Development&amp;rft.date=2019-07-10&amp;rft_id=info%3Aarxiv%2F1907.04534&amp;rft.aulast=Askell&amp;rft.aufirst=Amanda&amp;rft.au=Brundage%2C+Miles&amp;rft.au=Hadfield%2C+Gillian&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-143"><span class="mw-cite-backlink"><b><a href="#cite_ref-143">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFGursoyKakadiaris2022" class="citation cs2">Gursoy, Furkan; Kakadiaris, Ioannis A. (2022-08-31), <i>System Cards for AI-Based Decision-Making for Public Policy</i>, <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2203.04754">2203.04754</a></span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=System+Cards+for+AI-Based+Decision-Making+for+Public+Policy&amp;rft.date=2022-08-31&amp;rft_id=info%3Aarxiv%2F2203.04754&amp;rft.aulast=Gursoy&amp;rft.aufirst=Furkan&amp;rft.au=Kakadiaris%2C+Ioannis+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-144"><span class="mw-cite-backlink"><b><a href="#cite_ref-144">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFCobbeLeeSingh2021" class="citation book cs1">Cobbe, Jennifer; Lee, Michelle Seng Ah; Singh, Jatinder (2021-03-01). "Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems". <i>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</i>. FAccT '21. New York, NY, USA: Association for Computing Machinery. pp.&#160;598–609. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3442188.3445921">10.1145/3442188.3445921</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-8309-7" title="Special:BookSources/978-1-4503-8309-7"><bdi>978-1-4503-8309-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Reviewable+Automated+Decision-Making%3A+A+Framework+for+Accountable+Algorithmic+Systems&amp;rft.btitle=Proceedings+of+the+2021+ACM+Conference+on+Fairness%2C+Accountability%2C+and+Transparency&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.series=FAccT+%2721&amp;rft.pages=598-609&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2021-03-01&amp;rft_id=info%3Adoi%2F10.1145%2F3442188.3445921&amp;rft.isbn=978-1-4503-8309-7&amp;rft.aulast=Cobbe&amp;rft.aufirst=Jennifer&amp;rft.au=Lee%2C+Michelle+Seng+Ah&amp;rft.au=Singh%2C+Jatinder&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-145"><span class="mw-cite-backlink"><b><a href="#cite_ref-145">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRajiSmartWhiteMitchell2020" class="citation book cs1">Raji, Inioluwa Deborah; Smart, Andrew; White, Rebecca N.; Mitchell, Margaret; Gebru, Timnit; Hutchinson, Ben; Smith-Loud, Jamila; Theron, Daniel; Barnes, Parker (2020-01-27). "Closing the AI accountability gap: Defining an end-to-end framework for internal algorithmic auditing". <i>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</i>. FAT* '20. New York, NY, USA: Association for Computing Machinery. pp.&#160;33–44. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1145%2F3351095.3372873">10.1145/3351095.3372873</a></span>. <a href="/wiki/ISBN_(identifier)" class="mw-redirect" title="ISBN (identifier)">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4503-6936-7" title="Special:BookSources/978-1-4503-6936-7"><bdi>978-1-4503-6936-7</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Closing+the+AI+accountability+gap%3A+Defining+an+end-to-end+framework+for+internal+algorithmic+auditing&amp;rft.btitle=Proceedings+of+the+2020+Conference+on+Fairness%2C+Accountability%2C+and+Transparency&amp;rft.place=New+York%2C+NY%2C+USA&amp;rft.series=FAT%2A+%2720&amp;rft.pages=33-44&amp;rft.pub=Association+for+Computing+Machinery&amp;rft.date=2020-01-27&amp;rft_id=info%3Adoi%2F10.1145%2F3351095.3372873&amp;rft.isbn=978-1-4503-6936-7&amp;rft.aulast=Raji&amp;rft.aufirst=Inioluwa+Deborah&amp;rft.au=Smart%2C+Andrew&amp;rft.au=White%2C+Rebecca+N.&amp;rft.au=Mitchell%2C+Margaret&amp;rft.au=Gebru%2C+Timnit&amp;rft.au=Hutchinson%2C+Ben&amp;rft.au=Smith-Loud%2C+Jamila&amp;rft.au=Theron%2C+Daniel&amp;rft.au=Barnes%2C+Parker&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-146"><span class="mw-cite-backlink"><b><a href="#cite_ref-146">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFTurchinDenchGreen2019" class="citation journal cs1">Turchin, Alexey; Dench, David; Green, Brian Patrick (2019). <a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Fbdcc3010016">"Global Solutions vs. Local Solutions for the AI Safety Problem"</a>. <i>Big Data and Cognitive Computing</i>. <b>3</b> (16): 1–25. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.3390%2Fbdcc3010016">10.3390/bdcc3010016</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Big+Data+and+Cognitive+Computing&amp;rft.atitle=Global+Solutions+vs.+Local+Solutions+for+the+AI+Safety+Problem&amp;rft.volume=3&amp;rft.issue=16&amp;rft.pages=1-25&amp;rft.date=2019&amp;rft_id=info%3Adoi%2F10.3390%2Fbdcc3010016&amp;rft.aulast=Turchin&amp;rft.aufirst=Alexey&amp;rft.au=Dench%2C+David&amp;rft.au=Green%2C+Brian+Patrick&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.3390%252Fbdcc3010016&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-147"><span class="mw-cite-backlink"><b><a href="#cite_ref-147">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZiegler2022" class="citation news cs1">Ziegler, Bart (8 April 2022). "Is It Time to Regulate AI?". <i>Wall Street Journal</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wall+Street+Journal&amp;rft.atitle=Is+It+Time+to+Regulate+AI%3F&amp;rft.date=2022-04-08&amp;rft.aulast=Ziegler&amp;rft.aufirst=Bart&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-148"><span class="mw-cite-backlink"><b><a href="#cite_ref-148">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSmith2022" class="citation news cs1">Smith, John (15 May 2022). "Global Governance of Artificial Intelligence: Opportunities and Challenges". <i>The Guardian</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Global+Governance+of+Artificial+Intelligence%3A+Opportunities+and+Challenges&amp;rft.date=2022-05-15&amp;rft.aulast=Smith&amp;rft.aufirst=John&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-149"><span class="mw-cite-backlink"><b><a href="#cite_ref-149">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFZiegler2022" class="citation news cs1">Ziegler, Bart (8 April 2022). <a rel="nofollow" class="external text" href="https://www.wsj.com/articles/is-it-time-to-regulate-ai-11649433600">"Is It Time to Regulate AI?"</a>. <i>Wall Street Journal</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124125645/https://www.wsj.com/articles/is-it-time-to-regulate-ai-11649433600">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Wall+Street+Journal&amp;rft.atitle=Is+It+Time+to+Regulate+AI%3F&amp;rft.date=2022-04-08&amp;rft.aulast=Ziegler&amp;rft.aufirst=Bart&amp;rft_id=https%3A%2F%2Fwww.wsj.com%2Farticles%2Fis-it-time-to-regulate-ai-11649433600&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-150"><span class="mw-cite-backlink"><b><a href="#cite_ref-150">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFReed2018" class="citation journal cs1">Reed, Chris (2018-09-13). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6107539">"How should we regulate artificial intelligence?"</a>. <i>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</i>. <b>376</b> (2128): 20170360. <a href="/wiki/Bibcode_(identifier)" class="mw-redirect" title="Bibcode (identifier)">Bibcode</a>:<a rel="nofollow" class="external text" href="https://ui.adsabs.harvard.edu/abs/2018RSPTA.37670360R">2018RSPTA.37670360R</a>. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1098%2Frsta.2017.0360">10.1098/rsta.2017.0360</a>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/1364-503X">1364-503X</a>. <a href="/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6107539">6107539</a></span>. <a href="/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/30082306">30082306</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Transactions+of+the+Royal+Society+A%3A+Mathematical%2C+Physical+and+Engineering+Sciences&amp;rft.atitle=How+should+we+regulate+artificial+intelligence%3F&amp;rft.volume=376&amp;rft.issue=2128&amp;rft.pages=20170360&amp;rft.date=2018-09-13&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6107539%23id-name%3DPMC&amp;rft_id=info%3Abibcode%2F2018RSPTA.37670360R&amp;rft_id=info%3Apmid%2F30082306&amp;rft_id=info%3Adoi%2F10.1098%2Frsta.2017.0360&amp;rft.issn=1364-503X&amp;rft.aulast=Reed&amp;rft.aufirst=Chris&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC6107539&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-151"><span class="mw-cite-backlink"><b><a href="#cite_ref-151">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBelton2019" class="citation web cs1">Belton, Keith B. (2019-03-07). <a rel="nofollow" class="external text" href="https://www.industryweek.com/technology-and-iiot/article/22027274/how-should-ai-be-regulated">"How Should AI Be Regulated?"</a>. <i>IndustryWeek</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20220129114109/https://www.industryweek.com/technology-and-iiot/article/22027274/how-should-ai-be-regulated">Archived</a> from the original on 2022-01-29<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=IndustryWeek&amp;rft.atitle=How+Should+AI+Be+Regulated%3F&amp;rft.date=2019-03-07&amp;rft.aulast=Belton&amp;rft.aufirst=Keith+B.&amp;rft_id=https%3A%2F%2Fwww.industryweek.com%2Ftechnology-and-iiot%2Farticle%2F22027274%2Fhow-should-ai-be-regulated&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-152"><span class="mw-cite-backlink"><b><a href="#cite_ref-152">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNational_Security_Commission_on_Artificial_Intelligence2021" class="citation cs2">National Security Commission on Artificial Intelligence (2021), <i>Final Report</i></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Final+Report&amp;rft.date=2021&amp;rft.au=National+Security+Commission+on+Artificial+Intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-153"><span class="mw-cite-backlink"><b><a href="#cite_ref-153">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNational_Institute_of_Standards_and_Technology2021" class="citation journal cs1">National Institute of Standards and Technology (2021-07-12). <a rel="nofollow" class="external text" href="https://www.nist.gov/itl/ai-risk-management-framework">"AI Risk Management Framework"</a>. <i>NIST</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124130402/https://www.nist.gov/itl/ai-risk-management-framework">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NIST&amp;rft.atitle=AI+Risk+Management+Framework&amp;rft.date=2021-07-12&amp;rft.au=National+Institute+of+Standards+and+Technology&amp;rft_id=https%3A%2F%2Fwww.nist.gov%2Fitl%2Fai-risk-management-framework&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-154"><span class="mw-cite-backlink"><b><a href="#cite_ref-154">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFRichardson2021" class="citation web cs1">Richardson, Tim (2021). <a rel="nofollow" class="external text" href="https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/">"Britain publishes 10-year National Artificial Intelligence Strategy"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114137/https://www.theregister.com/2021/09/22/uk_10_year_national_ai_strategy/">Archived</a> from the original on 2023-02-10<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Britain+publishes+10-year+National+Artificial+Intelligence+Strategy&amp;rft.date=2021&amp;rft.aulast=Richardson&amp;rft.aufirst=Tim&amp;rft_id=https%3A%2F%2Fwww.theregister.com%2F2021%2F09%2F22%2Fuk_10_year_national_ai_strategy%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:18-155"><span class="mw-cite-backlink">^ <a href="#cite_ref-:18_155-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:18_155-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version">"Guidance: National AI Strategy"</a>. <i>GOV.UK</i>. 2021. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230210114139/https://www.gov.uk/government/publications/national-ai-strategy/national-ai-strategy-html-version">Archived</a> from the original on 2023-02-10<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GOV.UK&amp;rft.atitle=Guidance%3A+National+AI+Strategy&amp;rft.date=2021&amp;rft_id=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fpublications%2Fnational-ai-strategy%2Fnational-ai-strategy-html-version&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-156"><span class="mw-cite-backlink"><b><a href="#cite_ref-156">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFHardcastle2023" class="citation web cs1">Hardcastle, Kimberley (2023-08-23). <a rel="nofollow" class="external text" href="https://theconversation.com/were-talking-about-ai-a-lot-right-now-and-its-not-a-moment-too-soon-211448">"We're talking about AI a lot right now – and it's not a moment too soon"</a>. <i>The Conversation</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-10-31</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Conversation&amp;rft.atitle=We%27re+talking+about+AI+a+lot+right+now+%E2%80%93+and+it%27s+not+a+moment+too+soon&amp;rft.date=2023-08-23&amp;rft.aulast=Hardcastle&amp;rft.aufirst=Kimberley&amp;rft_id=http%3A%2F%2Ftheconversation.com%2Fwere-talking-about-ai-a-lot-right-now-and-its-not-a-moment-too-soon-211448&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-157"><span class="mw-cite-backlink"><b><a href="#cite_ref-157">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://www.gov.uk/government/news/iconic-bletchley-park-to-host-uk-ai-safety-summit-in-early-november">"Iconic Bletchley Park to host UK AI Safety Summit in early November"</a>. <i>GOV.UK</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2023-10-31</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=GOV.UK&amp;rft.atitle=Iconic+Bletchley+Park+to+host+UK+AI+Safety+Summit+in+early+November&amp;rft_id=https%3A%2F%2Fwww.gov.uk%2Fgovernment%2Fnews%2Ficonic-bletchley-park-to-host-uk-ai-safety-summit-in-early-november&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-158"><span class="mw-cite-backlink"><b><a href="#cite_ref-158">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOffice_of_the_Director_of_National_Intelligence,_Intelligence_Advanced_Research_Projects_Activity" class="citation web cs1">Office of the Director of National Intelligence, Intelligence Advanced Research Projects Activity. <a rel="nofollow" class="external text" href="https://www.iarpa.gov/research-programs/trojai">"IARPA – TrojAI"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124131956/https://www.iarpa.gov/research-programs/trojai">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=IARPA+%E2%80%93+TrojAI&amp;rft.au=Office+of+the+Director+of+National+Intelligence%2C+Intelligence+Advanced+Research+Projects+Activity&amp;rft_id=https%3A%2F%2Fwww.iarpa.gov%2Fresearch-programs%2Ftrojai&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-159"><span class="mw-cite-backlink"><b><a href="#cite_ref-159">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFTurek" class="citation web cs1">Turek, Matt. <a rel="nofollow" class="external text" href="https://www.darpa.mil/program/explainable-artificial-intelligence">"Explainable Artificial Intelligence"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20210219210013/https://www.darpa.mil/program/explainable-artificial-intelligence">Archived</a> from the original on 2021-02-19<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Explainable+Artificial+Intelligence&amp;rft.aulast=Turek&amp;rft.aufirst=Matt&amp;rft_id=https%3A%2F%2Fwww.darpa.mil%2Fprogram%2Fexplainable-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-160"><span class="mw-cite-backlink"><b><a href="#cite_ref-160">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFDraper" class="citation web cs1">Draper, Bruce. <a rel="nofollow" class="external text" href="https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception">"Guaranteeing AI Robustness Against Deception"</a>. <i>Defense Advanced Research Projects Agency</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230109021433/https://www.darpa.mil/program/guaranteeing-ai-robustness-against-deception">Archived</a> from the original on 2023-01-09<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Defense+Advanced+Research+Projects+Agency&amp;rft.atitle=Guaranteeing+AI+Robustness+Against+Deception&amp;rft.aulast=Draper&amp;rft.aufirst=Bruce&amp;rft_id=https%3A%2F%2Fwww.darpa.mil%2Fprogram%2Fguaranteeing-ai-robustness-against-deception&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-161"><span class="mw-cite-backlink"><b><a href="#cite_ref-161">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFNational_Science_Foundation2023" class="citation web cs1">National Science Foundation (23 February 2023). <a rel="nofollow" class="external text" href="https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems">"Safe Learning-Enabled Systems"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230226190627/https://beta.nsf.gov/funding/opportunities/safe-learning-enabled-systems">Archived</a> from the original on 2023-02-26<span class="reference-accessdate">. Retrieved <span class="nowrap">2023-02-27</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Safe+Learning-Enabled+Systems&amp;rft.date=2023-02-23&amp;rft.au=National+Science+Foundation&amp;rft_id=https%3A%2F%2Fbeta.nsf.gov%2Ffunding%2Fopportunities%2Fsafe-learning-enabled-systems&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-162"><span class="mw-cite-backlink"><b><a href="#cite_ref-162">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation news cs1"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20240420010734/https://news.un.org/en/story/2024/03/1147831">"General Assembly adopts landmark resolution on artificial intelligence"</a>. <i>UN News</i>. 21 March 2024. Archived from <a rel="nofollow" class="external text" href="https://news.un.org/en/story/2024/03/1147831">the original</a> on 20 April 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">21 April</span> 2024</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=UN+News&amp;rft.atitle=General+Assembly+adopts+landmark+resolution+on+artificial+intelligence&amp;rft.date=2024-03-21&amp;rft_id=https%3A%2F%2Fnews.un.org%2Fen%2Fstory%2F2024%2F03%2F1147831&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-163"><span class="mw-cite-backlink"><b><a href="#cite_ref-163">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFSay2024" class="citation news cs1">Say, Mark (23 May 2024). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20240524232313/https://www.ukauthority.com/articles/dsit-announces-funding-for-research-on-ai-safety/">"DSIT announces funding for research on AI safety"</a>. Archived from <a rel="nofollow" class="external text" href="https://www.ukauthority.com/articles/dsit-announces-funding-for-research-on-ai-safety/">the original</a> on 24 May 2024<span class="reference-accessdate">. Retrieved <span class="nowrap">11 June</span> 2024</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=DSIT+announces+funding+for+research+on+AI+safety&amp;rft.date=2024-05-23&amp;rft.aulast=Say&amp;rft.aufirst=Mark&amp;rft_id=https%3A%2F%2Fwww.ukauthority.com%2Farticles%2Fdsit-announces-funding-for-research-on-ai-safety%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-164"><span class="mw-cite-backlink"><b><a href="#cite_ref-164">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFMäntymäkiMinkkinenBirkstedtViljanen2022" class="citation journal cs1">Mäntymäki, Matti; Minkkinen, Matti; Birkstedt, Teemu; Viljanen, Mika (2022). <a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs43681-022-00143-x">"Defining organizational AI governance"</a>. <i>AI and Ethics</i>. <b>2</b> (4): 603–609. <a href="/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs43681-022-00143-x">10.1007/s43681-022-00143-x</a></span>. <a href="/wiki/ISSN_(identifier)" class="mw-redirect" title="ISSN (identifier)">ISSN</a>&#160;<a rel="nofollow" class="external text" href="https://www.worldcat.org/issn/2730-5953">2730-5953</a>. <a href="/wiki/S2CID_(identifier)" class="mw-redirect" title="S2CID (identifier)">S2CID</a>&#160;<a rel="nofollow" class="external text" href="https://api.semanticscholar.org/CorpusID:247119668">247119668</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=AI+and+Ethics&amp;rft.atitle=Defining+organizational+AI+governance&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=603-609&amp;rft.date=2022&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A247119668%23id-name%3DS2CID&amp;rft.issn=2730-5953&amp;rft_id=info%3Adoi%2F10.1007%2Fs43681-022-00143-x&amp;rft.aulast=M%C3%A4ntym%C3%A4ki&amp;rft.aufirst=Matti&amp;rft.au=Minkkinen%2C+Matti&amp;rft.au=Birkstedt%2C+Teemu&amp;rft.au=Viljanen%2C+Mika&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1007%252Fs43681-022-00143-x&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-:19-165"><span class="mw-cite-backlink">^ <a href="#cite_ref-:19_165-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:19_165-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-:19_165-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFBrundageAvinWangBelfield2020" class="citation journal cs1">Brundage, Miles; Avin, Shahar; Wang, Jasmine; Belfield, Haydn; Krueger, Gretchen; Hadfield, Gillian; Khlaaf, Heidy; Yang, Jingying; Toner, Helen; Fong, Ruth; Maharaj, Tegan; Koh, Pang Wei; Hooker, Sara; Leung, Jade; Trask, Andrew (2020-04-20). "Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims". <a href="/wiki/ArXiv_(identifier)" class="mw-redirect" title="ArXiv (identifier)">arXiv</a>:<span class="id-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/2004.07213">2004.07213</a></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Toward+Trustworthy+AI+Development%3A+Mechanisms+for+Supporting+Verifiable+Claims&amp;rft.date=2020-04-20&amp;rft_id=info%3Aarxiv%2F2004.07213&amp;rft.aulast=Brundage&amp;rft.aufirst=Miles&amp;rft.au=Avin%2C+Shahar&amp;rft.au=Wang%2C+Jasmine&amp;rft.au=Belfield%2C+Haydn&amp;rft.au=Krueger%2C+Gretchen&amp;rft.au=Hadfield%2C+Gillian&amp;rft.au=Khlaaf%2C+Heidy&amp;rft.au=Yang%2C+Jingying&amp;rft.au=Toner%2C+Helen&amp;rft.au=Fong%2C+Ruth&amp;rft.au=Maharaj%2C+Tegan&amp;rft.au=Koh%2C+Pang+Wei&amp;rft.au=Hooker%2C+Sara&amp;rft.au=Leung%2C+Jade&amp;rft.au=Trask%2C+Andrew&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span> <span class="cs1-visible-error citation-comment"><code class="cs1-code">{{<a href="/wiki/Template:Cite_journal" title="Template:Cite journal">cite journal</a>}}</code>: </span><span class="cs1-visible-error citation-comment">Cite journal requires <code class="cs1-code">&#124;journal=</code> (<a href="/wiki/Help:CS1_errors#missing_periodical" title="Help:CS1 errors">help</a>)</span></span>
</li>
<li id="cite_note-166"><span class="mw-cite-backlink"><b><a href="#cite_ref-166">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite class="citation web cs1"><a rel="nofollow" class="external text" href="https://incidentdatabase.ai/">"Welcome to the Artificial Intelligence Incident Database"</a>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124132715/https://incidentdatabase.ai/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Welcome+to+the+Artificial+Intelligence+Incident+Database&amp;rft_id=https%3A%2F%2Fincidentdatabase.ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-167"><span class="mw-cite-backlink"><b><a href="#cite_ref-167">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFWiblinHarris2022" class="citation web cs1">Wiblin, Robert; Harris, Keiran (2022). <a rel="nofollow" class="external text" href="https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/">"Nova DasSarma on why information security may be critical to the safe development of AI systems"</a>. <i>80,000 Hours</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20221124132927/https://80000hours.org/podcast/episodes/nova-dassarma-information-security-and-ai-systems/">Archived</a> from the original on 2022-11-24<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=80%2C000+Hours&amp;rft.atitle=Nova+DasSarma+on+why+information+security+may+be+critical+to+the+safe+development+of+AI+systems&amp;rft.date=2022&amp;rft.aulast=Wiblin&amp;rft.aufirst=Robert&amp;rft.au=Harris%2C+Keiran&amp;rft_id=https%3A%2F%2F80000hours.org%2Fpodcast%2Fepisodes%2Fnova-dassarma-information-security-and-ai-systems%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-168"><span class="mw-cite-backlink"><b><a href="#cite_ref-168">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOpenAI2022" class="citation web cs1">OpenAI (2022-06-02). <a rel="nofollow" class="external text" href="https://openai.com/blog/best-practices-for-deploying-language-models/">"Best Practices for Deploying Language Models"</a>. <i>OpenAI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230315184334/https://openai.com/blog/best-practices-for-deploying-language-models/">Archived</a> from the original on 2023-03-15<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=Best+Practices+for+Deploying+Language+Models&amp;rft.date=2022-06-02&amp;rft.au=OpenAI&amp;rft_id=https%3A%2F%2Fopenai.com%2Fblog%2Fbest-practices-for-deploying-language-models%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-169"><span class="mw-cite-backlink"><b><a href="#cite_ref-169">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFOpenAI" class="citation web cs1">OpenAI. <a rel="nofollow" class="external text" href="https://openai.com/charter/">"OpenAI Charter"</a>. <i>OpenAI</i>. <a rel="nofollow" class="external text" href="https://web.archive.org/web/20210304235618/https://openai.com/charter/">Archived</a> from the original on 2021-03-04<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=OpenAI&amp;rft.atitle=OpenAI+Charter&amp;rft.au=OpenAI&amp;rft_id=https%3A%2F%2Fopenai.com%2Fcharter%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
<li id="cite_note-170"><span class="mw-cite-backlink"><b><a href="#cite_ref-170">^</a></b></span> <span class="reference-text"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1215172403"><cite id="CITEREFFuture_of_Life_Institute2016" class="citation web cs1">Future of Life Institute (2016). <a rel="nofollow" class="external text" href="https://web.archive.org/web/20230922183710/https://invisiosolutions.com/navigating-the-london-web-space-ai-website-builders-vs-local-web-development-companies/">"Autonomous Weapons Open Letter: AI &amp; Robotics Researchers"</a>. <i>Future of Life Institute</i>. Archived from <a rel="nofollow" class="external text" href="https://invisiosolutions.com/navigating-the-london-web-space-ai-website-builders-vs-local-web-development-companies/">the original</a> on 2023-09-22<span class="reference-accessdate">. Retrieved <span class="nowrap">2022-11-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Future+of+Life+Institute&amp;rft.atitle=Autonomous+Weapons+Open+Letter%3A+AI+%26+Robotics+Researchers&amp;rft.date=2016&amp;rft.au=Future+of+Life+Institute&amp;rft_id=https%3A%2F%2Finvisiosolutions.com%2Fnavigating-the-london-web-space-ai-website-builders-vs-local-web-development-companies%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAI+safety" class="Z3988"></span></span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=AI_safety&amp;action=edit&amp;section=25" title="Edit section: External links"><span>edit</span></a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://arxiv.org/abs/2109.13916" class="extiw" title="arxiv:2109.13916">Unsolved Problems in ML Safety</a></li>
<li><a href="https://arxiv.org/abs/2108.07258" class="extiw" title="arxiv:2108.07258">On the Opportunities and Risks of Foundation Models</a></li>
<li><a href="https://arxiv.org/abs/2306.12001" class="extiw" title="arxiv:2306.12001">An Overview of Catastrophic AI Risks</a></li>
<li><a rel="nofollow" class="external text" href="https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/">AI Accidents: An Emerging Threat</a></li>
<li><a rel="nofollow" class="external text" href="https://mitpress.mit.edu/9780262533690/engineering-a-safer-world/">Engineering a Safer World</a></li></ul>
<div class="navbox-styles"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><style data-mw-deduplicate="TemplateStyles:r1228936124">.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}</style></div><div role="navigation" class="navbox" aria-labelledby="Existential_risk_from_artificial_intelligence" style="padding:3px"><table class="nowraplinks mw-collapsible expanded navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1129693374"><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r1063604349"><div class="navbar plainlinks hlist navbar-mini"><ul><li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a href="/wiki/Special:EditPage/Template:Existential_risk_from_artificial_intelligence" title="Special:EditPage/Template:Existential risk from artificial intelligence"><abbr title="Edit this template">e</abbr></a></li></ul></div><div id="Existential_risk_from_artificial_intelligence" style="font-size:114%;margin:0 4em"><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk</a> from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div></th></tr><tr><th scope="row" class="navbox-group" style="width:1%">Concepts</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">AGI</a></li>
<li><a href="/wiki/AI_alignment" title="AI alignment">AI alignment</a></li>
<li><a href="/wiki/AI_capability_control" title="AI capability control">AI capability control</a></li>
<li><a class="mw-selflink selflink">AI safety</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/Consequentialism" title="Consequentialism">Consequentialism</a></li>
<li><a href="/wiki/Effective_accelerationism" title="Effective accelerationism">Effective accelerationism</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" class="mw-redirect" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Longtermism" title="Longtermism">Longtermism</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Suffering_risks" title="Suffering risks">Suffering risks</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Organizations</th><td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Alignment_Research_Center" title="Alignment Research Center">Alignment Research Center</a></li>
<li><a href="/wiki/Center_for_AI_Safety" title="Center for AI Safety">Center for AI Safety</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Center_for_Human-Compatible_Artificial_Intelligence" title="Center for Human-Compatible Artificial Intelligence">Center for Human-Compatible Artificial Intelligence</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/EleutherAI" title="EleutherAI">EleutherAI</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Google_DeepMind" title="Google DeepMind">Google DeepMind</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">People</th><td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Slate_Star_Codex" title="Slate Star Codex">Scott Alexander</a></li>
<li><a href="/wiki/Sam_Altman" title="Sam Altman">Sam Altman</a></li>
<li><a href="/wiki/Yoshua_Bengio" title="Yoshua Bengio">Yoshua Bengio</a></li>
<li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/Paul_Christiano_(researcher)" title="Paul Christiano (researcher)">Paul Christiano</a></li>
<li><a href="/wiki/K._Eric_Drexler" title="K. Eric Drexler">Eric Drexler</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Dan_Hendrycks" title="Dan Hendrycks">Dan Hendrycks</a></li>
<li><a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Shane_Legg" title="Shane Legg">Shane Legg</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%">Other</th><td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0"><div style="padding:0 0.25em">
<ul><li><a href="/wiki/Statement_on_AI_risk_of_extinction" title="Statement on AI risk of extinction">Statement on AI risk of extinction</a></li>
<li><i><a href="/wiki/Human_Compatible" title="Human Compatible">Human Compatible</a></i></li>
<li><a href="/wiki/Open_letter_on_artificial_intelligence_(2015)" title="Open letter on artificial intelligence (2015)">Open letter on artificial intelligence (2015)</a></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
<li><i><a href="/wiki/The_Precipice:_Existential_Risk_and_the_Future_of_Humanity" title="The Precipice: Existential Risk and the Future of Humanity">The Precipice</a></i></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li>
<li><i><a href="/wiki/Do_You_Trust_This_Computer%3F" title="Do You Trust This Computer?">Do You Trust This Computer?</a></i></li>
<li><a href="/wiki/Artificial_Intelligence_Act" title="Artificial Intelligence Act">Artificial Intelligence Act</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2"><div><span class="noviewer" typeof="mw:File"><span title="Category"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png" decoding="async" width="16" height="16" class="mw-file-element" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/23px-Symbol_category_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/31px-Symbol_category_class.svg.png 2x" data-file-width="180" data-file-height="185" /></span></span> <a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Category</a></div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw‐web.codfw.main‐6457fbf49b‐zbq5f
Cached time: 20240707004623
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 1.601 seconds
Real time usage: 1.796 seconds
Preprocessor visited node count: 10044/1000000
Post‐expand include size: 424512/2097152 bytes
Template argument size: 2204/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 7/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 697711/5000000 bytes
Lua time usage: 1.137/10.000 seconds
Lua memory usage: 7704941/52428800 bytes
Lua Profile:
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::callParserFunction      160 ms       14.3%
    ?                                                                120 ms       10.7%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::preprocess      120 ms       10.7%
    dataWrapper <mw.lua:672>                                         120 ms       10.7%
    <mw.lua:694>                                                      80 ms        7.1%
    is_set <Module:Citation/CS1/Utilities:23>                         60 ms        5.4%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::gsub       60 ms        5.4%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::getAllExpandedArguments       40 ms        3.6%
    MediaWiki\Extension\Scribunto\Engines\LuaSandbox\LuaSandboxCallback::formatDate       40 ms        3.6%
    pairs                                                             40 ms        3.6%
    [others]                                                         280 ms       25.0%
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00% 1619.379      1 -total
 68.41% 1107.848      1 Template:Reflist
 26.70%  432.399     74 Template:Cite_journal
 13.72%  222.225     49 Template:Cite_web
  7.18%  116.275      1 Template:Short_description
  5.87%   95.082      1 Template:Cite_magazine
  5.71%   92.524      1 Template:Excerpt
  5.18%   83.962      1 Template:Artificial_intelligence
  5.03%   81.523      1 Template:Sidebar_with_collapsible_lists
  3.88%   62.811      5 Template:Main_other
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:72360809-0!canonical and timestamp 20240707004623 and revision id 1233050013. Rendering was triggered because: page-view
 -->
</div><!--esi <esi:include src="/esitest-fa8a495983347898/content" /> --><noscript><img src="https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" width="1" height="1" style="border: none; position: absolute;"></noscript>
<div class="printfooter" data-nosnippet="">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=AI_safety&amp;oldid=1233050013">https://en.wikipedia.org/w/index.php?title=AI_safety&amp;oldid=1233050013</a>"</div></div>
					<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_intelligence" title="Category:Artificial intelligence">Artificial intelligence</a></li><li><a href="/wiki/Category:Existential_risk_from_artificial_general_intelligence" title="Category:Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li><li><a href="/wiki/Category:Cybernetics" title="Category:Cybernetics">Cybernetics</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_missing_periodical" title="Category:CS1 errors: missing periodical">CS1 errors: missing periodical</a></li><li><a href="/wiki/Category:CS1_maint:_DOI_inactive_as_of_January_2024" title="Category:CS1 maint: DOI inactive as of January 2024">CS1 maint: DOI inactive as of January 2024</a></li><li><a href="/wiki/Category:CS1_maint:_location_missing_publisher" title="Category:CS1 maint: location missing publisher">CS1 maint: location missing publisher</a></li><li><a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">Articles with short description</a></li><li><a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">Short description is different from Wikidata</a></li><li><a href="/wiki/Category:Articles_lacking_reliable_references_from_July_2023" title="Category:Articles lacking reliable references from July 2023">Articles lacking reliable references from July 2023</a></li><li><a href="/wiki/Category:All_articles_lacking_reliable_references" title="Category:All articles lacking reliable references">All articles lacking reliable references</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2024" title="Category:Articles with unsourced statements from March 2024">Articles with unsourced statements from March 2024</a></li><li><a href="/wiki/Category:Articles_with_excerpts" title="Category:Articles with excerpts">Articles with excerpts</a></li></ul></div></div>
				</div>
			</main>
			
		</div>
		<div class="mw-footer-container">
			
<footer id="footer" class="mw-footer" >
	<ul id="footer-info">
	<li id="footer-info-lastmod"> This page was last edited on 7 July 2024, at 00:45<span class="anonymous-show">&#160;(UTC)</span>.</li>
	<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License">Creative Commons Attribution-ShareAlike License 4.0</a><a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License" style="display:none;"></a>;
additional terms may apply. By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>

	<ul id="footer-places">
	<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy">Privacy policy</a></li>
	<li id="footer-places-about"><a href="/wiki/Wikipedia:About">About Wikipedia</a></li>
	<li id="footer-places-disclaimers"><a href="/wiki/Wikipedia:General_disclaimer">Disclaimers</a></li>
	<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
	<li id="footer-places-wm-codeofconduct"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct">Code of Conduct</a></li>
	<li id="footer-places-developers"><a href="https://developer.wikimedia.org">Developers</a></li>
	<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
	<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement">Cookie statement</a></li>
	<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=AI_safety&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
</ul>

	<ul id="footer-icons" class="noprint">
	<li id="footer-copyrightico"><a class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled" style="padding-left: 8px; padding-right: 8px;" href="https://wikimediafoundation.org/" target="https://wikimediafoundation.org/"><img src="/static/images/footer/wikimedia-button.svg" width="84" height="29" alt="Wikimedia Foundation"></a></li>
	<li id="footer-poweredbyico"><a class="cdx-button cdx-button--fake-button cdx-button--size-large cdx-button--fake-button--enabled" style="padding-left: 8px; padding-right: 8px;" href="https://www.mediawiki.org" target="https://www.mediawiki.org"><img src="/static/images/footer/poweredby_mediawiki.svg" width="84" height="29" alt="Powered by MediaWiki"></a></li>
</ul>

</footer>

		</div>
	</div> 
</div> 
<div class="vector-settings" id="p-dock-bottom">
	<ul>
		<li>
		</li>
	</ul>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgHostname":"mw-web.codfw.main-6457fbf49b-g2mn7","wgBackendResponseTime":167,"wgPageParseReport":{"limitreport":{"cputime":"1.601","walltime":"1.796","ppvisitednodes":{"value":10044,"limit":1000000},"postexpandincludesize":{"value":424512,"limit":2097152},"templateargumentsize":{"value":2204,"limit":2097152},"expansiondepth":{"value":12,"limit":100},"expensivefunctioncount":{"value":7,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":697711,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00% 1619.379      1 -total"," 68.41% 1107.848      1 Template:Reflist"," 26.70%  432.399     74 Template:Cite_journal"," 13.72%  222.225     49 Template:Cite_web","  7.18%  116.275      1 Template:Short_description","  5.87%   95.082      1 Template:Cite_magazine","  5.71%   92.524      1 Template:Excerpt","  5.18%   83.962      1 Template:Artificial_intelligence","  5.03%   81.523      1 Template:Sidebar_with_collapsible_lists","  3.88%   62.811      5 Template:Main_other"]},"scribunto":{"limitreport-timeusage":{"value":"1.137","limit":"10.000"},"limitreport-memusage":{"value":7704941,"limit":52428800},"limitreport-profile":[["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::callParserFunction","160","14.3"],["?","120","10.7"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::preprocess","120","10.7"],["dataWrapper \u003Cmw.lua:672\u003E","120","10.7"],["\u003Cmw.lua:694\u003E","80","7.1"],["is_set \u003CModule:Citation/CS1/Utilities:23\u003E","60","5.4"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::gsub","60","5.4"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::getAllExpandedArguments","40","3.6"],["MediaWiki\\Extension\\Scribunto\\Engines\\LuaSandbox\\LuaSandboxCallback::formatDate","40","3.6"],["pairs","40","3.6"],["[others]","280","25.0"]]},"cachereport":{"origin":"mw-web.codfw.main-6457fbf49b-zbq5f","timestamp":"20240707004623","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"AI safety","url":"https:\/\/en.wikipedia.org\/wiki\/AI_safety","sameAs":"http:\/\/www.wikidata.org\/entity\/Q116291231","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q116291231","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2022-07-15T19:32:03Z","dateModified":"2024-07-07T00:45:33Z","headline":"research area on making artificial intelligence safe and beneficial"}</script>
</body>
</html>